{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"examples/\")\n",
    "\n",
    "import logging\n",
    "import argparse\n",
    "import json\n",
    "from tqdm import tqdm, trange\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForPreTraining \n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_sent_cond import InputExample, random_word, InputFeatures, BERTDataset, x_in_y_int, mask_question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args\n",
    "gradient_accumulation_steps = 1\n",
    "train_batch_size = 1\n",
    "eval_file = \"dataset/dev-v2.0.json\"\n",
    "max_seq_length=256\n",
    "on_memory = True\n",
    "bert_model = \"model_sent2/pytorch_model1.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/08/2019 15:39:39 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/daniter/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "Loading Squad: 100%|██████████| 35/35 [00:00<00:00, 1587.81it/s]\n",
      "Loading Squad: 100%|██████████| 35/35 [00:00<00:00, 1529.19it/s]\n",
      "05/08/2019 15:39:44 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/daniter/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "05/08/2019 15:39:44 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /Users/daniter/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/xx/8h5l1j614vv5wmbx9fbj69wm0000gn/T/tmp00qlr2rp\n",
      "05/08/2019 15:39:47 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the vocab size: 30522\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "\n",
    "# Load eval_data\n",
    "eval_dataset_answerable = BERTDataset(eval_file, \"qparts/copy_parts2/parsed_qs_labels%s.pkl\", tokenizer, seq_len=max_seq_length,\n",
    "                                    on_memory=on_memory)\n",
    "eval_dataset_unanswerable = BERTDataset(eval_file, \"qparts/copy_parts2/parsed_qs_labels%s.pkl\", tokenizer, seq_len=max_seq_length,\n",
    "                                    on_memory=on_memory, keep_answerable=False)\n",
    "\n",
    "# Prepare model\n",
    "model_state_dict = torch.load(bert_model, map_location='cpu') #TODO daniter: remove this map_location\n",
    "## TODO daniter: check if bert model is being loaded correctly\n",
    "model = BertForPreTraining.from_pretrained(\"bert-base-uncased\", state_dict=model_state_dict)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Prepare optimizer\n",
    "print(\"Checking the vocab size:\", len(tokenizer.vocab))\n",
    "# 768 is bert hidden size, 256 is GRU hidden size, 1 is the layers in the GRU\n",
    "\n",
    "# eval loader\n",
    "eval_sampler_ans = SequentialSampler(eval_dataset_answerable)\n",
    "eval_dataloader_ans = DataLoader(eval_dataset_answerable, sampler=eval_sampler_ans,\n",
    "                                 batch_size=train_batch_size)\n",
    "eval_sampler_unans = SequentialSampler(eval_dataset_unanswerable)\n",
    "eval_dataloader_unans = DataLoader(eval_dataset_unanswerable, sampler=eval_sampler_unans,\n",
    "                                   batch_size=train_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples(contexts):\n",
    "    ans_examples = []\n",
    "    unans_examples = []\n",
    "    for context in contexts:\n",
    "        ans_questions = set()\n",
    "        unans_questions = set()\n",
    "        for x in eval_dataloader_unans.dataset.examples:\n",
    "            if x[0] == context:\n",
    "                if x[1] not in unans_questions:\n",
    "                    unans_examples.append(x)\n",
    "                unans_questions.add(x[1])        \n",
    "        for x in eval_dataloader_ans.dataset.examples:\n",
    "            if x[0] == context:\n",
    "                if x[1] not in ans_questions:\n",
    "                    ans_examples.append(x)\n",
    "                ans_questions.add(x[1])\n",
    "        #print (eval_dataloader_unans.dataset.contexts[context])\n",
    "    return((ans_examples, unans_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import LogSoftmax\n",
    "softmax_model = LogSoftmax(dim=0)\n",
    "\n",
    "def perplexity(logit_idx, dist):\n",
    "    log_prob = 0\n",
    "    for i, lg_idx in enumerate(logit_idx):\n",
    "        prob = softmax_model(dist[i])[lg_idx]\n",
    "        log_prob += prob\n",
    "    return (log_prob / len(logit_idx)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input(context, tokens_b, target_tokens, multihint=False):\n",
    "    tokenized_context = tokenizer.tokenize(context)\n",
    "    tokens_b = tokenizer.tokenize(tokens_b)\n",
    "    target_span, target_tag, _ = target_tokens\n",
    "    tokens_b = mask_question(tokens_b, target_span)\n",
    "    tokens_b += [\"[SEP]\"] + target_tag + [\"[SEP]\"]\n",
    "    targets = target_span + [\"[SEP]\"]\n",
    "    \n",
    "    buff_size = len(tokens_b) + len(targets)\n",
    "    if len(tokenized_context) + buff_size > max_seq_length - 3:\n",
    "        end = max_seq_length - 3 - buff_size\n",
    "        tokenized_context = tokenized_context[:end]\n",
    "    \n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokenized_context:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    for token in tokens_b:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(1)\n",
    "    \n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    return torch.tensor([input_ids]), torch.tensor([input_mask]), torch.tensor([segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_odds(examples, dataloader, multihint=False):\n",
    "    if len(examples) == 0:\n",
    "        return 0\n",
    "    total_total_odds = 0\n",
    "    total_total_perlex = 0\n",
    "    max_perplex = 0\n",
    "    results = {}\n",
    "    for example in examples:\n",
    "        cid, qid, targetid, _ = example\n",
    "        context = dataloader.dataset.contexts[cid]\n",
    "        question = dataloader.dataset.questions[qid]\n",
    "        raw_targ = dataloader.dataset.raw_targets[targetid]\n",
    "        results[(context, question)] = {}\n",
    "\n",
    "        raw_targ_copy = list(raw_targ)\n",
    "        raw_targ = [(tag,word) for (word,(_,tag)) in raw_targ if word]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            targs_2_tokens = []#[tokenizer.tokenize(t) for _, t in raw_targ]            \n",
    "            for tidx, (tag, words) in enumerate(raw_targ):\n",
    "                clean_tag = tag\n",
    "                if multihint:\n",
    "                    span = tokenizer.tokenize(clean_tag) + [\"[SEP]\"] + words\n",
    "                else:\n",
    "                    span = words\n",
    "                targs_2_tokens.append((span, tokenizer.tokenize(clean_tag), words))\n",
    "                    \n",
    "            targs_2_ids = [list(map(tokenizer.convert_tokens_to_ids, t)) for t in targs_2_tokens]\n",
    "            total_perplex = 0\n",
    "            for token_idx in range(len(raw_targ)):\n",
    "                input_ids, input_mask, segment_ids = build_input(context, question, targs_2_tokens[token_idx], multihint)\n",
    "                output, _ = model(input_ids, segment_ids, input_mask, None, None)\n",
    "                \n",
    "                start_i = np.where(input_mask.data.numpy() == 0)[1][0]\n",
    "                if len(targs_2_ids[token_idx]) == 0:\n",
    "                    print(token_idx, targs_2_ids, targs_2_ids[token_idx], raw_targ)\n",
    "\n",
    "                perplex = perplexity(targs_2_ids[token_idx][2], output[0][start_i:])\n",
    "                #print(question)\n",
    "                #print(input_ids.numpy()[0])\n",
    "                in_data = tokenizer.convert_ids_to_tokens(input_ids.numpy()[0])\n",
    "                in_data = in_data[in_data.index('[SEP]'):]\n",
    "                in_data = \" \".join([tmp for tmp in in_data if tmp not in ['[SEP]', '[PAD]']])\n",
    "                counters = {}\n",
    "                for j in range(len(raw_targ[token_idx][1])):\n",
    "                    c = Counter()\n",
    "                    for i, o in enumerate(output[0][start_i+j]):\n",
    "                        c[tokenizer.convert_ids_to_tokens([i])[0]]= o\n",
    "                    counters[raw_targ[token_idx][1][j]] = c\n",
    "                print(in_data)\n",
    "                print(\"Target\", raw_targ[token_idx][1])\n",
    "                headform = \"{:<20s}{:^10s}\" * len(raw_targ[token_idx][1])\n",
    "                form = \"{:<20s}{:^10.2f}\" * len(raw_targ[token_idx][1])\n",
    "\n",
    "                header = []\n",
    "                for h in raw_targ[token_idx][1]:\n",
    "                    header.append(h)\n",
    "                    header.append('score')\n",
    "                print(headform.format(*header))\n",
    "                print(\"-\"*80)\n",
    "                for search_depth in range(10):\n",
    "                    text = []\n",
    "                    for tok in raw_targ[token_idx][1]:\n",
    "                        word, score = counters[tok].most_common(10)[search_depth]\n",
    "                        text.append(word)\n",
    "                        text.append(score.item())\n",
    "                    print(form.format(*text))\n",
    "                print(\"perplexity:\", -perplex)\n",
    "                print(\"~\"*20)\n",
    "                results[(context, question)][(str([tt[0] for tt in targs_2_tokens[:token_idx]]), \n",
    "                                              str(targs_2_tokens[token_idx][1:]))] = -perplex\n",
    "                total_perplex += perplex / len(raw_targ)\n",
    "            print(\"Total Perplex for Q\", -total_perplex)\n",
    "            print(\"#\"*20)\n",
    "            total_total_perlex += -total_perplex\n",
    "            if -total_perplex > max_perplex:\n",
    "                max_perplex = -total_perplex\n",
    "\n",
    "    return (total_total_perlex / len(examples)), max_perplex, results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Plotting the relationship between level of income and inequality, Kuznets saw middle-income developing economies level of inequality bulging out to form what is now known as the Kuznets curve. Kuznets demonstrated this relationship using cross-sectional data. However, more recent testing of this theory with superior panel data has shown it to be very weak. Kuznets' curve predicts that income inequality will eventually decrease given time. As an example, income inequality did fall in the United States during its High school movement from 1910 to 1940 and thereafter.[citation needed] However, recent data shows that the level of income inequality began to rise after the 1970s. This does not necessarily disprove Kuznets' theory.[citation needed] It may be possible that another Kuznets' cycle is occurring, specifically the move from the manufacturing sector to the service sector.[citation needed] This implies that it may be possible for multiple Kuznets' cycles to be in effect at any given time.\n",
      "ans\n",
      "what is a a developing economy ' s [MASK] called ? sq\n",
      "Target ['level', 'of', 'inequality', 'bulging', 'out']\n",
      "level                 score   of                    score   inequality            score   bulging               score   out                   score   \n",
      "--------------------------------------------------------------------------------\n",
      "level                  9.76   bulging               11.14   inequality             9.99   [SEP]                  9.06   [SEP]                  9.18   \n",
      "inequality             7.62   of                     9.50   [SEP]                  8.13   to                     7.47   to                     7.28   \n",
      "rate                   6.01   expanding              8.03   in                     5.78   from                   5.90   inequality             5.98   \n",
      "levels                 5.65   in                     7.20   of                     5.43   in                     5.65   from                   5.93   \n",
      "flow                   5.58   to                     7.17   increase               5.22   inequality             5.61   increase               5.85   \n",
      "population             5.43   bulge                  7.07   with                   5.12   increase               5.49   by                     5.63   \n",
      "high                   5.30   inequality             7.06   the                    4.90   by                     5.39   decrease               5.48   \n",
      "current                5.23   increasing             6.90   to                     4.87   the                    4.97   in                     5.05   \n",
      "growth                 5.18   -                      6.63   and                    4.83   into                   4.94   the                    5.00   \n",
      "income                 5.14   being                  6.43   bulging                4.58   form                   4.76   form                   4.79   \n",
      "perplexity: 3.4077460765838623\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "what is a a [MASK] level of inequality bulging out called ? np\n",
      "Target ['developing', 'economy', \"'\", 's']\n",
      "developing            score   economy               score   '                     score   s                     score   \n",
      "--------------------------------------------------------------------------------\n",
      "group                  6.45   [SEP]                  9.50   income                 9.38   [SEP]                 10.25   \n",
      "country                6.32   between                8.28   [SEP]                  9.38   developing             8.20   \n",
      "high                   6.26   '                      6.99   s                      8.32   s                      7.61   \n",
      "development            5.79   -                      6.38   '                      5.76   '                      7.26   \n",
      "complex                5.78   in                     5.89   in                     5.34   growing                7.15   \n",
      "middle                 5.72   of                     5.29   and                    5.15   development            6.32   \n",
      "large                  5.66   level                  5.24   the                    4.80   having                 5.86   \n",
      "given                  5.48   for                    5.13   -                      4.78   a                      5.33   \n",
      "low                    5.41   with                   5.11   based                  4.63   developed              5.08   \n",
      "lower                  5.39   economy                5.01   growing                4.59   making                 4.99   \n",
      "perplexity: 4.547416687011719\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "what is a a developing economy ' s level of inequality bulging out [MASK] ? v ##bn\n",
      "Target ['called']\n",
      "called                score   \n",
      "--------------------------------------------------------------------------------\n",
      "called                16.85   \n",
      "named                 11.39   \n",
      "termed                10.47   \n",
      "known                 10.11   \n",
      "considered             9.80   \n",
      "titled                 8.30   \n",
      "formed                 8.10   \n",
      "renamed                7.61   \n",
      "pronounced             7.31   \n",
      "described              7.11   \n",
      "perplexity: 0.009700775146484375\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "[MASK] is a a developing economy ' s level of inequality bulging out called ? w ##hn ##p\n",
      "Target ['what']\n",
      "what                  score   \n",
      "--------------------------------------------------------------------------------\n",
      "what                  17.34   \n",
      "which                  9.51   \n",
      "how                    7.64   \n",
      "who                    6.61   \n",
      "whose                  6.03   \n",
      "[SEP]                  5.85   \n",
      "whatever               5.61   \n",
      "where                  5.14   \n",
      "that                   5.09   \n",
      "the                    4.63   \n",
      "perplexity: 0.000576019287109375\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Total Perplex for Q 1.9913598895072937\n",
      "####################\n",
      "what does [MASK] about income inequality given time ? sq\n",
      "Target ['ku', '##z', '##nets', \"'\", 'curve', 'predict']\n",
      "ku                    score   ##z                   score   ##nets                score   '                     score   curve                 score   predict               score   \n",
      "--------------------------------------------------------------------------------\n",
      "ku                    15.61   ##z                   15.82   ##nets                16.38   curve                  9.87   predict               11.58   predict               10.56   \n",
      "qu                     8.34   ##s                    8.00   ##net                 11.72   '                      8.91   [SEP]                 10.22   [SEP]                 10.18   \n",
      "k                      6.57   ##з                    7.71   '                      8.19   [SEP]                  8.31   '                      7.02   '                      6.68   \n",
      "it                     5.65   ##ez                   7.54   ##ten                  7.76   s                      7.87   predicting             6.24   s                      5.77   \n",
      "cu                     5.65   '                      7.49   s                      7.15   predict                6.77   s                      6.00   to                     5.66   \n",
      "kansas                 5.64   ##zu                   7.40   min                    7.12   ##s                    6.01   ##s                    5.87   predicting             5.63   \n",
      "ク                      5.28   ##zi                   7.30   [SEP]                  7.09   curves                 5.24   to                     5.38   ##s                    5.62   \n",
      "gu                     5.07   s                      6.64   ##tens                 6.83   find                   4.96   predicted              5.15   prediction             4.77   \n",
      "he                     4.36   ##zes                  6.59   ##nas                  6.82   speed                  4.62   prediction             5.15   predicted              4.74   \n",
      "ka                     4.32   ##zh                   6.46   ##uman                 6.62   ##z                    4.55   for                    4.76   for                    4.69   \n",
      "perplexity: 1.6826272010803223\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "what does ku ##z ##nets ' curve predict about [MASK] given time ? np\n",
      "Target ['income', 'inequality']\n",
      "income                score   inequality            score   \n",
      "--------------------------------------------------------------------------------\n",
      "income                12.81   inequality            12.93   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inequality             8.00   '                      7.92   \n",
      "average                6.45   [SEP]                  7.60   \n",
      "increase               6.31   equality               6.88   \n",
      "wealth                 6.28   and                    6.66   \n",
      "incomes                6.08   in                     6.21   \n",
      "money                  5.99   income                 6.19   \n",
      "economic               5.68   ##s                    5.62   \n",
      "their                  5.68   ##qual                 5.38   \n",
      "unemployment           5.60   or                     5.33   \n",
      "perplexity: 0.04287433624267578\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "what does ku ##z ##nets ' curve predict about income inequality [MASK] ? pp\n",
      "Target ['given', 'time']\n",
      "given                 score   time                  score   \n",
      "--------------------------------------------------------------------------------\n",
      "in                    11.37   given                  9.07   \n",
      "from                  11.18   time                   8.37   \n",
      "during                 9.50   give                   7.20   \n",
      "between                9.48   the                    7.19   \n",
      "after                  9.48   giving                 6.14   \n",
      "for                    8.66   a                      6.12   \n",
      "by                     8.58   year                   5.32   \n",
      "at                     8.48   times                  5.16   \n",
      "per                    7.72   difference             4.78   \n",
      "since                  7.72   passing                4.76   \n",
      "perplexity: 3.859240770339966\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "[MASK] does ku ##z ##nets ' curve predict about income inequality given time ? w ##hn ##p\n",
      "Target ['what']\n",
      "what                  score   \n",
      "--------------------------------------------------------------------------------\n",
      "what                  14.96   \n",
      "how                   10.09   \n",
      "which                  9.56   \n",
      "who                    8.96   \n",
      "whose                  7.59   \n",
      "that                   6.45   \n",
      "when                   5.72   \n",
      "[SEP]                  5.45   \n",
      "to                     4.75   \n",
      "in                     4.59   \n",
      "perplexity: 0.016134262084960938\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Total Perplex for Q 1.4002191424369812\n",
      "####################\n",
      "in what sector are [MASK] beginning to increase ? np\n",
      "Target ['jobs']\n",
      "jobs                  score   \n",
      "--------------------------------------------------------------------------------\n",
      "income                10.80   \n",
      "economic               6.36   \n",
      "higher                 5.84   \n",
      "unemployment           5.76   \n",
      "employment             5.72   \n",
      "wages                  5.63   \n",
      "jobs                   5.52   \n",
      "incomes                5.42   \n",
      "rate                   5.35   \n",
      "industrial             5.12   \n",
      "perplexity: 5.500776290893555\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "in what sector are jobs [MASK] to increase ? v ##b ##g\n",
      "Target ['beginning']\n",
      "beginning             score   \n",
      "--------------------------------------------------------------------------------\n",
      "starting              12.70   \n",
      "beginning             12.20   \n",
      "going                 11.20   \n",
      "trying                11.15   \n",
      "continuing            10.63   \n",
      "having                10.08   \n",
      "attempting            10.00   \n",
      "needing                9.18   \n",
      "seeming                8.83   \n",
      "hoping                 8.34   \n",
      "perplexity: 1.425480842590332\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "in what sector are jobs beginning to [MASK] ? v ##b\n",
      "Target ['increase']\n",
      "increase              score   \n",
      "--------------------------------------------------------------------------------\n",
      "increase              10.86   \n",
      "grow                  10.62   \n",
      "rise                   9.76   \n",
      "decline                9.67   \n",
      "appear                 9.60   \n",
      "emerge                 9.59   \n",
      "decrease               9.27   \n",
      "expand                 9.17   \n",
      "develop                9.12   \n",
      "change                 8.96   \n",
      "perplexity: 1.570749282836914\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "in [MASK] are jobs beginning to increase ? w ##hn ##p\n",
      "Target ['what', 'sector']\n",
      "what                  score   sector                score   \n",
      "--------------------------------------------------------------------------------\n",
      "what                  15.97   decade                 9.97   \n",
      "which                 13.28   sector                 9.17   \n",
      "how                    8.80   area                   7.98   \n",
      "who                    8.45   century                7.75   \n",
      "where                  7.18   industry               7.53   \n",
      "whose                  7.10   year                   7.48   \n",
      "whom                   7.01   era                    7.31   \n",
      "w                      5.64   country                7.18   \n",
      "when                   5.40   city                   7.13   \n",
      "that                   4.91   direction              6.58   \n",
      "perplexity: 0.9067244529724121\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Total Perplex for Q 2.3509327173233032\n",
      "####################\n",
      "in what sector are jobs beginning to [MASK] ? v ##b\n",
      "Target ['decrease']\n",
      "decrease              score   \n",
      "--------------------------------------------------------------------------------\n",
      "increase              10.86   \n",
      "grow                  10.62   \n",
      "rise                   9.76   \n",
      "decline                9.67   \n",
      "appear                 9.60   \n",
      "emerge                 9.59   \n",
      "decrease               9.27   \n",
      "expand                 9.17   \n",
      "develop                9.12   \n",
      "change                 8.96   \n",
      "perplexity: 3.1659469604492188\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "in what sector are [MASK] beginning to decrease ? np\n",
      "Target ['jobs']\n",
      "jobs                  score   \n",
      "--------------------------------------------------------------------------------\n",
      "income                11.11   \n",
      "economic               6.24   \n",
      "unemployment           5.93   \n",
      "wages                  5.61   \n",
      "poverty                5.56   \n",
      "employment             5.50   \n",
      "incomes                5.34   \n",
      "inequality             5.31   \n",
      "higher                 5.30   \n",
      "rate                   5.24   \n",
      "perplexity: 6.442903518676758\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "in what sector are jobs [MASK] to decrease ? v ##b ##g\n",
      "Target ['beginning']\n",
      "beginning             score   \n",
      "--------------------------------------------------------------------------------\n",
      "starting              13.29   \n",
      "beginning             12.65   \n",
      "going                 11.00   \n",
      "trying                10.90   \n",
      "having                10.55   \n",
      "continuing            10.53   \n",
      "attempting             9.94   \n",
      "needing                9.30   \n",
      "seeming                8.49   \n",
      "hoping                 7.78   \n",
      "perplexity: 1.324728012084961\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "in [MASK] are jobs beginning to decrease ? w ##hn ##p\n",
      "Target ['what', 'sector']\n",
      "what                  score   sector                score   \n",
      "--------------------------------------------------------------------------------\n",
      "what                  15.77   decade                 9.37   \n",
      "which                 13.06   sector                 8.14   \n",
      "how                    8.86   area                   7.66   \n",
      "who                    8.23   country                7.46   \n",
      "where                  7.15   century                7.39   \n",
      "whose                  6.88   era                    7.04   \n",
      "whom                   6.75   year                   7.04   \n",
      "when                   5.71   city                   6.87   \n",
      "w                      5.51   industry               6.81   \n",
      "why                    5.12   period                 6.64   \n",
      "perplexity: 1.1950206756591797\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Total Perplex for Q 3.0321497917175293\n",
      "####################\n",
      "what has recent testing of [MASK] data show it to be ? sp ##t ##k\n",
      "Target ['ku', '##z', '##nets', 'theory', 'with', 'superior']\n",
      "ku                    score   ##z                   score   ##nets                score   theory                score   with                  score   superior              score   \n",
      "--------------------------------------------------------------------------------\n",
      "ku                    12.07   ##z                   10.28   with                   8.69   '                     10.02   '                     10.01   [SEP]                 10.29   \n",
      "k                      6.32   with                   7.85   ##nets                 8.02   [SEP]                  8.92   [SEP]                  9.70   '                      9.36   \n",
      "qu                     5.87   -                      7.40   '                      7.92   s                      8.42   s                      8.72   s                      8.66   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kansas                 5.13   '                      6.19   ##net                  7.65   with                   7.85   with                   6.95   ##s                    6.15   \n",
      "different              5.10   and                    6.10   s                      7.41   ##s                    6.19   ##s                    6.27   with                   6.07   \n",
      "similar                4.93   ##s                    5.55   -                      7.24   ##nets                 5.50   superior               5.65   superior               5.51   \n",
      "ka                     4.66   ##zu                   5.28   based                  6.39   system                 5.41   system                 5.40   system                 5.38   \n",
      "kahn                   4.47   [SEP]                  4.59   and                    6.38   and                    5.28   and                    5.02   in                     4.86   \n",
      "this                   4.33   theory                 4.46   [SEP]                  6.25   based                  5.19   in                     5.00   and                    4.79   \n",
      "cu                     4.16   system                 4.35   net                    5.30   from                   5.15   from                   4.93   from                   4.65   \n",
      "perplexity: 3.2435741424560547\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "what has [MASK] of ku ##z ##nets theory with superior data show it to be ? np\n",
      "Target ['recent', 'testing']\n",
      "recent                score   testing               score   \n",
      "--------------------------------------------------------------------------------\n",
      "more                  13.63   recent                13.03   \n",
      "most                   8.08   [SEP]                  8.50   \n",
      "less                   8.04   testing                8.22   \n",
      "newer                  7.51   studies                8.18   \n",
      "some                   7.49   current                7.85   \n",
      "studies                7.31   latest                 7.24   \n",
      "research               7.28   recently               6.94   \n",
      "recent                 7.27   newer                  6.88   \n",
      "other                  7.02   tests                  6.64   \n",
      "tests                  6.72   findings               6.63   \n",
      "perplexity: 5.645021438598633\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "what has recent testing of ku ##z ##nets theory with superior data show [MASK] to be ? np\n",
      "Target ['it']\n",
      "it                    score   \n",
      "--------------------------------------------------------------------------------\n",
      "it                    12.15   \n",
      "them                   8.50   \n",
      "they                   7.26   \n",
      "their                  7.04   \n",
      "its                    6.81   \n",
      "there                  5.52   \n",
      "this                   4.99   \n",
      "his                    4.56   \n",
      "itself                 4.35   \n",
      "themselves             4.34   \n",
      "perplexity: 0.05428123474121094\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "[MASK] has recent testing of ku ##z ##nets theory with superior data show it to be ? w ##hn ##p\n",
      "Target ['what']\n",
      "what                  score   \n",
      "--------------------------------------------------------------------------------\n",
      "what                  13.66   \n",
      "how                   10.86   \n",
      "which                  7.13   \n",
      "[SEP]                  6.89   \n",
      "who                    6.68   \n",
      "whose                  6.13   \n",
      "where                  4.98   \n",
      "why                    4.42   \n",
      "and                    4.03   \n",
      "about                  4.00   \n",
      "perplexity: 0.06505298614501953\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Total Perplex for Q 2.2519824504852295\n",
      "####################\n",
      "when did [MASK] begin to increase in the us ? np\n",
      "Target ['income', 'inequality']\n",
      "income                score   inequality            score   \n",
      "--------------------------------------------------------------------------------\n",
      "income                13.01   inequality            14.10   \n",
      "high                   7.24   income                 7.36   \n",
      "poverty                6.59   equality               7.14   \n",
      "economic               6.16   poverty                6.84   \n",
      "incomes                5.69   [SEP]                  6.64   \n",
      "unemployment           5.59   rate                   6.27   \n",
      "higher                 5.37   education              6.11   \n",
      "average                5.31   enrollment             6.02   \n",
      "inequality             5.30   and                    5.72   \n",
      "wealth                 5.24   rates                  5.67   \n",
      "perplexity: 0.019150733947753906\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "when did income inequality [MASK] to increase in the us ? v ##b\n",
      "Target ['begin']\n",
      "begin                 score   \n",
      "--------------------------------------------------------------------------------\n",
      "begin                 16.84   \n",
      "start                 15.73   \n",
      "beginning              9.78   \n",
      "continue               9.49   \n",
      "cease                  9.38   \n",
      "begins                 8.97   \n",
      "began                  8.36   \n",
      "starts                 8.22   \n",
      "starting               7.94   \n",
      "begun                  7.52   \n",
      "perplexity: 0.28647613525390625\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "when did income inequality begin to [MASK] in the us ? v ##b\n",
      "Target ['increase']\n",
      "increase              score   \n",
      "--------------------------------------------------------------------------------\n",
      "fall                  12.35   \n",
      "rise                  10.53   \n",
      "decrease              10.03   \n",
      "decline               10.02   \n",
      "drop                   9.27   \n",
      "increase               9.21   \n",
      "grow                   8.09   \n",
      "occur                  7.75   \n",
      "happen                 7.56   \n",
      "collapse               7.41   \n",
      "perplexity: 3.5888891220092773\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "when did income inequality begin to increase in the [MASK] ? np\n",
      "Target ['us']\n",
      "us                    score   \n",
      "--------------------------------------------------------------------------------\n",
      "u                     12.44   \n",
      "us                    12.44   \n",
      "united                10.42   \n",
      "usa                    7.44   \n",
      "american               6.65   \n",
      "country                6.42   \n",
      "high                   6.16   \n",
      "states                 5.77   \n",
      "america                5.51   \n",
      "world                  4.86   \n",
      "perplexity: 0.7735805511474609\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "[MASK] did income inequality begin to increase in the us ? w ##had ##v ##p\n",
      "Target ['when']\n",
      "when                  score   \n",
      "--------------------------------------------------------------------------------\n",
      "when                  16.11   \n",
      "where                 10.61   \n",
      "how                   10.08   \n",
      "why                    9.81   \n",
      "what                   6.98   \n",
      "after                  6.84   \n",
      "and                    6.02   \n",
      "whenever               5.95   \n",
      "before                 5.82   \n",
      "which                  5.70   \n",
      "perplexity: 0.0089263916015625\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Total Perplex for Q 0.9354045867919921\n",
      "####################\n",
      "during what time period did income inequality decrease [MASK] ? pp\n",
      "Target ['in', 'the', 'united', 'states']\n",
      "in                    score   the                   score   united                score   states                score   \n",
      "--------------------------------------------------------------------------------\n",
      "in                    14.24   the                   11.29   united                10.87   [SEP]                 10.29   \n",
      "by                     7.56   its                    6.49   [SEP]                  8.99   states                 8.67   \n",
      "from                   7.51   2010                   6.41   states                 8.84   .                      6.59   \n",
      "between                6.95   a                      5.57   us                     8.72   movement               6.21   \n",
      "for                    6.51   in                     5.39   u                      8.03   s                      6.10   \n",
      "according              6.15   1910                   5.33   .                      7.12   school                 5.45   \n",
      "due                    6.01   washington             4.72   high                   6.98   us                     5.01   \n",
      "among                  5.61   to                     4.71   usa                    6.40   in                     4.79   \n",
      "as                     5.46   time                   4.52   country                5.87   united                 4.74   \n",
      "along                  5.43   [SEP]                  4.35   state                  5.52   u                      4.63   \n",
      "perplexity: 0.611915111541748\n",
      "~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "during what time period did [MASK] decrease in the united states ? np\n",
      "Target ['income', 'inequality']\n",
      "income                score   inequality            score   \n",
      "--------------------------------------------------------------------------------\n",
      "income                12.59   inequality            14.34   \n",
      "high                   7.62   equality               7.15   \n",
      "poverty                6.86   poverty                7.13   \n",
      "economic               6.42   income                 7.04   \n",
      "unemployment           5.96   [SEP]                  6.89   \n",
      "incomes                5.59   enrollment             6.48   \n",
      "education              5.56   education              6.16   \n",
      "inequality             5.56   rate                   6.09   \n",
      "average                5.22   rates                  5.89   \n",
      "employment             5.15   unemployment           5.78   \n",
      "perplexity: 0.027944564819335938\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "during what time period did income inequality [MASK] in the united states ? n ##n\n",
      "Target ['decrease']\n",
      "decrease              score   \n",
      "--------------------------------------------------------------------------------\n",
      "fall                  15.69   \n",
      "drop                  11.61   \n",
      "decline               11.59   \n",
      "rise                  11.26   \n",
      "decrease              10.89   \n",
      "increase               9.41   \n",
      "collapse               8.48   \n",
      "falls                  8.33   \n",
      "falling                7.91   \n",
      "fell                   7.68   \n",
      "perplexity: 4.85770320892334\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "[MASK] did income inequality decrease in the united states ? w ##hp ##p\n",
      "Target ['during', 'what', 'time', 'period']\n",
      "during                score   what                  score   time                  score   period                score   \n",
      "--------------------------------------------------------------------------------\n",
      "during                12.82   what                  14.12   period                10.39   [SEP]                 11.45   \n",
      "in                    11.35   which                 12.63   time                   9.25   period                 9.93   \n",
      "from                  10.79   whose                  8.35   years                  7.77   time                   6.96   \n",
      "between               10.46   when                   7.75   year                   7.14   of                     6.40   \n",
      "at                     8.51   how                    7.36   movement               6.84   movement               6.19   \n",
      "under                  8.07   whom                   7.06   span                   6.40   years                  5.65   \n",
      "after                  7.89   approximately          6.89   [SEP]                  6.37   and                    5.46   \n",
      "around                 7.51   who                    6.70   decade                 6.15   span                   5.17   \n",
      "by                     7.48   about                  6.11   point                  5.38   in                     4.84   \n",
      "over                   7.39   where                  5.41   school                 5.29   the                    4.34   \n",
      "perplexity: 1.0065128803253174\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Total Perplex for Q 1.6260189414024353\n",
      "####################\n",
      "who plotted the relationships between levels of [MASK] ? np\n",
      "Target ['income', 'and', 'inequality']\n",
      "income                score   and                   score   inequality            score   \n",
      "--------------------------------------------------------------------------------\n",
      "income                10.78   and                   10.57   inequality            11.61   \n",
      "inequality            10.38   inequality             9.81   [SEP]                  9.27   \n",
      "wealth                 6.96   income                 7.93   and                    7.62   \n",
      "poverty                6.34   [SEP]                  7.81   equality               5.91   \n",
      "growth                 5.53   wealth                 6.09   ,                      5.39   \n",
      "economic               5.26   rates                  6.03   income                 5.23   \n",
      "equality               5.09   ,                      6.00   wealth                 4.42   \n",
      "and                    5.04   ##s                    5.54   ##qual                 4.41   \n",
      "incomes                5.04   poverty                5.37   ##s                    4.30   \n",
      "both                   4.91   ##qual                 5.30   s                      4.11   \n",
      "perplexity: 0.4567597806453705\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "who [MASK] the relationships between levels of income and inequality ? v ##b ##d\n",
      "Target ['plotted']\n",
      "plotted               score   \n",
      "--------------------------------------------------------------------------------\n",
      "plotted               13.94   \n",
      "studied               10.70   \n",
      "showed                10.41   \n",
      "wrote                  9.97   \n",
      "plotting               9.91   \n",
      "described              9.65   \n",
      "measured               9.37   \n",
      "determined             9.16   \n",
      "diagrams               9.10   \n",
      "calculated             8.98   \n",
      "perplexity: 0.2967214584350586\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "who plotted the [MASK] between levels of income and inequality ? np\n",
      "Target ['relationships']\n",
      "relationships         score   \n",
      "--------------------------------------------------------------------------------\n",
      "relationship          11.72   \n",
      "relation               8.04   \n",
      "relationships          7.79   \n",
      "connection             7.21   \n",
      "relations              7.03   \n",
      "difference             6.92   \n",
      "correlation            6.79   \n",
      "differences            6.31   \n",
      "interaction            6.24   \n",
      "similarity             5.90   \n",
      "perplexity: 4.136817932128906\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "who plotted the relationships between [MASK] of income and inequality ? np\n",
      "Target ['levels']\n",
      "levels                score   \n",
      "--------------------------------------------------------------------------------\n",
      "levels                11.46   \n",
      "middle                 7.86   \n",
      "level                  7.71   \n",
      "high                   7.32   \n",
      "different              6.33   \n",
      "lower                  6.33   \n",
      "two                    6.25   \n",
      "low                    5.83   \n",
      "higher                 5.58   \n",
      "rates                  5.31   \n",
      "perplexity: 0.15701580047607422\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "[MASK] plotted the relationships between levels of income and inequality ? w ##hn ##p\n",
      "Target ['who']\n",
      "who                   score   \n",
      "--------------------------------------------------------------------------------\n",
      "who                   16.08   \n",
      "what                  11.33   \n",
      "which                 10.99   \n",
      "whom                   9.33   \n",
      "whose                  8.39   \n",
      "how                    6.99   \n",
      "that                   6.25   \n",
      "where                  6.22   \n",
      "he                     4.88   \n",
      "and                    4.63   \n",
      "perplexity: 0.016628265380859375\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Total Perplex for Q 1.0127886474132537\n",
      "####################\n",
      "what [MASK] at any given time ? sq\n",
      "Target ['may', 'be', 'possible', 'for', 'multiple', 'ku', '##z', '##nets', \"'\", 'cycles', 'to', 'be', 'in']\n",
      "may                   score   be                    score   possible              score   for                   score   multiple              score   ku                    score   ##z                   score   ##nets                score   '                     score   cycles                score   to                    score   be                    score   in                    score   \n",
      "--------------------------------------------------------------------------------\n",
      "may                   15.13   [SEP]                  9.15   possible               8.81   to                     7.55   [SEP]                  8.19   [SEP]                  8.82   [SEP]                  9.14   [SEP]                  9.19   [SEP]                  9.21   [SEP]                  9.23   [SEP]                  9.17   [SEP]                  9.15   [SEP]                  9.17   \n",
      "predict               13.58   decrease               7.96   that                   8.63   increase               7.23   decrease               8.00   decrease               8.08   decrease               7.93   decrease               7.87   decrease               7.78   decrease               7.82   decrease               7.94   decrease               7.96   decrease               7.93   \n",
      "will                  11.82   to                     7.03   to                     7.63   will                   7.09   increase               7.67   increase               7.24   to                     6.93   to                     6.89   to                     6.80   increase               6.69   to                     6.88   to                     7.03   to                     7.11   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can                   11.39   increase               6.94   be                     7.42   [SEP]                  7.03   to                     7.38   to                     7.17   increase               6.78   increase               6.67   increase               6.59   to                     6.69   increase               6.82   increase               6.94   increase               6.89   \n",
      "might                 10.97   '                      6.25   in                     6.34   decrease               6.50   '                      5.88   '                      6.17   '                      6.13   '                      6.10   '                      6.05   '                      6.06   '                      6.26   '                      6.25   '                      6.19   \n",
      "suggests               9.84   by                     5.99   [SEP]                  5.87   that                   5.90   by                     5.82   by                     5.88   by                     5.85   by                     5.76   by                     5.64   by                     5.81   by                     5.83   by                     5.99   by                     5.97   \n",
      "must                   9.74   in                     5.69   if                     5.72   inequality             5.64   in                     5.62   in                     5.70   in                     5.63   in                     5.62   in                     5.57   in                     5.51   in                     5.52   in                     5.69   in                     5.69   \n",
      "cannot                 9.56   be                     5.07   the                    5.39   be                     5.45   decreases              5.51   decreases              5.31   be                     4.96   be                     4.93   be                     4.75   be                     4.84   decreases              4.91   be                     5.07   decreases              5.10   \n",
      "would                  9.35   decreases              5.03   happen                 5.32   in                     5.27   be                     5.49   be                     5.18   decreases              4.93   decreases              4.85   decreases              4.71   s                      4.77   be                     4.89   decreases              5.03   be                     5.09   \n",
      "appears                8.77   s                      4.56   any                    5.29   by                     5.14   will                   5.49   the                    4.63   s                      4.60   s                      4.60   s                      4.63   decreases              4.76   s                      4.66   s                      4.56   and                    4.53   \n",
      "perplexity: 5.885782241821289\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "what may be possible for multiple ku ##z ##nets ' cycles to be in [MASK] ? pp\n",
      "Target ['at', 'any', 'given', 'time']\n",
      "at                    score   any                   score   given                 score   time                  score   \n",
      "--------------------------------------------------------------------------------\n",
      "effect                10.14   any                   11.79   given                 10.37   time                  12.28   \n",
      "any                    9.18   at                    11.28   time                   8.90   [SEP]                  9.67   \n",
      "at                     8.48   or                     7.14   any                    8.64   given                  7.39   \n",
      "only                   6.64   with                   6.68   [SEP]                  8.36   times                  6.64   \n",
      "in                     6.48   of                     6.53   at                     7.27   period                 5.52   \n",
      "on                     6.38   the                    6.38   year                   7.07   of                     5.37   \n",
      "effective              6.29   [SEP]                  6.37   ##able                 6.70   year                   5.17   \n",
      "all                    6.14   a                      6.36   of                     5.84   in                     4.63   \n",
      "no                     6.08   -                      6.36   or                     5.79   a                      4.56   \n",
      "with                   5.75   absence                6.08   season                 5.74   at                     4.49   \n",
      "perplexity: 0.927170991897583\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "[MASK] may be possible for multiple ku ##z ##nets ' cycles to be in at any given time ? w ##hn ##p\n",
      "Target ['what']\n",
      "what                  score   \n",
      "--------------------------------------------------------------------------------\n",
      "what                  15.06   \n",
      "which                 10.25   \n",
      "how                    9.42   \n",
      "who                    7.97   \n",
      "that                   7.16   \n",
      "whose                  6.73   \n",
      "why                    6.19   \n",
      "when                   6.01   \n",
      "where                  5.48   \n",
      "whom                   4.15   \n",
      "perplexity: 0.013787269592285156\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Total Perplex for Q 2.2755801677703857\n",
      "####################\n",
      "unans\n",
      "who never plotted the relationships between levels of [MASK] ? np\n",
      "Target ['income', 'and', 'inequality']\n",
      "income                score   and                   score   inequality            score   \n",
      "--------------------------------------------------------------------------------\n",
      "income                10.88   inequality            10.15   inequality            11.31   \n",
      "inequality            10.20   and                   10.01   [SEP]                  9.52   \n",
      "wealth                 6.93   [SEP]                  8.32   and                    7.21   \n",
      "poverty                6.23   income                 7.90   equality               5.75   \n",
      "growth                 5.48   rates                  6.29   income                 5.47   \n",
      "incomes                5.32   wealth                 5.91   ,                      5.02   \n",
      "economic               5.25   ,                      5.59   wealth                 4.52   \n",
      "equality               5.08   ##s                    5.58   ##qual                 4.48   \n",
      "living                 4.83   or                     5.56   s                      4.46   \n",
      "wages                  4.72   ##qual                 5.51   ##s                    4.42   \n",
      "perplexity: 0.5900421142578125\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "who [MASK] plotted the relationships between levels of income and inequality ? rb\n",
      "Target ['never']\n",
      "never                 score   \n",
      "--------------------------------------------------------------------------------\n",
      "first                  9.81   \n",
      "originally             9.72   \n",
      "actually               8.89   \n",
      "initially              8.70   \n",
      "also                   8.51   \n",
      "previously             8.28   \n",
      "supposedly             7.88   \n",
      "visually               7.81   \n",
      "once                   7.79   \n",
      "really                 7.23   \n",
      "perplexity: 7.674601078033447\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "who never [MASK] the relationships between levels of income and inequality ? v ##b ##d\n",
      "Target ['plotted']\n",
      "plotted               score   \n",
      "--------------------------------------------------------------------------------\n",
      "plotted               12.28   \n",
      "showed                10.46   \n",
      "found                 10.14   \n",
      "studied                9.88   \n",
      "discovered             9.85   \n",
      "described              9.69   \n",
      "reported               9.61   \n",
      "measured               9.32   \n",
      "detailed               9.22   \n",
      "determined             9.15   \n",
      "perplexity: 1.044795036315918\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "who never plotted the [MASK] between levels of income and inequality ? np\n",
      "Target ['relationships']\n",
      "relationships         score   \n",
      "--------------------------------------------------------------------------------\n",
      "relationship          11.16   \n",
      "relationships          7.97   \n",
      "relation               7.25   \n",
      "connection             7.18   \n",
      "correlation            6.83   \n",
      "relations              6.64   \n",
      "difference             6.63   \n",
      "link                   6.11   \n",
      "differences            5.92   \n",
      "interaction            5.58   \n",
      "perplexity: 3.4612889289855957\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "who never plotted the relationships between [MASK] of income and inequality ? np\n",
      "Target ['levels']\n",
      "levels                score   \n",
      "--------------------------------------------------------------------------------\n",
      "levels                11.03   \n",
      "level                  7.75   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high                   7.02   \n",
      "their                  6.68   \n",
      "middle                 6.33   \n",
      "two                    5.83   \n",
      "lower                  5.76   \n",
      "different              5.75   \n",
      "higher                 5.69   \n",
      "rates                  5.50   \n",
      "perplexity: 0.1948537826538086\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "[MASK] never plotted the relationships between levels of income and inequality ? w ##hn ##p\n",
      "Target ['who']\n",
      "who                   score   \n",
      "--------------------------------------------------------------------------------\n",
      "who                   15.45   \n",
      "what                  11.62   \n",
      "which                 11.61   \n",
      "whom                   9.38   \n",
      "whose                  8.74   \n",
      "how                    6.70   \n",
      "that                   6.59   \n",
      "where                  6.32   \n",
      "he                     4.44   \n",
      "when                   4.41   \n",
      "perplexity: 0.04620361328125\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Total Perplex for Q 2.168630758921305\n",
      "####################\n",
      "in what sector are jobs beginning to [MASK] ? v ##b\n",
      "Target ['decrease']\n",
      "decrease              score   \n",
      "--------------------------------------------------------------------------------\n",
      "increase              10.86   \n",
      "grow                  10.62   \n",
      "rise                   9.76   \n",
      "decline                9.67   \n",
      "appear                 9.60   \n",
      "emerge                 9.59   \n",
      "decrease               9.27   \n",
      "expand                 9.17   \n",
      "develop                9.12   \n",
      "change                 8.96   \n",
      "perplexity: 3.1659469604492188\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "in what sector are [MASK] beginning to decrease ? np\n",
      "Target ['jobs']\n",
      "jobs                  score   \n",
      "--------------------------------------------------------------------------------\n",
      "income                11.11   \n",
      "economic               6.24   \n",
      "unemployment           5.93   \n",
      "wages                  5.61   \n",
      "poverty                5.56   \n",
      "employment             5.50   \n",
      "incomes                5.34   \n",
      "inequality             5.31   \n",
      "higher                 5.30   \n",
      "rate                   5.24   \n",
      "perplexity: 6.442903518676758\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "in what sector are jobs [MASK] to decrease ? v ##b ##g\n",
      "Target ['beginning']\n",
      "beginning             score   \n",
      "--------------------------------------------------------------------------------\n",
      "starting              13.29   \n",
      "beginning             12.65   \n",
      "going                 11.00   \n",
      "trying                10.90   \n",
      "having                10.55   \n",
      "continuing            10.53   \n",
      "attempting             9.94   \n",
      "needing                9.30   \n",
      "seeming                8.49   \n",
      "hoping                 7.78   \n",
      "perplexity: 1.324728012084961\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "in [MASK] are jobs beginning to decrease ? w ##hn ##p\n",
      "Target ['what', 'sector']\n",
      "what                  score   sector                score   \n",
      "--------------------------------------------------------------------------------\n",
      "what                  15.77   decade                 9.37   \n",
      "which                 13.06   sector                 8.14   \n",
      "how                    8.86   area                   7.66   \n",
      "who                    8.23   country                7.46   \n",
      "where                  7.15   century                7.39   \n",
      "whose                  6.88   era                    7.04   \n",
      "whom                   6.75   year                   7.04   \n",
      "when                   5.71   city                   6.87   \n",
      "w                      5.51   industry               6.81   \n",
      "why                    5.12   period                 6.64   \n",
      "perplexity: 1.1950206756591797\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Total Perplex for Q 3.0321497917175293\n",
      "####################\n",
      "during what time period did income inequality increase [MASK] ? pp\n",
      "Target ['in', 'the', 'united', 'states']\n",
      "in                    score   the                   score   united                score   states                score   \n",
      "--------------------------------------------------------------------------------\n",
      "in                    14.41   the                   11.67   united                10.70   [SEP]                 10.43   \n",
      "by                     7.87   its                    6.66   [SEP]                  9.00   states                 9.16   \n",
      "from                   7.68   in                     5.66   states                 8.79   .                      7.00   \n",
      "between                6.78   a                      5.48   us                     8.56   s                      6.52   \n",
      "for                    6.57   2010                   4.86   u                      7.95   movement               6.20   \n",
      "during                 5.99   time                   4.64   high                   7.16   us                     5.17   \n",
      "among                  5.78   to                     4.61   .                      7.04   school                 5.08   \n",
      "along                  5.48   america                4.53   usa                    6.44   united                 5.05   \n",
      "according              5.42   [SEP]                  4.46   country                6.15   u                      4.96   \n",
      "within                 5.35   washington             4.11   america                5.63   in                     4.83   \n",
      "perplexity: 0.5468401908874512\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "during what time period did [MASK] increase in the united states ? np\n",
      "Target ['income', 'inequality']\n",
      "income                score   inequality            score   \n",
      "--------------------------------------------------------------------------------\n",
      "income                12.54   inequality            14.11   \n",
      "high                   8.02   equality               7.22   \n",
      "poverty                6.63   income                 7.15   \n",
      "economic               6.41   poverty                6.97   \n",
      "higher                 5.67   [SEP]                  6.74   \n",
      "unemployment           5.67   education              6.39   \n",
      "education              5.51   enrollment             6.36   \n",
      "incomes                5.45   rate                   6.08   \n",
      "inequality             5.39   rates                  5.80   \n",
      "wealth                 5.35   and                    5.67   \n",
      "perplexity: 0.031552791595458984\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "[MASK] did income inequality increase in the united states ? w ##hp ##p\n",
      "Target ['during', 'what', 'time', 'period']\n",
      "during                score   what                  score   time                  score   period                score   \n",
      "--------------------------------------------------------------------------------\n",
      "during                12.64   what                  14.03   period                10.40   [SEP]                 11.41   \n",
      "in                    11.28   which                 12.47   time                   9.28   period                 9.95   \n",
      "from                  10.63   whose                  8.12   years                  7.82   time                   7.02   \n",
      "between               10.13   when                   7.51   year                   7.04   of                     6.35   \n",
      "at                     8.41   how                    7.11   movement               6.90   movement               6.22   \n",
      "under                  7.91   whom                   6.88   decade                 6.43   years                  5.77   \n",
      "after                  7.75   approximately          6.75   span                   6.36   and                    5.48   \n",
      "over                   7.47   who                    6.42   [SEP]                  6.22   span                   5.20   \n",
      "by                     7.40   about                  5.96   point                  5.36   in                     4.74   \n",
      "around                 7.34   where                  5.28   school                 5.22   the                    4.34   \n",
      "perplexity: 0.9896409511566162\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Total Perplex for Q 0.522677977879842\n",
      "####################\n",
      "when did [MASK] begin to decrease in the us ? np\n",
      "Target ['income', 'inequality']\n",
      "income                score   inequality            score   \n",
      "--------------------------------------------------------------------------------\n",
      "income                13.11   inequality            14.39   \n",
      "poverty                6.84   income                 7.30   \n",
      "high                   6.75   equality               7.14   \n",
      "economic               6.25   poverty                7.02   \n",
      "incomes                5.88   [SEP]                  6.71   \n",
      "unemployment           5.78   rate                   6.20   \n",
      "inequality             5.66   enrollment             6.16   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average                5.51   education              5.86   \n",
      "wealth                 5.15   rates                  5.79   \n",
      "rate                   5.06   and                    5.68   \n",
      "perplexity: 0.01641082763671875\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "when did income inequality begin to [MASK] in the us ? v ##b\n",
      "Target ['decrease']\n",
      "decrease              score   \n",
      "--------------------------------------------------------------------------------\n",
      "fall                  12.35   \n",
      "rise                  10.53   \n",
      "decrease              10.03   \n",
      "decline               10.02   \n",
      "drop                   9.27   \n",
      "increase               9.21   \n",
      "grow                   8.09   \n",
      "occur                  7.75   \n",
      "happen                 7.56   \n",
      "collapse               7.41   \n",
      "perplexity: 2.7690649032592773\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "when did income inequality [MASK] to decrease in the us ? v ##b\n",
      "Target ['begin']\n",
      "begin                 score   \n",
      "--------------------------------------------------------------------------------\n",
      "begin                 17.35   \n",
      "start                 16.27   \n",
      "beginning             10.54   \n",
      "continue               9.69   \n",
      "begins                 9.63   \n",
      "cease                  9.34   \n",
      "began                  9.09   \n",
      "starts                 8.84   \n",
      "starting               8.62   \n",
      "begun                  8.21   \n",
      "perplexity: 0.2963848114013672\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "when did income inequality begin to decrease in the [MASK] ? np\n",
      "Target ['us']\n",
      "us                    score   \n",
      "--------------------------------------------------------------------------------\n",
      "u                     12.46   \n",
      "us                    12.33   \n",
      "united                10.45   \n",
      "usa                    7.40   \n",
      "american               6.49   \n",
      "country                6.32   \n",
      "high                   6.12   \n",
      "states                 5.77   \n",
      "america                5.29   \n",
      "s                      4.93   \n",
      "perplexity: 0.8436489105224609\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "[MASK] did income inequality begin to decrease in the us ? w ##had ##v ##p\n",
      "Target ['when']\n",
      "when                  score   \n",
      "--------------------------------------------------------------------------------\n",
      "when                  16.29   \n",
      "where                 10.72   \n",
      "how                   10.36   \n",
      "why                   10.27   \n",
      "what                   7.22   \n",
      "after                  7.00   \n",
      "whenever               6.04   \n",
      "which                  5.98   \n",
      "before                 5.91   \n",
      "and                    5.89   \n",
      "perplexity: 0.009510040283203125\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Total Perplex for Q 0.7870038986206055\n",
      "####################\n",
      "in what sector are [MASK] beginning to increase ? np\n",
      "Target ['jobs']\n",
      "jobs                  score   \n",
      "--------------------------------------------------------------------------------\n",
      "income                10.80   \n",
      "economic               6.36   \n",
      "higher                 5.84   \n",
      "unemployment           5.76   \n",
      "employment             5.72   \n",
      "wages                  5.63   \n",
      "jobs                   5.52   \n",
      "incomes                5.42   \n",
      "rate                   5.35   \n",
      "industrial             5.12   \n",
      "perplexity: 5.500776290893555\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "in what sector are jobs [MASK] to increase ? v ##b ##g\n",
      "Target ['beginning']\n",
      "beginning             score   \n",
      "--------------------------------------------------------------------------------\n",
      "starting              12.70   \n",
      "beginning             12.20   \n",
      "going                 11.20   \n",
      "trying                11.15   \n",
      "continuing            10.63   \n",
      "having                10.08   \n",
      "attempting            10.00   \n",
      "needing                9.18   \n",
      "seeming                8.83   \n",
      "hoping                 8.34   \n",
      "perplexity: 1.425480842590332\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "in what sector are jobs beginning to [MASK] ? v ##b\n",
      "Target ['increase']\n",
      "increase              score   \n",
      "--------------------------------------------------------------------------------\n",
      "increase              10.86   \n",
      "grow                  10.62   \n",
      "rise                   9.76   \n",
      "decline                9.67   \n",
      "appear                 9.60   \n",
      "emerge                 9.59   \n",
      "decrease               9.27   \n",
      "expand                 9.17   \n",
      "develop                9.12   \n",
      "change                 8.96   \n",
      "perplexity: 1.570749282836914\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "in [MASK] are jobs beginning to increase ? w ##hn ##p\n",
      "Target ['what', 'sector']\n",
      "what                  score   sector                score   \n",
      "--------------------------------------------------------------------------------\n",
      "what                  15.97   decade                 9.97   \n",
      "which                 13.28   sector                 9.17   \n",
      "how                    8.80   area                   7.98   \n",
      "who                    8.45   century                7.75   \n",
      "where                  7.18   industry               7.53   \n",
      "whose                  7.10   year                   7.48   \n",
      "whom                   7.01   era                    7.31   \n",
      "w                      5.64   country                7.18   \n",
      "when                   5.40   city                   7.13   \n",
      "that                   4.91   direction              6.58   \n",
      "perplexity: 0.9067244529724121\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Total Perplex for Q 2.3509327173233032\n",
      "####################\n",
      "1.8751595927609341 1.7722790288925168 3.0321497917175293 3.0321497917175293\n"
     ]
    }
   ],
   "source": [
    "contexts = random.sample(range(1203), 1)\n",
    "ans_e, unans_e = get_examples(contexts)\n",
    "ans_res, unans_res = {}, {}\n",
    "for context in contexts:\n",
    "    #print_context_and_questions(context, ans_e, unans_e)\n",
    "    print(\"Context\", eval_dataloader_ans.dataset.contexts[context])\n",
    "    print(\"ans\")\n",
    "    avg_ans_odds, max_ent_ans, r = get_avg_odds([e for e in ans_e if e[0] == context], eval_dataloader_ans)\n",
    "    ans_res.update(r)\n",
    "    print(\"unans\")\n",
    "    avg_unans_odds, max_ent_unans, r = get_avg_odds([e for e in unans_e if e[0] == context], eval_dataloader_unans)\n",
    "    unans_res.update(r)\n",
    "    print(avg_ans_odds, avg_unans_odds, max_ent_ans, max_ent_unans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas:\n",
    "Some kind of weighting scheme based on how hard words are\n",
    "- what should weigh less than 'lisbon treaty'\n",
    "- when was fresno county courthouse demolished has high probability:\n",
    "    - this is because we know that it's been demolished and the entity is the same, the article just doesn't say when\n",
    "    - maybe include answer somewhere but not directly (ie. using a sentence or candidate answer to condition the model or else it might never learn that when questions need dates)\n",
    "    - there is something that looks like a date (between the 1880s and wwii) and a number (1401) but no relationship showing that a demolition took place at a certain date \n",
    "    \n",
    "Some paraphrases are surprising (GDP instead of gross domestic product). Is there another way to describe the probability, such as, given data we said GDP, how far is the meaning from the most likely meaning?\n",
    "\n",
    "Can we make some small edits to siginficantly increase the probability of the question?\n",
    "\n",
    "TODO:\n",
    "- read about bert speaking model again\n",
    "- implement language model for christ sakes\n",
    "- get rid of w- words in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-2ab61fd9a76f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mtotal_odds\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0modds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mperplex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargs_2_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                 results[(context_text, question)][(str([tt[0] for tt in targs_2_tokens[:token_idx]]), \n\u001b[1;32m     61\u001b[0m                                               str(targs_2_tokens[token_idx][1:]))] = -perplex\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "contexts = random.sample(range(1203), 1)\n",
    "ans_e, unans_e = get_examples(contexts)\n",
    "for context in contexts:\n",
    "    #avg_ans_odds, max_ent_ans, r = get_avg_odds([e for e in ans_e if e[0] == context], eval_dataloader_ans)\n",
    "    examples = [e for e in unans_e if e[0] == context]\n",
    "    dataloader = eval_dataloader_unans\n",
    "    multihint = False\n",
    "    total_total_odds = 0\n",
    "    total_total_perlex = 0\n",
    "    max_perplex = 0\n",
    "    results = {}\n",
    "    for example in examples:\n",
    "        cid, qid, targetid, _ = example\n",
    "        context_text = dataloader.dataset.contexts[cid]\n",
    "        question = dataloader.dataset.questions[qid]\n",
    "        raw_targ = dataloader.dataset.raw_targets[targetid]\n",
    "        results[(context_text, question)] = {}\n",
    "\n",
    "        raw_targ_copy = list(raw_targ)\n",
    "        raw_targ = [(tag,word) for (word,(_,tag)) in raw_targ if word]\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            targs_2_tokens = []#[tokenizer.tokenize(t) for _, t in raw_targ]            \n",
    "            for tidx, (tag, words) in enumerate(raw_targ):\n",
    "                clean_tag = tag\n",
    "                if multihint:\n",
    "                    span = tokenizer.tokenize(clean_tag) + [\"[SEP]\"] + words\n",
    "                else:\n",
    "                    span = words\n",
    "                targs_2_tokens.append((span, tokenizer.tokenize(clean_tag), words))\n",
    "\n",
    "            targs_2_ids = [list(map(tokenizer.convert_tokens_to_ids, t)) for t in targs_2_tokens]\n",
    "\n",
    "            total_odds = 0\n",
    "            min_odds = 100\n",
    "            total_perplex = 0\n",
    "            for token_idx in range(len(raw_targ)):\n",
    "                odds = 0\n",
    "                odds_list = []\n",
    "                input_ids, input_mask, segment_ids = build_input(context_text, targs_2_tokens[:token_idx], targs_2_tokens[token_idx], multihint)\n",
    "                output, _ = model(input_ids, segment_ids, input_mask, None, None)\n",
    "\n",
    "                #print(input_mask)\n",
    "                start_i = np.where(input_mask.data.numpy() == 0)[1][0]\n",
    "                for t_i, t in enumerate(targs_2_ids[token_idx][2]):\n",
    "                    odds += output[0][start_i+t_i][t]\n",
    "                    odds_list.append(output[0][start_i+t_i][t])\n",
    "                if len(targs_2_ids[token_idx]) == 0:\n",
    "                    print(token_idx, targs_2_ids, targs_2_ids[token_idx], raw_targ)\n",
    "                odds = odds/len(targs_2_ids[token_idx])\n",
    "                if odds < min_odds:\n",
    "                    min_odds = odds\n",
    "                # print(odds)\n",
    "                total_odds += odds\n",
    "                perplex = perplexity(targs_2_ids[token_idx][2], output[0][start_i:])\n",
    "                assert False\n",
    "                results[(context_text, question)][(str([tt[0] for tt in targs_2_tokens[:token_idx]]), \n",
    "                                              str(targs_2_tokens[token_idx][1:]))] = -perplex\n",
    "                total_perplex += perplex / len(raw_targ)\n",
    "                #print(perplex)\n",
    "            print(\"Perplexity\", -total_perplex)\n",
    "            total_odds /= len(raw_targ)\n",
    "            total_total_odds += total_odds\n",
    "            total_total_perlex += -total_perplex\n",
    "            if -total_perplex > max_perplex:\n",
    "                max_perplex = -total_perplex\n",
    "            #print(\"Total Odds:\", total_odds)\n",
    "            #print(\"Min odds:\", min_odds)\n",
    "    #return (total_total_perlex / len(examples)), max_perplex, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As in the House of Commons, a number of qualifications apply to being an MSP. Such qualifications were introduced under the House of Commons Disqualification Act 1975 and the British Nationality Act 1981. Specifically, members must be over the age of 18 and must be a citizen of the United Kingdom, the Republic of Ireland, one of the countries in the Commonwealth of Nations, a citizen of a British overseas territory, or a European Union citizen resident in the UK. Members of the police and the armed forces are disqualified from sitting in the Scottish Parliament as elected MSPs, and similarly, civil servants and members of foreign legislatures are disqualified. An individual may not sit in the Scottish Parliament if he or she is judged to be insane under the terms of the Mental Health (Care and Treatment) (Scotland) Act 2003.\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "The House of Lords introduced qualifications for which position?\n",
      "('NNP', ['house']) 9.108257293701172\n",
      "('VBD', ['introduced']) 4.0582075119018555\n",
      "('NNS', ['qualifications']) 0.14187145233154297\n",
      "('NP', ['lords']) 6.993594169616699\n",
      "('WHNP', ['which', 'position']) 7.222714424133301\n"
     ]
    }
   ],
   "source": [
    "print(context_text)\n",
    "#print(raw_targ)\n",
    "print(\"~\"*20)\n",
    "print(question)\n",
    "for step in range(len(targs_2_tokens)):\n",
    "    with torch.no_grad():\n",
    "        input_ids, input_mask, segment_ids = build_input(context_text, targs_2_tokens[:step], targs_2_tokens[step], multihint)\n",
    "        output, _ = model(input_ids, segment_ids, input_mask, None, None)\n",
    "        start_i = np.where(input_mask.data.numpy() == 0)[1][0]\n",
    "\n",
    "    perplex = perplexity(targs_2_ids[step][2], output[0][start_i:])\n",
    "    print(raw_targ[step], -perplex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-bb70c3df58ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mtotal_odds\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0modds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mperplex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargs_2_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                 results[(context_text, question)][(str([tt[0] for tt in targs_2_tokens[:token_idx]]), \n\u001b[1;32m     61\u001b[0m                                               str(targs_2_tokens[token_idx][1:]))] = -perplex\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "contexts = random.sample(range(1203), 1)\n",
    "ans_e, unans_e = get_examples(contexts)\n",
    "for context in contexts:\n",
    "    #avg_ans_odds, max_ent_ans, r = get_avg_odds([e for e in ans_e if e[0] == context], eval_dataloader_ans)\n",
    "    examples = [e for e in ans_e if e[0] == context]\n",
    "    dataloader = eval_dataloader_ans\n",
    "    multihint = False\n",
    "    total_total_odds = 0\n",
    "    total_total_perlex = 0\n",
    "    max_perplex = 0\n",
    "    results = {}\n",
    "    for example in examples:\n",
    "        cid, qid, targetid, _ = example\n",
    "        context_text = dataloader.dataset.contexts[cid]\n",
    "        question = dataloader.dataset.questions[qid]\n",
    "        raw_targ = dataloader.dataset.raw_targets[targetid]\n",
    "        results[(context_text, question)] = {}\n",
    "\n",
    "        raw_targ_copy = list(raw_targ)\n",
    "        raw_targ = [(tag,word) for (word,(_,tag)) in raw_targ if word]\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            targs_2_tokens = []#[tokenizer.tokenize(t) for _, t in raw_targ]            \n",
    "            for tidx, (tag, words) in enumerate(raw_targ):\n",
    "                clean_tag = tag\n",
    "                if multihint:\n",
    "                    span = tokenizer.tokenize(clean_tag) + [\"[SEP]\"] + words\n",
    "                else:\n",
    "                    span = words\n",
    "                targs_2_tokens.append((span, tokenizer.tokenize(clean_tag), words))\n",
    "\n",
    "            targs_2_ids = [list(map(tokenizer.convert_tokens_to_ids, t)) for t in targs_2_tokens]\n",
    "\n",
    "            total_odds = 0\n",
    "            min_odds = 100\n",
    "            total_perplex = 0\n",
    "            for token_idx in range(len(raw_targ)):\n",
    "                odds = 0\n",
    "                odds_list = []\n",
    "                input_ids, input_mask, segment_ids = build_input(context_text, targs_2_tokens[:token_idx], targs_2_tokens[token_idx], multihint)\n",
    "                output, _ = model(input_ids, segment_ids, input_mask, None, None)\n",
    "\n",
    "                #print(input_mask)\n",
    "                start_i = np.where(input_mask.data.numpy() == 0)[1][0]\n",
    "                for t_i, t in enumerate(targs_2_ids[token_idx][2]):\n",
    "                    odds += output[0][start_i+t_i][t]\n",
    "                    odds_list.append(output[0][start_i+t_i][t])\n",
    "                if len(targs_2_ids[token_idx]) == 0:\n",
    "                    print(token_idx, targs_2_ids, targs_2_ids[token_idx], raw_targ)\n",
    "                odds = odds/len(targs_2_ids[token_idx])\n",
    "                if odds < min_odds:\n",
    "                    min_odds = odds\n",
    "                # print(odds)\n",
    "                total_odds += odds\n",
    "                perplex = perplexity(targs_2_ids[token_idx][2], output[0][start_i:])\n",
    "                assert False\n",
    "                results[(context_text, question)][(str([tt[0] for tt in targs_2_tokens[:token_idx]]), \n",
    "                                              str(targs_2_tokens[token_idx][1:]))] = -perplex\n",
    "                total_perplex += perplex / len(raw_targ)\n",
    "                #print(perplex)\n",
    "            print(\"Perplexity\", -total_perplex)\n",
    "            total_odds /= len(raw_targ)\n",
    "            total_total_odds += total_odds\n",
    "            total_total_perlex += -total_perplex\n",
    "            if -total_perplex > max_perplex:\n",
    "                max_perplex = -total_perplex\n",
    "            #print(\"Total Odds:\", total_odds)\n",
    "            #print(\"Min odds:\", min_odds)\n",
    "    #return (total_total_perlex / len(examples)), max_perplex, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Victoria contains many topographically, geologically and climatically diverse areas, ranging from the wet, temperate climate of Gippsland in the southeast to the snow-covered Victorian alpine areas which rise to almost 2,000 m (6,600 ft), with Mount Bogong the highest peak at 1,986 m (6,516 ft). There are extensive semi-arid plains to the west and northwest. There is an extensive series of river systems in Victoria. Most notable is the Murray River system. Other rivers include: Ovens River, Goulburn River, Patterson River, King River, Campaspe River, Loddon River, Wimmera River, Elgin River, Barwon River, Thomson River, Snowy River, Latrobe River, Yarra River, Maribyrnong River, Mitta River, Hopkins River, Merri River and Kiewa River. The state symbols include the pink heath (state flower), Leadbeater's possum (state animal) and the helmeted honeyeater (state bird).\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "How high is Victoria's Mount Bogong?\n",
      "('NP', ['victoria', \"'\", 's'])\n",
      "['state'] tensor(7.3829)\n",
      "['highest'] tensor(6.1930)\n",
      "['which'] tensor(5.6877)\n",
      "['most'] tensor(5.3977)\n",
      "['melbourne'] tensor(4.8965)\n",
      "['victoria'] tensor(4.6781)\n",
      "['southern'] tensor(4.6694)\n",
      "['victorian'] tensor(4.6679)\n",
      "['australian'] tensor(4.5393)\n",
      "['high'] tensor(4.4581)\n",
      "['eastern'] tensor(4.3021)\n",
      "['western'] tensor(4.2257)\n",
      "['northern'] tensor(4.2054)\n",
      "['pink'] tensor(4.2001)\n",
      "['many'] tensor(3.9946)\n",
      "['queen'] tensor(3.9877)\n",
      "['first'] tensor(3.7905)\n",
      "['st'] tensor(3.6604)\n",
      "['red'] tensor(3.6560)\n",
      "['north'] tensor(3.5849)\n",
      "['wa'] tensor(3.4109)\n",
      "['blue'] tensor(3.2639)\n",
      "['rose'] tensor(3.2561)\n",
      "['australia'] tensor(3.2050)\n",
      "['south'] tensor(3.1841)\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "['peak'] tensor(5.7998)\n",
      "[\"'\"] tensor(5.7973)\n",
      "['bog'] tensor(5.6373)\n",
      "['of'] tensor(5.5753)\n",
      "['bird'] tensor(5.5210)\n",
      "['river'] tensor(5.4738)\n",
      "['flower'] tensor(5.2808)\n",
      "['park'] tensor(5.1705)\n",
      "['mountain'] tensor(5.1319)\n",
      "['birds'] tensor(5.1124)\n",
      "['and'] tensor(4.8983)\n",
      "[','] tensor(4.5836)\n",
      "['victoria'] tensor(4.4258)\n",
      "['mountains'] tensor(4.3249)\n",
      "['forest'] tensor(4.2798)\n",
      "['peaks'] tensor(4.2034)\n",
      "['forests'] tensor(4.0988)\n",
      "['plant'] tensor(4.0892)\n",
      "['[SEP]'] tensor(4.0651)\n",
      "['valley'] tensor(3.9388)\n",
      "['southern'] tensor(3.8523)\n",
      "['symbol'] tensor(3.8400)\n",
      "['level'] tensor(3.8314)\n",
      "['-'] tensor(3.8206)\n",
      "['water'] tensor(3.7989)\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "['##ong'] tensor(8.5280)\n",
      "['[SEP]'] tensor(8.1214)\n",
      "['s'] tensor(6.5129)\n",
      "['of'] tensor(5.0094)\n",
      "['##land'] tensor(4.9993)\n",
      "['the'] tensor(4.7144)\n",
      "[\"'\"] tensor(4.6620)\n",
      "['birds'] tensor(4.5188)\n",
      "['and'] tensor(4.4560)\n",
      "['forests'] tensor(4.3757)\n",
      "['##y'] tensor(4.3058)\n",
      "['water'] tensor(4.2453)\n",
      "['river'] tensor(4.1657)\n",
      "['##s'] tensor(4.1528)\n",
      "['in'] tensor(4.1092)\n",
      "[','] tensor(3.9654)\n",
      "['##est'] tensor(3.9201)\n",
      "['##water'] tensor(3.7911)\n",
      "['south'] tensor(3.7875)\n",
      "['forest'] tensor(3.7234)\n",
      "['region'] tensor(3.5377)\n",
      "['##op'] tensor(3.5250)\n",
      "['##es'] tensor(3.4224)\n",
      "['valley'] tensor(3.3844)\n",
      "['species'] tensor(3.3502)\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "####################\n",
      "('NP', ['victoria', \"'\", 's']) 3.4457778930664062\n",
      "('NP', ['mount', 'bog', '##ong'])\n",
      "['state'] tensor(9.4240)\n",
      "['highest'] tensor(6.2870)\n",
      "['which'] tensor(5.5208)\n",
      "['southern'] tensor(4.8476)\n",
      "['northern'] tensor(4.8269)\n",
      "['most'] tensor(4.5212)\n",
      "['many'] tensor(4.5083)\n",
      "['high'] tensor(4.4856)\n",
      "['river'] tensor(4.4395)\n",
      "['australian'] tensor(4.3066)\n",
      "['water'] tensor(4.2270)\n",
      "['official'] tensor(4.1252)\n",
      "['range'] tensor(4.1006)\n",
      "['eastern'] tensor(3.9920)\n",
      "['national'] tensor(3.9609)\n",
      "['western'] tensor(3.9270)\n",
      "['red'] tensor(3.6614)\n",
      "['north'] tensor(3.6175)\n",
      "['main'] tensor(3.5694)\n",
      "['native'] tensor(3.5342)\n",
      "['wa'] tensor(3.4419)\n",
      "['first'] tensor(3.3853)\n",
      "['largest'] tensor(3.3522)\n",
      "['victoria'] tensor(3.3336)\n",
      "['major'] tensor(3.3099)\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "['birds'] tensor(8.0244)\n",
      "['bird'] tensor(7.4391)\n",
      "['symbols'] tensor(7.3263)\n",
      "['symbol'] tensor(7.0214)\n",
      "['flower'] tensor(6.8355)\n",
      "['animal'] tensor(6.3296)\n",
      "['[SEP]'] tensor(5.8158)\n",
      "['emblem'] tensor(5.7812)\n",
      "['park'] tensor(5.5740)\n",
      "['name'] tensor(5.4204)\n",
      "['peak'] tensor(5.4135)\n",
      "['animals'] tensor(5.2959)\n",
      "['mammal'] tensor(5.0642)\n",
      "['forests'] tensor(4.9858)\n",
      "['feature'] tensor(4.9563)\n",
      "['flowers'] tensor(4.8804)\n",
      "['mountain'] tensor(4.8708)\n",
      "['plant'] tensor(4.8000)\n",
      "['river'] tensor(4.7300)\n",
      "['range'] tensor(4.6723)\n",
      "[\"'\"] tensor(4.5973)\n",
      "['forest'] tensor(4.4885)\n",
      "['valley'] tensor(4.4423)\n",
      "['rivers'] tensor(4.3869)\n",
      "['##birds'] tensor(4.3689)\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "['[SEP]'] tensor(10.5081)\n",
      "['s'] tensor(6.4584)\n",
      "['area'] tensor(5.7161)\n",
      "['##land'] tensor(5.5546)\n",
      "['system'] tensor(5.1317)\n",
      "['areas'] tensor(5.0618)\n",
      "['region'] tensor(4.9968)\n",
      "['rivers'] tensor(4.8051)\n",
      "['water'] tensor(4.7584)\n",
      "['##s'] tensor(4.7433)\n",
      "['birds'] tensor(4.6515)\n",
      "['river'] tensor(4.3718)\n",
      "['land'] tensor(4.3272)\n",
      "[\"'\"] tensor(4.2912)\n",
      "['forests'] tensor(4.2704)\n",
      "['regions'] tensor(4.0512)\n",
      "['of'] tensor(3.6472)\n",
      "['valley'] tensor(3.6269)\n",
      "['##water'] tensor(3.5697)\n",
      "['and'] tensor(3.5326)\n",
      "['feature'] tensor(3.5081)\n",
      "['animals'] tensor(3.4612)\n",
      "['the'] tensor(3.4553)\n",
      "['##er'] tensor(3.3848)\n",
      "['##ong'] tensor(3.3781)\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "####################\n",
      "('NP', ['mount', 'bog', '##ong']) 8.748794555664062\n",
      "('WHADVP', ['how', 'high'])\n",
      "['how'] tensor(15.2280)\n",
      "['where'] tensor(12.9014)\n",
      "['when'] tensor(10.6720)\n",
      "['why'] tensor(10.3310)\n",
      "['what'] tensor(6.6405)\n",
      "['about'] tensor(5.9532)\n",
      "['approximately'] tensor(5.6089)\n",
      "['which'] tensor(5.4992)\n",
      "['at'] tensor(5.0039)\n",
      "['in'] tensor(4.7431)\n",
      "['as'] tensor(4.4067)\n",
      "['who'] tensor(4.4021)\n",
      "['that'] tensor(4.1482)\n",
      "['.'] tensor(3.9665)\n",
      "['and'] tensor(3.9567)\n",
      "['on'] tensor(3.8250)\n",
      "['high'] tensor(3.6311)\n",
      "['[SEP]'] tensor(3.4314)\n",
      "['so'] tensor(3.3807)\n",
      "['up'] tensor(3.3468)\n",
      "['whereby'] tensor(3.1883)\n",
      "['far'] tensor(3.1161)\n",
      "['if'] tensor(3.0970)\n",
      "[\"'\"] tensor(3.0834)\n",
      "[','] tensor(3.0104)\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "['high'] tensor(11.0994)\n",
      "['tall'] tensor(11.0554)\n",
      "['[SEP]'] tensor(9.9883)\n",
      "['far'] tensor(9.5174)\n",
      "['long'] tensor(8.0485)\n",
      "['many'] tensor(7.6853)\n",
      "['big'] tensor(7.4185)\n",
      "['large'] tensor(7.0083)\n",
      "['much'] tensor(6.8264)\n",
      "['in'] tensor(6.5351)\n",
      "['deep'] tensor(6.5036)\n",
      "['low'] tensor(6.3079)\n",
      "['how'] tensor(5.6859)\n",
      "['close'] tensor(5.6024)\n",
      "['highest'] tensor(5.0193)\n",
      "['old'] tensor(4.4858)\n",
      "['well'] tensor(4.3130)\n",
      "['wide'] tensor(4.1668)\n",
      "[','] tensor(4.1196)\n",
      "['often'] tensor(3.8402)\n",
      "['great'] tensor(3.7192)\n",
      "['at'] tensor(3.5714)\n",
      "['level'] tensor(3.5632)\n",
      "['higher'] tensor(3.5124)\n",
      "['thick'] tensor(3.4353)\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "####################\n",
      "('WHADVP', ['how', 'high']) 0.5511116981506348\n"
     ]
    }
   ],
   "source": [
    "print(context_text)\n",
    "print(\"~\"*20)\n",
    "#print(raw_targ)\n",
    "print(question)\n",
    "for step in range(len(targs_2_tokens)):\n",
    "    with torch.no_grad():\n",
    "        input_ids, input_mask, segment_ids = build_input(context_text, targs_2_tokens[:step], targs_2_tokens[step], multihint)\n",
    "        output, _ = model(input_ids, segment_ids, input_mask, None, None)\n",
    "        start_i = np.where(input_mask.data.numpy() == 0)[1][0]\n",
    "        print(raw_targ[step])\n",
    "        for j in range(len(raw_targ[step][1])):\n",
    "            c = Counter()\n",
    "            for i, o in enumerate(output[0][start_i+j]):\n",
    "                c[i] = o\n",
    "            for x, val in c.most_common(25):\n",
    "                print(tokenizer.convert_ids_to_tokens([x]), val)\n",
    "            print(\"~\"*20)\n",
    "        print(\"#\"*20)\n",
    "\n",
    "    perplex = perplexity(targs_2_ids[step][2], output[0][start_i:])\n",
    "    print(raw_targ[step], -perplex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger drugs (>500 Da) can provoke a neutralizing immune response, particularly if the drugs are administered repeatedly, or in larger doses. This limits the effectiveness of drugs based on larger peptides and proteins (which are typically larger than 6000 Da). In some cases, the drug itself is not immunogenic, but may be co-administered with an immunogenic compound, as is sometimes the case for Taxol. Computational methods have been developed to predict the immunogenicity of peptides and proteins, which are particularly useful in designing therapeutic antibodies, assessing likely virulence of mutations in viral coat particles, and validation of proposed peptide-based drug treatments. Early techniques relied mainly on the observation that hydrophilic amino acids are overrepresented in epitope regions than hydrophobic amino acids; however, more recent developments rely on machine learning techniques using databases of existing known epitopes, usually on well-studied virus proteins, as a training set. A publicly accessible database has been established for the cataloguing of epitopes from pathogens known to be recognizable by B cells. The emerging field of bioinformatics-based studies of immunogenicity is referred to as immunoinformatics. Immunoproteomics is the study of large sets of proteins (proteomics) involved in the immune response.\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "What is the field of studying immunogenicity through bioinformatics known as?\n",
      "[['im'], ['n', '##n'], ['im']]\n",
      "tensor([[  101,  3469,  5850,  1006,  1028,  3156,  4830,  1007,  2064, 27895,\n",
      "          1037,  8699,  6026, 11311,  3433,  1010,  3391,  2065,  1996,  5850,\n",
      "          2024,  8564,  8385,  1010,  2030,  1999,  3469, 21656,  1012,  2023,\n",
      "          6537,  1996, 12353,  1997,  5850,  2241,  2006,  3469, 25117,  2015,\n",
      "          1998,  8171,  1006,  2029,  2024,  4050,  3469,  2084, 25961,  4830,\n",
      "          1007,  1012,  1999,  2070,  3572,  1010,  1996,  4319,  2993,  2003,\n",
      "          2025, 10047, 23041, 24278,  1010,  2021,  2089,  2022,  2522,  1011,\n",
      "          8564,  2007,  2019, 10047, 23041, 24278,  7328,  1010,  2004,  2003,\n",
      "          2823,  1996,  2553,  2005,  4171,  4747,  1012, 15078,  4725,  2031,\n",
      "          2042,  2764,  2000, 16014,  1996, 10047, 23041, 24278,  3012,  1997,\n",
      "         25117,  2015,  1998,  8171,  1010,  2029,  2024,  3391,  6179,  1999,\n",
      "         12697, 17261, 22931,  1010, 20077,  3497,  6819,  6820, 22717,  1997,\n",
      "         14494,  1999, 13434,  5435,  9309,  1010,  1998, 27354,  1997,  3818,\n",
      "         25117,  1011,  2241,  4319, 13441,  1012,  2220,  5461, 13538,  3701,\n",
      "          2006,  1996,  8089,  2008, 18479, 21850, 10415, 13096, 12737,  2024,\n",
      "          2058,  2890, 28994, 14088,  1999,  4958,  9956,  5051,  4655,  2084,\n",
      "         18479, 20200, 13096, 12737,  1025,  2174,  1010,  2062,  3522,  8973,\n",
      "         11160,  2006,  3698,  4083,  5461,  2478, 17881,  1997,  4493,  2124,\n",
      "          4958,  9956, 10374,  1010,  2788,  2006,  2092,  1011,  3273,  7865,\n",
      "          8171,  1010,  2004,  1037,  2731,  2275,  1012,  1037,  7271,  7801,\n",
      "          7809,  2038,  2042,  2511,  2005,  1996, 12105, 25165,  1997,  4958,\n",
      "          9956, 10374,  2013, 26835,  2015,  2124,  2000,  2022, 20123,  2011,\n",
      "          1038,  4442,  1012,  1996,  8361,  2492,  1997, 16012,  2378, 14192,\n",
      "         17592,  1011,  2241,  2913,  1997, 10047, 23041, 24278,  3012,  2003,\n",
      "          3615,  2000,  2004, 10047, 23041, 28765, 14192, 17592,  1012, 10047,\n",
      "         23041,   102,  1050,  2078,   102,     0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])\n",
      "['[CLS]', 'larger', 'drugs', '(', '>', '500', 'da', ')', 'can', 'provoke', 'a', 'neutral', '##izing', 'immune', 'response', ',', 'particularly', 'if', 'the', 'drugs', 'are', 'administered', 'repeatedly', ',', 'or', 'in', 'larger', 'doses', '.', 'this', 'limits', 'the', 'effectiveness', 'of', 'drugs', 'based', 'on', 'larger', 'peptide', '##s', 'and', 'proteins', '(', 'which', 'are', 'typically', 'larger', 'than', '6000', 'da', ')', '.', 'in', 'some', 'cases', ',', 'the', 'drug', 'itself', 'is', 'not', 'im', '##mun', '##ogenic', ',', 'but', 'may', 'be', 'co', '-', 'administered', 'with', 'an', 'im', '##mun', '##ogenic', 'compound', ',', 'as', 'is', 'sometimes', 'the', 'case', 'for', 'tax', '##ol', '.', 'computational', 'methods', 'have', 'been', 'developed', 'to', 'predict', 'the', 'im', '##mun', '##ogenic', '##ity', 'of', 'peptide', '##s', 'and', 'proteins', ',', 'which', 'are', 'particularly', 'useful', 'in', 'designing', 'therapeutic', 'antibodies', ',', 'assessing', 'likely', 'vi', '##ru', '##lence', 'of', 'mutations', 'in', 'viral', 'coat', 'particles', ',', 'and', 'validation', 'of', 'proposed', 'peptide', '-', 'based', 'drug', 'treatments', '.', 'early', 'techniques', 'relied', 'mainly', 'on', 'the', 'observation', 'that', 'hydro', '##phi', '##lic', 'amino', 'acids', 'are', 'over', '##re', '##pres', '##ented', 'in', 'ep', '##ito', '##pe', 'regions', 'than', 'hydro', '##phobic', 'amino', 'acids', ';', 'however', ',', 'more', 'recent', 'developments', 'rely', 'on', 'machine', 'learning', 'techniques', 'using', 'databases', 'of', 'existing', 'known', 'ep', '##ito', '##pes', ',', 'usually', 'on', 'well', '-', 'studied', 'virus', 'proteins', ',', 'as', 'a', 'training', 'set', '.', 'a', 'publicly', 'accessible', 'database', 'has', 'been', 'established', 'for', 'the', 'catalog', '##uing', 'of', 'ep', '##ito', '##pes', 'from', 'pathogen', '##s', 'known', 'to', 'be', 'recognizable', 'by', 'b', 'cells', '.', 'the', 'emerging', 'field', 'of', 'bio', '##in', '##form', '##atics', '-', 'based', 'studies', 'of', 'im', '##mun', '##ogenic', '##ity', 'is', 'referred', 'to', 'as', 'im', '##mun', '##oin', '##form', '##atics', '.', 'im', '##mun', '[SEP]', 'n', '##n', '[SEP]', '[PAD]']\n",
      "[10047]\n",
      "tensor(-23.5859)\n",
      "im 23.58592987060547\n",
      "[['bio'], ['n', '##ns'], ['bio']]\n",
      "tensor([[  101,  3469,  5850,  1006,  1028,  3156,  4830,  1007,  2064, 27895,\n",
      "          1037,  8699,  6026, 11311,  3433,  1010,  3391,  2065,  1996,  5850,\n",
      "          2024,  8564,  8385,  1010,  2030,  1999,  3469, 21656,  1012,  2023,\n",
      "          6537,  1996, 12353,  1997,  5850,  2241,  2006,  3469, 25117,  2015,\n",
      "          1998,  8171,  1006,  2029,  2024,  4050,  3469,  2084, 25961,  4830,\n",
      "          1007,  1012,  1999,  2070,  3572,  1010,  1996,  4319,  2993,  2003,\n",
      "          2025, 10047, 23041, 24278,  1010,  2021,  2089,  2022,  2522,  1011,\n",
      "          8564,  2007,  2019, 10047, 23041, 24278,  7328,  1010,  2004,  2003,\n",
      "          2823,  1996,  2553,  2005,  4171,  4747,  1012, 15078,  4725,  2031,\n",
      "          2042,  2764,  2000, 16014,  1996, 10047, 23041, 24278,  3012,  1997,\n",
      "         25117,  2015,  1998,  8171,  1010,  2029,  2024,  3391,  6179,  1999,\n",
      "         12697, 17261, 22931,  1010, 20077,  3497,  6819,  6820, 22717,  1997,\n",
      "         14494,  1999, 13434,  5435,  9309,  1010,  1998, 27354,  1997,  3818,\n",
      "         25117,  1011,  2241,  4319, 13441,  1012,  2220,  5461, 13538,  3701,\n",
      "          2006,  1996,  8089,  2008, 18479, 21850, 10415, 13096, 12737,  2024,\n",
      "          2058,  2890, 28994, 14088,  1999,  4958,  9956,  5051,  4655,  2084,\n",
      "         18479, 20200, 13096, 12737,  1025,  2174,  1010,  2062,  3522,  8973,\n",
      "         11160,  2006,  3698,  4083,  5461,  2478, 17881,  1997,  4493,  2124,\n",
      "          4958,  9956, 10374,  1010,  2788,  2006,  2092,  1011,  3273,  7865,\n",
      "          8171,  1010,  2004,  1037,  2731,  2275,  1012,  1037,  7271,  7801,\n",
      "          7809,  2038,  2042,  2511,  2005,  1996, 12105, 25165,  1997,  4958,\n",
      "          9956, 10374,  2013, 26835,  2015,  2124,  2000,  2022, 20123,  2011,\n",
      "          1038,  4442,  1012,  1996,  8361,  2492,  1997, 16012,  2378, 14192,\n",
      "         17592,  1011,  2241,  2913,  1997, 10047, 23041, 24278,  3012,  2003,\n",
      "          3615,  2000,  2004, 10047, 23041, 28765,   102, 10047, 23041, 24278,\n",
      "          3012,   102,  1050,  3619,   102,     0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])\n",
      "['[CLS]', 'larger', 'drugs', '(', '>', '500', 'da', ')', 'can', 'provoke', 'a', 'neutral', '##izing', 'immune', 'response', ',', 'particularly', 'if', 'the', 'drugs', 'are', 'administered', 'repeatedly', ',', 'or', 'in', 'larger', 'doses', '.', 'this', 'limits', 'the', 'effectiveness', 'of', 'drugs', 'based', 'on', 'larger', 'peptide', '##s', 'and', 'proteins', '(', 'which', 'are', 'typically', 'larger', 'than', '6000', 'da', ')', '.', 'in', 'some', 'cases', ',', 'the', 'drug', 'itself', 'is', 'not', 'im', '##mun', '##ogenic', ',', 'but', 'may', 'be', 'co', '-', 'administered', 'with', 'an', 'im', '##mun', '##ogenic', 'compound', ',', 'as', 'is', 'sometimes', 'the', 'case', 'for', 'tax', '##ol', '.', 'computational', 'methods', 'have', 'been', 'developed', 'to', 'predict', 'the', 'im', '##mun', '##ogenic', '##ity', 'of', 'peptide', '##s', 'and', 'proteins', ',', 'which', 'are', 'particularly', 'useful', 'in', 'designing', 'therapeutic', 'antibodies', ',', 'assessing', 'likely', 'vi', '##ru', '##lence', 'of', 'mutations', 'in', 'viral', 'coat', 'particles', ',', 'and', 'validation', 'of', 'proposed', 'peptide', '-', 'based', 'drug', 'treatments', '.', 'early', 'techniques', 'relied', 'mainly', 'on', 'the', 'observation', 'that', 'hydro', '##phi', '##lic', 'amino', 'acids', 'are', 'over', '##re', '##pres', '##ented', 'in', 'ep', '##ito', '##pe', 'regions', 'than', 'hydro', '##phobic', 'amino', 'acids', ';', 'however', ',', 'more', 'recent', 'developments', 'rely', 'on', 'machine', 'learning', 'techniques', 'using', 'databases', 'of', 'existing', 'known', 'ep', '##ito', '##pes', ',', 'usually', 'on', 'well', '-', 'studied', 'virus', 'proteins', ',', 'as', 'a', 'training', 'set', '.', 'a', 'publicly', 'accessible', 'database', 'has', 'been', 'established', 'for', 'the', 'catalog', '##uing', 'of', 'ep', '##ito', '##pes', 'from', 'pathogen', '##s', 'known', 'to', 'be', 'recognizable', 'by', 'b', 'cells', '.', 'the', 'emerging', 'field', 'of', 'bio', '##in', '##form', '##atics', '-', 'based', 'studies', 'of', 'im', '##mun', '##ogenic', '##ity', 'is', 'referred', 'to', 'as', 'im', '##mun', '##oin', '[SEP]', 'im', '##mun', '##ogenic', '##ity', '[SEP]', 'n', '##ns', '[SEP]', '[PAD]']\n",
      "[16012]\n",
      "tensor(-27.9732)\n",
      "bio 27.973154067993164\n",
      "[['field'], ['n', '##n'], ['field']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  3469,  5850,  1006,  1028,  3156,  4830,  1007,  2064, 27895,\n",
      "          1037,  8699,  6026, 11311,  3433,  1010,  3391,  2065,  1996,  5850,\n",
      "          2024,  8564,  8385,  1010,  2030,  1999,  3469, 21656,  1012,  2023,\n",
      "          6537,  1996, 12353,  1997,  5850,  2241,  2006,  3469, 25117,  2015,\n",
      "          1998,  8171,  1006,  2029,  2024,  4050,  3469,  2084, 25961,  4830,\n",
      "          1007,  1012,  1999,  2070,  3572,  1010,  1996,  4319,  2993,  2003,\n",
      "          2025, 10047, 23041, 24278,  1010,  2021,  2089,  2022,  2522,  1011,\n",
      "          8564,  2007,  2019, 10047, 23041, 24278,  7328,  1010,  2004,  2003,\n",
      "          2823,  1996,  2553,  2005,  4171,  4747,  1012, 15078,  4725,  2031,\n",
      "          2042,  2764,  2000, 16014,  1996, 10047, 23041, 24278,  3012,  1997,\n",
      "         25117,  2015,  1998,  8171,  1010,  2029,  2024,  3391,  6179,  1999,\n",
      "         12697, 17261, 22931,  1010, 20077,  3497,  6819,  6820, 22717,  1997,\n",
      "         14494,  1999, 13434,  5435,  9309,  1010,  1998, 27354,  1997,  3818,\n",
      "         25117,  1011,  2241,  4319, 13441,  1012,  2220,  5461, 13538,  3701,\n",
      "          2006,  1996,  8089,  2008, 18479, 21850, 10415, 13096, 12737,  2024,\n",
      "          2058,  2890, 28994, 14088,  1999,  4958,  9956,  5051,  4655,  2084,\n",
      "         18479, 20200, 13096, 12737,  1025,  2174,  1010,  2062,  3522,  8973,\n",
      "         11160,  2006,  3698,  4083,  5461,  2478, 17881,  1997,  4493,  2124,\n",
      "          4958,  9956, 10374,  1010,  2788,  2006,  2092,  1011,  3273,  7865,\n",
      "          8171,  1010,  2004,  1037,  2731,  2275,  1012,  1037,  7271,  7801,\n",
      "          7809,  2038,  2042,  2511,  2005,  1996, 12105, 25165,  1997,  4958,\n",
      "          9956, 10374,  2013, 26835,  2015,  2124,  2000,  2022, 20123,  2011,\n",
      "          1038,  4442,  1012,  1996,  8361,  2492,  1997, 16012,  2378, 14192,\n",
      "         17592,  1011,  2241,  2913,  1997, 10047, 23041, 24278,  3012,  2003,\n",
      "          3615,   102, 10047, 23041, 24278,  3012,   102, 16012,  2378, 14192,\n",
      "         17592,   102,  1050,  2078,   102,     0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])\n",
      "['[CLS]', 'larger', 'drugs', '(', '>', '500', 'da', ')', 'can', 'provoke', 'a', 'neutral', '##izing', 'immune', 'response', ',', 'particularly', 'if', 'the', 'drugs', 'are', 'administered', 'repeatedly', ',', 'or', 'in', 'larger', 'doses', '.', 'this', 'limits', 'the', 'effectiveness', 'of', 'drugs', 'based', 'on', 'larger', 'peptide', '##s', 'and', 'proteins', '(', 'which', 'are', 'typically', 'larger', 'than', '6000', 'da', ')', '.', 'in', 'some', 'cases', ',', 'the', 'drug', 'itself', 'is', 'not', 'im', '##mun', '##ogenic', ',', 'but', 'may', 'be', 'co', '-', 'administered', 'with', 'an', 'im', '##mun', '##ogenic', 'compound', ',', 'as', 'is', 'sometimes', 'the', 'case', 'for', 'tax', '##ol', '.', 'computational', 'methods', 'have', 'been', 'developed', 'to', 'predict', 'the', 'im', '##mun', '##ogenic', '##ity', 'of', 'peptide', '##s', 'and', 'proteins', ',', 'which', 'are', 'particularly', 'useful', 'in', 'designing', 'therapeutic', 'antibodies', ',', 'assessing', 'likely', 'vi', '##ru', '##lence', 'of', 'mutations', 'in', 'viral', 'coat', 'particles', ',', 'and', 'validation', 'of', 'proposed', 'peptide', '-', 'based', 'drug', 'treatments', '.', 'early', 'techniques', 'relied', 'mainly', 'on', 'the', 'observation', 'that', 'hydro', '##phi', '##lic', 'amino', 'acids', 'are', 'over', '##re', '##pres', '##ented', 'in', 'ep', '##ito', '##pe', 'regions', 'than', 'hydro', '##phobic', 'amino', 'acids', ';', 'however', ',', 'more', 'recent', 'developments', 'rely', 'on', 'machine', 'learning', 'techniques', 'using', 'databases', 'of', 'existing', 'known', 'ep', '##ito', '##pes', ',', 'usually', 'on', 'well', '-', 'studied', 'virus', 'proteins', ',', 'as', 'a', 'training', 'set', '.', 'a', 'publicly', 'accessible', 'database', 'has', 'been', 'established', 'for', 'the', 'catalog', '##uing', 'of', 'ep', '##ito', '##pes', 'from', 'pathogen', '##s', 'known', 'to', 'be', 'recognizable', 'by', 'b', 'cells', '.', 'the', 'emerging', 'field', 'of', 'bio', '##in', '##form', '##atics', '-', 'based', 'studies', 'of', 'im', '##mun', '##ogenic', '##ity', 'is', 'referred', '[SEP]', 'im', '##mun', '##ogenic', '##ity', '[SEP]', 'bio', '##in', '##form', '##atics', '[SEP]', 'n', '##n', '[SEP]', '[PAD]']\n",
      "[2492]\n",
      "tensor(-25.9523)\n",
      "field 25.952255249023438\n",
      "[['known'], ['v', '##bn'], ['known']]\n",
      "tensor([[  101,  3469,  5850,  1006,  1028,  3156,  4830,  1007,  2064, 27895,\n",
      "          1037,  8699,  6026, 11311,  3433,  1010,  3391,  2065,  1996,  5850,\n",
      "          2024,  8564,  8385,  1010,  2030,  1999,  3469, 21656,  1012,  2023,\n",
      "          6537,  1996, 12353,  1997,  5850,  2241,  2006,  3469, 25117,  2015,\n",
      "          1998,  8171,  1006,  2029,  2024,  4050,  3469,  2084, 25961,  4830,\n",
      "          1007,  1012,  1999,  2070,  3572,  1010,  1996,  4319,  2993,  2003,\n",
      "          2025, 10047, 23041, 24278,  1010,  2021,  2089,  2022,  2522,  1011,\n",
      "          8564,  2007,  2019, 10047, 23041, 24278,  7328,  1010,  2004,  2003,\n",
      "          2823,  1996,  2553,  2005,  4171,  4747,  1012, 15078,  4725,  2031,\n",
      "          2042,  2764,  2000, 16014,  1996, 10047, 23041, 24278,  3012,  1997,\n",
      "         25117,  2015,  1998,  8171,  1010,  2029,  2024,  3391,  6179,  1999,\n",
      "         12697, 17261, 22931,  1010, 20077,  3497,  6819,  6820, 22717,  1997,\n",
      "         14494,  1999, 13434,  5435,  9309,  1010,  1998, 27354,  1997,  3818,\n",
      "         25117,  1011,  2241,  4319, 13441,  1012,  2220,  5461, 13538,  3701,\n",
      "          2006,  1996,  8089,  2008, 18479, 21850, 10415, 13096, 12737,  2024,\n",
      "          2058,  2890, 28994, 14088,  1999,  4958,  9956,  5051,  4655,  2084,\n",
      "         18479, 20200, 13096, 12737,  1025,  2174,  1010,  2062,  3522,  8973,\n",
      "         11160,  2006,  3698,  4083,  5461,  2478, 17881,  1997,  4493,  2124,\n",
      "          4958,  9956, 10374,  1010,  2788,  2006,  2092,  1011,  3273,  7865,\n",
      "          8171,  1010,  2004,  1037,  2731,  2275,  1012,  1037,  7271,  7801,\n",
      "          7809,  2038,  2042,  2511,  2005,  1996, 12105, 25165,  1997,  4958,\n",
      "          9956, 10374,  2013, 26835,  2015,  2124,  2000,  2022, 20123,  2011,\n",
      "          1038,  4442,  1012,  1996,  8361,  2492,  1997, 16012,  2378, 14192,\n",
      "         17592,  1011,  2241,  2913,  1997, 10047, 23041, 24278,  3012,   102,\n",
      "         10047, 23041, 24278,  3012,   102, 16012,  2378, 14192, 17592,   102,\n",
      "          2492,   102,  1058, 24700,   102,     0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])\n",
      "['[CLS]', 'larger', 'drugs', '(', '>', '500', 'da', ')', 'can', 'provoke', 'a', 'neutral', '##izing', 'immune', 'response', ',', 'particularly', 'if', 'the', 'drugs', 'are', 'administered', 'repeatedly', ',', 'or', 'in', 'larger', 'doses', '.', 'this', 'limits', 'the', 'effectiveness', 'of', 'drugs', 'based', 'on', 'larger', 'peptide', '##s', 'and', 'proteins', '(', 'which', 'are', 'typically', 'larger', 'than', '6000', 'da', ')', '.', 'in', 'some', 'cases', ',', 'the', 'drug', 'itself', 'is', 'not', 'im', '##mun', '##ogenic', ',', 'but', 'may', 'be', 'co', '-', 'administered', 'with', 'an', 'im', '##mun', '##ogenic', 'compound', ',', 'as', 'is', 'sometimes', 'the', 'case', 'for', 'tax', '##ol', '.', 'computational', 'methods', 'have', 'been', 'developed', 'to', 'predict', 'the', 'im', '##mun', '##ogenic', '##ity', 'of', 'peptide', '##s', 'and', 'proteins', ',', 'which', 'are', 'particularly', 'useful', 'in', 'designing', 'therapeutic', 'antibodies', ',', 'assessing', 'likely', 'vi', '##ru', '##lence', 'of', 'mutations', 'in', 'viral', 'coat', 'particles', ',', 'and', 'validation', 'of', 'proposed', 'peptide', '-', 'based', 'drug', 'treatments', '.', 'early', 'techniques', 'relied', 'mainly', 'on', 'the', 'observation', 'that', 'hydro', '##phi', '##lic', 'amino', 'acids', 'are', 'over', '##re', '##pres', '##ented', 'in', 'ep', '##ito', '##pe', 'regions', 'than', 'hydro', '##phobic', 'amino', 'acids', ';', 'however', ',', 'more', 'recent', 'developments', 'rely', 'on', 'machine', 'learning', 'techniques', 'using', 'databases', 'of', 'existing', 'known', 'ep', '##ito', '##pes', ',', 'usually', 'on', 'well', '-', 'studied', 'virus', 'proteins', ',', 'as', 'a', 'training', 'set', '.', 'a', 'publicly', 'accessible', 'database', 'has', 'been', 'established', 'for', 'the', 'catalog', '##uing', 'of', 'ep', '##ito', '##pes', 'from', 'pathogen', '##s', 'known', 'to', 'be', 'recognizable', 'by', 'b', 'cells', '.', 'the', 'emerging', 'field', 'of', 'bio', '##in', '##form', '##atics', '-', 'based', 'studies', 'of', 'im', '##mun', '##ogenic', '##ity', '[SEP]', 'im', '##mun', '##ogenic', '##ity', '[SEP]', 'bio', '##in', '##form', '##atics', '[SEP]', 'field', '[SEP]', 'v', '##bn', '[SEP]', '[PAD]']\n",
      "[2124]\n",
      "tensor(-18.3575)\n",
      "known 18.357526779174805\n",
      "[['studying'], ['v', '##b', '##g'], ['studying']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  3469,  5850,  1006,  1028,  3156,  4830,  1007,  2064, 27895,\n",
      "          1037,  8699,  6026, 11311,  3433,  1010,  3391,  2065,  1996,  5850,\n",
      "          2024,  8564,  8385,  1010,  2030,  1999,  3469, 21656,  1012,  2023,\n",
      "          6537,  1996, 12353,  1997,  5850,  2241,  2006,  3469, 25117,  2015,\n",
      "          1998,  8171,  1006,  2029,  2024,  4050,  3469,  2084, 25961,  4830,\n",
      "          1007,  1012,  1999,  2070,  3572,  1010,  1996,  4319,  2993,  2003,\n",
      "          2025, 10047, 23041, 24278,  1010,  2021,  2089,  2022,  2522,  1011,\n",
      "          8564,  2007,  2019, 10047, 23041, 24278,  7328,  1010,  2004,  2003,\n",
      "          2823,  1996,  2553,  2005,  4171,  4747,  1012, 15078,  4725,  2031,\n",
      "          2042,  2764,  2000, 16014,  1996, 10047, 23041, 24278,  3012,  1997,\n",
      "         25117,  2015,  1998,  8171,  1010,  2029,  2024,  3391,  6179,  1999,\n",
      "         12697, 17261, 22931,  1010, 20077,  3497,  6819,  6820, 22717,  1997,\n",
      "         14494,  1999, 13434,  5435,  9309,  1010,  1998, 27354,  1997,  3818,\n",
      "         25117,  1011,  2241,  4319, 13441,  1012,  2220,  5461, 13538,  3701,\n",
      "          2006,  1996,  8089,  2008, 18479, 21850, 10415, 13096, 12737,  2024,\n",
      "          2058,  2890, 28994, 14088,  1999,  4958,  9956,  5051,  4655,  2084,\n",
      "         18479, 20200, 13096, 12737,  1025,  2174,  1010,  2062,  3522,  8973,\n",
      "         11160,  2006,  3698,  4083,  5461,  2478, 17881,  1997,  4493,  2124,\n",
      "          4958,  9956, 10374,  1010,  2788,  2006,  2092,  1011,  3273,  7865,\n",
      "          8171,  1010,  2004,  1037,  2731,  2275,  1012,  1037,  7271,  7801,\n",
      "          7809,  2038,  2042,  2511,  2005,  1996, 12105, 25165,  1997,  4958,\n",
      "          9956, 10374,  2013, 26835,  2015,  2124,  2000,  2022, 20123,  2011,\n",
      "          1038,  4442,  1012,  1996,  8361,  2492,  1997, 16012,  2378, 14192,\n",
      "         17592,  1011,  2241,  2913,  1997, 10047,   102, 10047, 23041, 24278,\n",
      "          3012,   102, 16012,  2378, 14192, 17592,   102,  2492,   102,  2124,\n",
      "           102,  1058,  2497,  2290,   102,     0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])\n",
      "['[CLS]', 'larger', 'drugs', '(', '>', '500', 'da', ')', 'can', 'provoke', 'a', 'neutral', '##izing', 'immune', 'response', ',', 'particularly', 'if', 'the', 'drugs', 'are', 'administered', 'repeatedly', ',', 'or', 'in', 'larger', 'doses', '.', 'this', 'limits', 'the', 'effectiveness', 'of', 'drugs', 'based', 'on', 'larger', 'peptide', '##s', 'and', 'proteins', '(', 'which', 'are', 'typically', 'larger', 'than', '6000', 'da', ')', '.', 'in', 'some', 'cases', ',', 'the', 'drug', 'itself', 'is', 'not', 'im', '##mun', '##ogenic', ',', 'but', 'may', 'be', 'co', '-', 'administered', 'with', 'an', 'im', '##mun', '##ogenic', 'compound', ',', 'as', 'is', 'sometimes', 'the', 'case', 'for', 'tax', '##ol', '.', 'computational', 'methods', 'have', 'been', 'developed', 'to', 'predict', 'the', 'im', '##mun', '##ogenic', '##ity', 'of', 'peptide', '##s', 'and', 'proteins', ',', 'which', 'are', 'particularly', 'useful', 'in', 'designing', 'therapeutic', 'antibodies', ',', 'assessing', 'likely', 'vi', '##ru', '##lence', 'of', 'mutations', 'in', 'viral', 'coat', 'particles', ',', 'and', 'validation', 'of', 'proposed', 'peptide', '-', 'based', 'drug', 'treatments', '.', 'early', 'techniques', 'relied', 'mainly', 'on', 'the', 'observation', 'that', 'hydro', '##phi', '##lic', 'amino', 'acids', 'are', 'over', '##re', '##pres', '##ented', 'in', 'ep', '##ito', '##pe', 'regions', 'than', 'hydro', '##phobic', 'amino', 'acids', ';', 'however', ',', 'more', 'recent', 'developments', 'rely', 'on', 'machine', 'learning', 'techniques', 'using', 'databases', 'of', 'existing', 'known', 'ep', '##ito', '##pes', ',', 'usually', 'on', 'well', '-', 'studied', 'virus', 'proteins', ',', 'as', 'a', 'training', 'set', '.', 'a', 'publicly', 'accessible', 'database', 'has', 'been', 'established', 'for', 'the', 'catalog', '##uing', 'of', 'ep', '##ito', '##pes', 'from', 'pathogen', '##s', 'known', 'to', 'be', 'recognizable', 'by', 'b', 'cells', '.', 'the', 'emerging', 'field', 'of', 'bio', '##in', '##form', '##atics', '-', 'based', 'studies', 'of', 'im', '[SEP]', 'im', '##mun', '##ogenic', '##ity', '[SEP]', 'bio', '##in', '##form', '##atics', '[SEP]', 'field', '[SEP]', 'known', '[SEP]', 'v', '##b', '##g', '[SEP]', '[PAD]']\n",
      "[5702]\n",
      "tensor(-26.1324)\n",
      "studying 26.132354736328125\n",
      "[['what'], ['w', '##hn', '##p'], ['what']]\n",
      "tensor([[  101,  3469,  5850,  1006,  1028,  3156,  4830,  1007,  2064, 27895,\n",
      "          1037,  8699,  6026, 11311,  3433,  1010,  3391,  2065,  1996,  5850,\n",
      "          2024,  8564,  8385,  1010,  2030,  1999,  3469, 21656,  1012,  2023,\n",
      "          6537,  1996, 12353,  1997,  5850,  2241,  2006,  3469, 25117,  2015,\n",
      "          1998,  8171,  1006,  2029,  2024,  4050,  3469,  2084, 25961,  4830,\n",
      "          1007,  1012,  1999,  2070,  3572,  1010,  1996,  4319,  2993,  2003,\n",
      "          2025, 10047, 23041, 24278,  1010,  2021,  2089,  2022,  2522,  1011,\n",
      "          8564,  2007,  2019, 10047, 23041, 24278,  7328,  1010,  2004,  2003,\n",
      "          2823,  1996,  2553,  2005,  4171,  4747,  1012, 15078,  4725,  2031,\n",
      "          2042,  2764,  2000, 16014,  1996, 10047, 23041, 24278,  3012,  1997,\n",
      "         25117,  2015,  1998,  8171,  1010,  2029,  2024,  3391,  6179,  1999,\n",
      "         12697, 17261, 22931,  1010, 20077,  3497,  6819,  6820, 22717,  1997,\n",
      "         14494,  1999, 13434,  5435,  9309,  1010,  1998, 27354,  1997,  3818,\n",
      "         25117,  1011,  2241,  4319, 13441,  1012,  2220,  5461, 13538,  3701,\n",
      "          2006,  1996,  8089,  2008, 18479, 21850, 10415, 13096, 12737,  2024,\n",
      "          2058,  2890, 28994, 14088,  1999,  4958,  9956,  5051,  4655,  2084,\n",
      "         18479, 20200, 13096, 12737,  1025,  2174,  1010,  2062,  3522,  8973,\n",
      "         11160,  2006,  3698,  4083,  5461,  2478, 17881,  1997,  4493,  2124,\n",
      "          4958,  9956, 10374,  1010,  2788,  2006,  2092,  1011,  3273,  7865,\n",
      "          8171,  1010,  2004,  1037,  2731,  2275,  1012,  1037,  7271,  7801,\n",
      "          7809,  2038,  2042,  2511,  2005,  1996, 12105, 25165,  1997,  4958,\n",
      "          9956, 10374,  2013, 26835,  2015,  2124,  2000,  2022, 20123,  2011,\n",
      "          1038,  4442,  1012,  1996,  8361,  2492,  1997, 16012,  2378, 14192,\n",
      "         17592,  1011,  2241,  2913,   102, 10047, 23041, 24278,  3012,   102,\n",
      "         16012,  2378, 14192, 17592,   102,  2492,   102,  2124,   102,  5702,\n",
      "           102,  1059,  7295,  2361,   102,     0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])\n",
      "['[CLS]', 'larger', 'drugs', '(', '>', '500', 'da', ')', 'can', 'provoke', 'a', 'neutral', '##izing', 'immune', 'response', ',', 'particularly', 'if', 'the', 'drugs', 'are', 'administered', 'repeatedly', ',', 'or', 'in', 'larger', 'doses', '.', 'this', 'limits', 'the', 'effectiveness', 'of', 'drugs', 'based', 'on', 'larger', 'peptide', '##s', 'and', 'proteins', '(', 'which', 'are', 'typically', 'larger', 'than', '6000', 'da', ')', '.', 'in', 'some', 'cases', ',', 'the', 'drug', 'itself', 'is', 'not', 'im', '##mun', '##ogenic', ',', 'but', 'may', 'be', 'co', '-', 'administered', 'with', 'an', 'im', '##mun', '##ogenic', 'compound', ',', 'as', 'is', 'sometimes', 'the', 'case', 'for', 'tax', '##ol', '.', 'computational', 'methods', 'have', 'been', 'developed', 'to', 'predict', 'the', 'im', '##mun', '##ogenic', '##ity', 'of', 'peptide', '##s', 'and', 'proteins', ',', 'which', 'are', 'particularly', 'useful', 'in', 'designing', 'therapeutic', 'antibodies', ',', 'assessing', 'likely', 'vi', '##ru', '##lence', 'of', 'mutations', 'in', 'viral', 'coat', 'particles', ',', 'and', 'validation', 'of', 'proposed', 'peptide', '-', 'based', 'drug', 'treatments', '.', 'early', 'techniques', 'relied', 'mainly', 'on', 'the', 'observation', 'that', 'hydro', '##phi', '##lic', 'amino', 'acids', 'are', 'over', '##re', '##pres', '##ented', 'in', 'ep', '##ito', '##pe', 'regions', 'than', 'hydro', '##phobic', 'amino', 'acids', ';', 'however', ',', 'more', 'recent', 'developments', 'rely', 'on', 'machine', 'learning', 'techniques', 'using', 'databases', 'of', 'existing', 'known', 'ep', '##ito', '##pes', ',', 'usually', 'on', 'well', '-', 'studied', 'virus', 'proteins', ',', 'as', 'a', 'training', 'set', '.', 'a', 'publicly', 'accessible', 'database', 'has', 'been', 'established', 'for', 'the', 'catalog', '##uing', 'of', 'ep', '##ito', '##pes', 'from', 'pathogen', '##s', 'known', 'to', 'be', 'recognizable', 'by', 'b', 'cells', '.', 'the', 'emerging', 'field', 'of', 'bio', '##in', '##form', '##atics', '-', 'based', 'studies', '[SEP]', 'im', '##mun', '##ogenic', '##ity', '[SEP]', 'bio', '##in', '##form', '##atics', '[SEP]', 'field', '[SEP]', 'known', '[SEP]', 'studying', '[SEP]', 'w', '##hn', '##p', '[SEP]', '[PAD]']\n",
      "[2054]\n",
      "tensor(-17.6354)\n",
      "what 17.635421752929688\n"
     ]
    }
   ],
   "source": [
    "print(context_text)\n",
    "print(\"~\"*20)\n",
    "#print(raw_targ)\n",
    "print(question)\n",
    "for step in range(len(targs_2_tokens)):\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(targs_2_tokens[step][0])):\n",
    "            thetarg = [targs_2_tokens[step][0][:i+1], targs_2_tokens[step][1], targs_2_tokens[step][2][:i+1]]\n",
    "            print(thetarg)\n",
    "            input_ids, input_mask, segment_ids = build_input(context_text, targs_2_tokens[:step], thetarg, multihint)\n",
    "            output, _ = model(input_ids, segment_ids, input_mask, None, None)\n",
    "            start_i = np.where(input_mask.data.numpy() == 0)[1][0]\n",
    "            print(input_ids)\n",
    "            print(input_mask)\n",
    "            print(tokenizer.convert_ids_to_tokens(input_ids[0].numpy()))\n",
    "        \n",
    "            the_ids = targs_2_ids[step][2][i:i+1]\n",
    "            print(the_ids)\n",
    "            perplex = perplexity(the_ids, output[0][start_i:])\n",
    "            print(raw_targ[step][1][i], -perplex)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "add check for end token in perplexiy counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_odds(examples, dataloader, multihint=False):\n",
    "    if len(examples) == 0:\n",
    "        return 0\n",
    "    total_total_odds = 0\n",
    "    total_total_perlex = 0\n",
    "    max_perplex = 0\n",
    "    results = {}\n",
    "    for example in examples:\n",
    "        cid, qid, targetid, _ = example\n",
    "        context = dataloader.dataset.contexts[cid]\n",
    "        question = dataloader.dataset.questions[qid]\n",
    "        print(question)\n",
    "        raw_targ = dataloader.dataset.raw_targets[targetid]\n",
    "        results[(context, question)] = {}\n",
    "\n",
    "        raw_targ_copy = list(raw_targ)\n",
    "        raw_targ = [(tag,word) for (word,(_,tag)) in raw_targ if word]\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            targs_2_tokens = []            \n",
    "            for tidx, (tag, words) in enumerate(raw_targ):\n",
    "                clean_tag = tag\n",
    "                if multihint:\n",
    "                    span = tokenizer.tokenize(clean_tag) + [\"[SEP]\"] + words\n",
    "                else:\n",
    "                    span = words\n",
    "                targs_2_tokens.append((span, tokenizer.tokenize(clean_tag), words))\n",
    "                    \n",
    "            targs_2_ids = [list(map(tokenizer.convert_tokens_to_ids, t)) for t in targs_2_tokens]\n",
    "\n",
    "            total_odds = 0\n",
    "            total_perplex = 0\n",
    "            for token_idx in range(len(raw_targ)):\n",
    "                input_ids, input_mask, segment_ids = build_input(context, targs_2_tokens[:token_idx], targs_2_tokens[token_idx], multihint)\n",
    "                output, _ = model(input_ids, segment_ids, input_mask, None, None)\n",
    "                \n",
    "                start_i = np.where(input_mask.data.numpy() == 0)[1][0]\n",
    "                if len(targs_2_ids[token_idx]) == 0:\n",
    "                    print(token_idx, targs_2_ids, targs_2_ids[token_idx], raw_targ)\n",
    "                perplex = perplexity(targs_2_ids[token_idx][2], output[0][start_i:])\n",
    "                results[(context, question)][(str([tt[0] for tt in targs_2_tokens[:token_idx]]), \n",
    "                                              str(targs_2_tokens[token_idx][1:]))] = -perplex\n",
    "                \n",
    "                normalizers = []\n",
    "                for ij in range(len(targs_2_ids[token_idx][2])):\n",
    "                    normalizers.append(int(np.argmax(output[0][start_i+ij]).numpy()))\n",
    "                norm_perplex = perplexity(normalizers, output[0][start_i:])\n",
    "                #print(norm_perplex)\n",
    "                #assert False\n",
    "                print(\"Target\", tokenizer.convert_ids_to_tokens(targs_2_ids[token_idx][2]))\n",
    "                print(\"Normalized\", tokenizer.convert_ids_to_tokens(normalizers))\n",
    "                print(\"perplex\", perplex)\n",
    "                print(\"Norm\", np.abs(norm_perplex))\n",
    "                print(\"normalized perplex\", perplex / np.abs(norm_perplex))\n",
    "                print(\"~\"*20)\n",
    "                total_perplex += perplex / np.abs(norm_perplex) / len(raw_targ)\n",
    "                #print(\"Total\", total_perplex)\n",
    "\n",
    "            total_odds /= len(raw_targ)\n",
    "            total_total_odds += total_odds\n",
    "            total_total_perlex += -total_perplex\n",
    "            if -total_perplex > max_perplex:\n",
    "                max_perplex = -total_perplex\n",
    "    return (total_total_perlex / len(examples)), max_perplex, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic equilibrium was first described by Galileo who noticed that certain assumptions of Aristotelian physics were contradicted by observations and logic. Galileo realized that simple velocity addition demands that the concept of an \"absolute rest frame\" did not exist. Galileo concluded that motion in a constant velocity was completely equivalent to rest. This was contrary to Aristotle's notion of a \"natural state\" of rest that objects with mass naturally approached. Simple experiments showed that Galileo's understanding of the equivalence of constant velocity and rest were correct. For example, if a mariner dropped a cannonball from the crow's nest of a ship moving at a constant velocity, Aristotelian physics would have the cannonball fall straight down while the ship moved beneath it. Thus, in an Aristotelian universe, the falling cannonball would land behind the foot of the mast of a moving ship. However, when this experiment is actually conducted, the cannonball always falls at the foot of the mast, as if the cannonball knows to travel with the ship despite being separated from it. Since there is no forward horizontal force being applied on the cannonball as it falls, the only conclusion left is that the cannonball continues to move with the same velocity as the boat as it falls. Thus, no force is required to keep the cannonball moving at the constant forward velocity.\n",
      "####################\n",
      "Start Answerable\n",
      "Where will a canonball dropped from the crow's nest of a ship land according to Aristotle?\n",
      "Target ['from', 'the', 'crow', \"'\", 's', 'nest', 'of', 'a', 'ship']\n",
      "Normalized ['at', 'an', 'ari', 'of', 'of', 'the', 'the', \"'\", 'time']\n",
      "perplex -4.236351490020752\n",
      "Norm 2.098233938217163\n",
      "normalized perplex -2.0190081824814605\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['canon', '##ball']\n",
      "Normalized ['cannon', 'cannon']\n",
      "perplex -6.3143205642700195\n",
      "Norm 1.0456690788269043\n",
      "normalized perplex -6.038545742744742\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['dropped']\n",
      "Normalized ['dropped']\n",
      "perplex -0.07864761352539062\n",
      "Norm 0.07864761352539062\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['aristotle']\n",
      "Normalized ['galileo']\n",
      "perplex -6.495388507843018\n",
      "Norm 0.3210906982421875\n",
      "normalized perplex -20.229139440669105\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['according']\n",
      "Normalized ['moving']\n",
      "perplex -5.352364540100098\n",
      "Norm 0.0744180679321289\n",
      "normalized perplex -71.9229172273269\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['where']\n",
      "Normalized ['how']\n",
      "perplex -1.7417707443237305\n",
      "Norm 0.8638916015625\n",
      "normalized perplex -2.016191315175922\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Who had the idea of a natural state for objects at rest?\n",
      "Target ['natural', 'state']\n",
      "Normalized ['dynamic', 'equilibrium']\n",
      "perplex -5.972507476806641\n",
      "Norm 0.8511242866516113\n",
      "normalized perplex -7.017197805860935\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['objects']\n",
      "Normalized ['objects']\n",
      "perplex -0.5525016784667969\n",
      "Norm 0.5525016784667969\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['rest']\n",
      "Normalized ['rest']\n",
      "perplex -0.45050621032714844\n",
      "Norm 0.45050621032714844\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['idea']\n",
      "Normalized ['it']\n",
      "perplex -3.8831820487976074\n",
      "Norm 1.9398469924926758\n",
      "normalized perplex -2.001798112854135\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['who']\n",
      "Normalized ['what']\n",
      "perplex -0.8874330520629883\n",
      "Norm 0.7084417343139648\n",
      "normalized perplex -1.252654959581614\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "What does motion at a constant velocity equal?\n",
      "Target ['at', 'a', 'constant', 'velocity']\n",
      "Normalized ['at', 'an', 'ari', 'time']\n",
      "perplex -3.2445621490478516\n",
      "Norm 1.9027128219604492\n",
      "normalized perplex -1.7052295604466656\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['motion']\n",
      "Normalized ['cannon']\n",
      "perplex -5.499201774597168\n",
      "Norm 0.3947772979736328\n",
      "normalized perplex -13.929883513627118\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['what']\n",
      "Normalized ['what']\n",
      "perplex -0.01660442352294922\n",
      "Norm 0.01660442352294922\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Where does a canonball dropped from the crow's nest of a ship actually land?\n",
      "Target ['from', 'the', 'crow', \"'\", 's', 'nest', 'of', 'a', 'ship']\n",
      "Normalized ['at', 'an', 'ari', 'of', 'of', 'the', 'the', \"'\", 'time']\n",
      "perplex -4.236351490020752\n",
      "Norm 2.098233938217163\n",
      "normalized perplex -2.0190081824814605\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['canon', '##ball']\n",
      "Normalized ['cannon', 'cannon']\n",
      "perplex -6.3143205642700195\n",
      "Norm 1.0456690788269043\n",
      "normalized perplex -6.038545742744742\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['dropped']\n",
      "Normalized ['dropped']\n",
      "perplex -0.07864761352539062\n",
      "Norm 0.07864761352539062\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['actually']\n",
      "Normalized ['down']\n",
      "perplex -4.61863374710083\n",
      "Norm 1.1667156219482422\n",
      "normalized perplex -3.9586628139840934\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['land']\n",
      "Normalized ['fall']\n",
      "perplex -4.927979946136475\n",
      "Norm 0.2170705795288086\n",
      "normalized perplex -22.70220108516574\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['where']\n",
      "Normalized ['when']\n",
      "perplex -2.265727996826172\n",
      "Norm 0.45937347412109375\n",
      "normalized perplex -4.9322133829366726\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Who first described dynamic equilibrium?\n",
      "Target ['first', 'described']\n",
      "Normalized ['cannon', 'in']\n",
      "perplex -5.847304344177246\n",
      "Norm 1.2204432487487793\n",
      "normalized perplex -4.791131705773299\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['dynamic', 'equilibrium']\n",
      "Normalized ['dynamic', 'equilibrium']\n",
      "perplex -0.11042308807373047\n",
      "Norm 0.11042308807373047\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['who']\n",
      "Normalized ['who']\n",
      "perplex -0.0898141860961914\n",
      "Norm 0.0898141860961914\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Start Unanswerbale\n",
      "Who concluded that motion in a constant velocity was completely equivalent to motion?\n",
      "Target ['concluded', 'that', 'motion', 'in', 'a', 'constant', 'velocity', 'was', 'completely', 'equivalent']\n",
      "Normalized ['fall', 'a', 'the', 'the', 'the', 'of', 'of', 'of', \"'\", 'time']\n",
      "perplex -6.998946189880371\n",
      "Norm 2.4291915893554688\n",
      "normalized perplex -2.8811832794700987\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['who']\n",
      "Normalized ['what']\n",
      "perplex -3.3338279724121094\n",
      "Norm 0.047430992126464844\n",
      "normalized perplex -70.28796622097114\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "In what universe would a falling cannonball land in front of the mast of a moving ship?\n",
      "Target ['mast', 'of', 'a', 'moving', 'ship']\n",
      "Normalized ['ari', '##sto', '##tel', '##ian', 'physics']\n",
      "perplex -7.095129489898682\n",
      "Norm 0.8154349327087402\n",
      "normalized perplex -8.701036962360483\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['falling', 'cannon', '##ball']\n",
      "Normalized ['cannon', 'cannon', 'ball']\n",
      "perplex -2.244511365890503\n",
      "Norm 0.6374896168708801\n",
      "normalized perplex -3.5208594877320425\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['land']\n",
      "Normalized ['foot']\n",
      "perplex -4.563633918762207\n",
      "Norm 0.7461042404174805\n",
      "normalized perplex -6.116617051001666\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['front']\n",
      "Normalized ['it']\n",
      "perplex -7.018465042114258\n",
      "Norm 0.7290782928466797\n",
      "normalized perplex -9.626490201361946\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['what', 'universe']\n",
      "Normalized ['what', 'universe']\n",
      "perplex -0.6747260093688965\n",
      "Norm 0.6747260093688965\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Which physics were not contradicted by observations and logic?\n",
      "Target ['contra', '##dict', '##ed', 'by', 'observations', 'and', 'logic']\n",
      "Normalized ['fall', 'a', 'the', 'the', 'the', \"'\", 'time']\n",
      "perplex -6.4881110191345215\n",
      "Norm 2.420208692550659\n",
      "normalized perplex -2.6808064276046784\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['which', 'physics', 'were']\n",
      "Normalized ['what', 'the', 'of']\n",
      "perplex -4.15728759765625\n",
      "Norm 2.2668673992156982\n",
      "normalized perplex -1.8339350590575383\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "What type of equilibrium was first described by Aristotle?\n",
      "Target ['what', 'type', 'of', 'equilibrium', 'was', 'first', 'described', 'by']\n",
      "Normalized ['what', 'the', 'of', 'the', 'the', 'the', 'the', 'time']\n",
      "perplex -5.04275369644165\n",
      "Norm 2.655755043029785\n",
      "normalized perplex -1.89880226705272\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['aristotle']\n",
      "Normalized ['galileo']\n",
      "perplex -7.675585746765137\n",
      "Norm 0.0025529861450195312\n",
      "normalized perplex -3006.5128875607024\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "6.8484967909787 387.21019778698013 17.204300318066355 1504.2058449138776\n",
      "The Catholic Church in France and many of its members opposed the Huguenots. Some Huguenot preachers and congregants were attacked as they attempted to meet for worship. The height of this persecution was the St. Bartholomew's Day massacre when 5,000 to 30,000 were killed, although there were also underlying political reasons for this as well, as some of the Huguenots were nobles trying to establish separate centers of power in southern France. Retaliating against the French Catholics, the Huguenots had their own militia.\n",
      "####################\n",
      "Start Answerable\n",
      "What was a non-religious reason for the massacre?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target ['non', '-', 'religious', 'reason']\n",
      "Normalized ['catholic', '.', '##ots', 'day']\n",
      "perplex -7.427223205566406\n",
      "Norm 1.5164649486541748\n",
      "normalized perplex -4.897721646753447\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['massacre']\n",
      "Normalized ['persecution']\n",
      "perplex -4.476167678833008\n",
      "Norm 0.34348583221435547\n",
      "normalized perplex -13.031593326503245\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['what']\n",
      "Normalized ['what']\n",
      "perplex -0.04803466796875\n",
      "Norm 0.04803466796875\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "How many Huguenots were killed during this purge?\n",
      "Target ['how', 'many', 'hug', '##uen', '##ots', 'were']\n",
      "Normalized ['how', 'many', 'protestants', '##ots', '##ots', '[SEP]']\n",
      "perplex -2.8615729808807373\n",
      "Norm 1.2262383699417114\n",
      "normalized perplex -2.33361885504917\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['killed']\n",
      "Normalized ['attacked']\n",
      "perplex -2.116086006164551\n",
      "Norm 0.47498321533203125\n",
      "normalized perplex -4.455075332894293\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['purge']\n",
      "Normalized ['persecution']\n",
      "perplex -9.608264923095703\n",
      "Norm 2.8478708267211914\n",
      "normalized perplex -3.3738415496035272\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "What group specifically opposed the Huguenots?\n",
      "Target ['opposed', 'the', 'hug', '##uen', '##ots']\n",
      "Normalized ['people', 'church', '##ots', '##ots', '[SEP]']\n",
      "perplex -6.972914218902588\n",
      "Norm 1.3956732749938965\n",
      "normalized perplex -4.996093529793412\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['specifically']\n",
      "Normalized ['strongly']\n",
      "perplex -5.824065685272217\n",
      "Norm 2.3947792053222656\n",
      "normalized perplex -2.431984406883336\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['what', 'group']\n",
      "Normalized ['what', '[SEP]']\n",
      "perplex -1.0179009437561035\n",
      "Norm 0.759373664855957\n",
      "normalized perplex -1.3404480440458593\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "How did the Huguenots defend themselves?\n",
      "Target ['hug', '##uen', '##ots']\n",
      "Normalized ['hug', '[SEP]', '##ots']\n",
      "perplex -0.8847723007202148\n",
      "Norm 0.7446317672729492\n",
      "normalized perplex -1.188201121153479\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['defend']\n",
      "Normalized ['try']\n",
      "perplex -6.133404731750488\n",
      "Norm 1.878493309020996\n",
      "normalized perplex -3.2650660517641135\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['themselves']\n",
      "Normalized ['themselves']\n",
      "perplex -0.9858713150024414\n",
      "Norm 0.9858713150024414\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['how']\n",
      "Normalized ['how']\n",
      "perplex -0.4722871780395508\n",
      "Norm 0.4722871780395508\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "What event was the worst example of Huguenot persecution?\n",
      "Target ['hug', '##uen', '##ot', 'persecution']\n",
      "Normalized ['catholic', '.', '##ots', 'day']\n",
      "perplex -4.702678203582764\n",
      "Norm 1.5164649486541748\n",
      "normalized perplex -3.101079393728338\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['worst', 'example']\n",
      "Normalized ['place', '[SEP]']\n",
      "perplex -6.687450408935547\n",
      "Norm 1.7278409004211426\n",
      "normalized perplex -3.8704086743782677\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['what', 'event']\n",
      "Normalized ['what', '[SEP]']\n",
      "perplex -1.4063084125518799\n",
      "Norm 0.15459775924682617\n",
      "normalized perplex -9.096564008451182\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Start Unanswerbale\n",
      "How many French Catholics died during the Bartholomew's Day massacre?\n",
      "Target ['bartholomew', \"'\", 's', 'day', 'massacre']\n",
      "Normalized ['catholic', '.', '##ots', 'day', '[SEP]']\n",
      "perplex -4.518028259277344\n",
      "Norm 1.5419267416000366\n",
      "normalized perplex -2.9301186219710065\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['died']\n",
      "Normalized ['died']\n",
      "perplex -0.20462989807128906\n",
      "Norm 0.20462989807128906\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['how', 'many', 'french', 'catholics']\n",
      "Normalized ['how', 'many', 'people', '[SEP]']\n",
      "perplex -3.3183679580688477\n",
      "Norm 0.23196983337402344\n",
      "normalized perplex -14.305170244780832\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "How many Huguenots were there in Northern France during this time?\n",
      "Target ['how', 'many', 'hug', '##uen', '##ots', 'were']\n",
      "Normalized ['how', 'many', 'protestants', '##ots', '##ots', '[SEP]']\n",
      "perplex -2.8615729808807373\n",
      "Norm 1.2262383699417114\n",
      "normalized perplex -2.33361885504917\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['northern', 'france']\n",
      "Normalized ['catholic', 'church']\n",
      "perplex -4.912137508392334\n",
      "Norm 1.1639716625213623\n",
      "normalized perplex -4.220152145071815\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['there']\n",
      "Normalized ['first']\n",
      "perplex -3.7242350578308105\n",
      "Norm 2.8014822006225586\n",
      "normalized perplex -1.3293802320083252\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['time']\n",
      "Normalized ['attacks']\n",
      "perplex -3.9359593391418457\n",
      "Norm 2.473994255065918\n",
      "normalized perplex -1.590933095775105\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "How many French nobles were Huguenots?\n",
      "Target ['hug', '##uen', '##ots']\n",
      "Normalized ['hug', '[SEP]', '##ots']\n",
      "perplex -0.8847723007202148\n",
      "Norm 0.7446317672729492\n",
      "normalized perplex -1.188201121153479\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['how', 'many', 'french', 'nobles']\n",
      "Normalized ['what', '[SEP]', '[SEP]', '[SEP]']\n",
      "perplex -5.428681373596191\n",
      "Norm 0.519812822341919\n",
      "normalized perplex -10.443531094785788\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "How many French Catholics died after the Huguenots retaliated?\n",
      "Target ['hug', '##uen', '##ots']\n",
      "Normalized ['hug', '[SEP]', '##ots']\n",
      "perplex -0.8847723007202148\n",
      "Norm 0.7446317672729492\n",
      "normalized perplex -1.188201121153479\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['re', '##tal', '##iated']\n",
      "Normalized ['attacked', '[SEP]', '[SEP]']\n",
      "perplex -5.784820556640625\n",
      "Norm 0.6012179255485535\n",
      "normalized perplex -9.621836460319331\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['died']\n",
      "Normalized ['attacked']\n",
      "perplex -7.171735763549805\n",
      "Norm 0.9488058090209961\n",
      "normalized perplex -7.558697148945366\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['how', 'many', 'french', 'catholics']\n",
      "Normalized ['who', '[SEP]', '[SEP]', '[SEP]']\n",
      "perplex -3.6639113426208496\n",
      "Norm 0.43520641326904297\n",
      "normalized perplex -8.418789868236232\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "3.917891943184818 5.239924490464988 6.309771657752231 6.696881149663602\n",
      "The principle of faunal succession is based on the appearance of fossils in sedimentary rocks. As organisms exist at the same time period throughout the world, their presence or (sometimes) absence may be used to provide a relative age of the formations in which they are found. Based on principles laid out by William Smith almost a hundred years before the publication of Charles Darwin's theory of evolution, the principles of succession were developed independently of evolutionary thought. The principle becomes quite complex, however, given the uncertainties of fossilization, the localization of fossil types due to lateral changes in habitat (facies change in sedimentary strata), and that not all fossils may be found globally at the same time.\n",
      "####################\n",
      "Start Answerable\n",
      "The presence or absence of what can be used to determine the relative age of the formations in which they are found? \n",
      "Target ['relative', 'age', 'of', 'the', 'formations', 'in', 'which', 'they', 'are', 'found']\n",
      "Normalized ['principles', 'of', 'succession', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "perplex -6.757565498352051\n",
      "Norm 0.9065224528312683\n",
      "normalized perplex -7.454382930336356\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['presence', 'or', 'absence']\n",
      "Normalized ['organisms', 'of', 'succession']\n",
      "perplex -6.276908874511719\n",
      "Norm 1.141871452331543\n",
      "normalized perplex -5.497036344761175\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['determine']\n",
      "Normalized ['provide']\n",
      "perplex -7.004630088806152\n",
      "Norm 0.059510231018066406\n",
      "normalized perplex -117.70463614365154\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['what']\n",
      "Normalized ['what']\n",
      "perplex -0.020630836486816406\n",
      "Norm 0.020630836486816406\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Whose principles were the principle of faunal succession built upon?\n",
      "Target ['principle', 'of', 'fauna', '##l', 'succession']\n",
      "Normalized ['principles', 'of', 'succession', '[SEP]', '[SEP]']\n",
      "perplex -3.942295789718628\n",
      "Norm 0.9276749491691589\n",
      "normalized perplex -4.249652093386173\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['built']\n",
      "Normalized ['based']\n",
      "perplex -6.7629075050354\n",
      "Norm 0.09575080871582031\n",
      "normalized perplex -70.63029122925838\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['whose', 'principles']\n",
      "Normalized ['what', '[SEP]']\n",
      "perplex -4.682814598083496\n",
      "Norm 0.5448813438415527\n",
      "normalized perplex -8.594191471244834\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Which principle is based on the appearance of fossils in sedimentary rocks?\n",
      "Target ['based', 'on', 'the', 'appearance', 'of', 'fossils', 'in', 'sedimentary', 'rocks']\n",
      "Normalized ['based', 'on', 'the', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "perplex -3.501389741897583\n",
      "Norm 1.212471842765808\n",
      "normalized perplex -2.887811179111963\n",
      "~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target ['which', 'principle']\n",
      "Normalized ['what', '[SEP]']\n",
      "perplex -2.380352020263672\n",
      "Norm 0.34424877166748047\n",
      "normalized perplex -6.914627490851066\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "The fact that not all fossils may be found globally at the same time causes the principle to become what?\n",
      "Target ['fact', 'that', 'not', 'all', 'fossils', 'may', 'be', 'found', 'globally', 'at', 'the', 'same', 'time']\n",
      "Normalized ['principles', 'of', 'succession', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "perplex -7.518015384674072\n",
      "Norm 0.9021682143211365\n",
      "normalized perplex -8.333274510598036\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['principle']\n",
      "Normalized ['existence']\n",
      "perplex -6.338626861572266\n",
      "Norm 1.9399747848510742\n",
      "normalized perplex -3.2673759015166075\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['causes']\n",
      "Normalized ['describes']\n",
      "perplex -3.7770395278930664\n",
      "Norm 2.2571983337402344\n",
      "normalized perplex -1.673330815212156\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "The principle of faunal succession was developed 100 years before whose theory of evolution?\n",
      "Target ['principle', 'of', 'fauna', '##l', 'succession']\n",
      "Normalized ['principles', 'of', 'succession', '[SEP]', '[SEP]']\n",
      "perplex -3.942295789718628\n",
      "Norm 0.9276749491691589\n",
      "normalized perplex -4.249652093386173\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['whose', 'theory', 'of', 'evolution']\n",
      "Normalized ['based', 'on', 'on', 'on']\n",
      "perplex -5.333693504333496\n",
      "Norm 0.7238907814025879\n",
      "normalized perplex -7.368091487501886\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['100', 'years', 'before']\n",
      "Normalized ['based', 'on', 'on']\n",
      "perplex -10.341841697692871\n",
      "Norm 0.3420480191707611\n",
      "normalized perplex -30.23505799789444\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['developed']\n",
      "Normalized ['published']\n",
      "perplex -1.781794548034668\n",
      "Norm 1.0028057098388672\n",
      "normalized perplex -1.7768093365971862\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Start Unanswerbale\n",
      "What can changes in habitat be used to provide about formations?\n",
      "Target ['be', 'used', 'to', 'provide']\n",
      "Normalized ['based', 'on', 'the', '[SEP]']\n",
      "perplex -7.800359725952148\n",
      "Norm 1.0327363014221191\n",
      "normalized perplex -7.553099194064101\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['changes', 'in', 'habitat']\n",
      "Normalized ['relative', 'of', '[SEP]']\n",
      "perplex -8.712723731994629\n",
      "Norm 0.9656794667243958\n",
      "normalized perplex -9.022376505061626\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['formations']\n",
      "Normalized ['organisms']\n",
      "perplex -9.613029479980469\n",
      "Norm 0.43136024475097656\n",
      "normalized perplex -22.285385816048144\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['what']\n",
      "Normalized ['what']\n",
      "perplex -0.021596908569335938\n",
      "Norm 0.021596908569335938\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "What is the theory of evolution based on?\n",
      "Target ['theory', 'of', 'evolution']\n",
      "Normalized ['principles', 'of', 'succession']\n",
      "perplex -3.188722610473633\n",
      "Norm 0.9003005027770996\n",
      "normalized perplex -3.5418425299525915\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['based', 'on']\n",
      "Normalized ['based', 'on']\n",
      "perplex -0.21177339553833008\n",
      "Norm 0.21177339553833008\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['what']\n",
      "Normalized ['what']\n",
      "perplex -0.5687885284423828\n",
      "Norm 0.5687885284423828\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "What is one uncertainty about the theory of evolution that makes it complex?\n",
      "Target ['theory', 'of', 'evolution']\n",
      "Normalized ['principles', 'of', 'succession']\n",
      "perplex -3.188722610473633\n",
      "Norm 0.9003005027770996\n",
      "normalized perplex -3.5418425299525915\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['one', 'uncertainty']\n",
      "Normalized ['charles', 'smith']\n",
      "perplex -8.65058422088623\n",
      "Norm 0.9207777976989746\n",
      "normalized perplex -9.394866212569477\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['complex']\n",
      "Normalized ['complex']\n",
      "perplex -0.03846549987792969\n",
      "Norm 0.03846549987792969\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['makes']\n",
      "Normalized ['makes']\n",
      "perplex -0.3389577865600586\n",
      "Norm 0.3389577865600586\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['it']\n",
      "Normalized ['it']\n",
      "perplex -1.1433658599853516\n",
      "Norm 1.1433658599853516\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['what']\n",
      "Normalized ['what']\n",
      "perplex -0.041656494140625\n",
      "Norm 0.041656494140625\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['that']\n",
      "Normalized ['that']\n",
      "perplex -0.02019214630126953\n",
      "Norm 0.02019214630126953\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "The principles of succession are dependent on what thought?\n",
      "Target ['principles', 'of', 'succession']\n",
      "Normalized ['principles', 'of', 'succession']\n",
      "perplex -0.9003005027770996\n",
      "Norm 0.9003005027770996\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['thought']\n",
      "Normalized ['development']\n",
      "perplex -6.53094482421875\n",
      "Norm 2.227182388305664\n",
      "normalized perplex -2.932379879847733\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['what']\n",
      "Normalized ['what']\n",
      "perplex -0.8075971603393555\n",
      "Norm 0.8075971603393555\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "How much time after Charles Darwin, did William Smith develop the principles of succession?\n",
      "Target ['principles', 'of', 'succession']\n",
      "Normalized ['principles', 'of', 'succession']\n",
      "perplex -0.9003005027770996\n",
      "Norm 0.9003005027770996\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['charles', 'darwin']\n",
      "Normalized ['charles', 'smith']\n",
      "perplex -2.518123149871826\n",
      "Norm 1.0257127285003662\n",
      "normalized perplex -2.454998441477298\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['william', 'smith']\n",
      "Normalized ['his', 'smith']\n",
      "perplex -1.484246015548706\n",
      "Norm 1.4438579082489014\n",
      "normalized perplex -1.0279723559147085\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['develop']\n",
      "Normalized ['publish']\n",
      "perplex -3.110356330871582\n",
      "Norm 0.6315479278564453\n",
      "normalized perplex -4.924972743444081\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['how', 'much', 'time']\n",
      "Normalized ['what', '[SEP]', '[SEP]']\n",
      "perplex -5.60528039932251\n",
      "Norm 0.3228050768375397\n",
      "normalized perplex -17.364288239318856\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "When was Charles Darwin's theory of evolution published?\n",
      "Target ['charles', 'darwin', \"'\", 's', 'theory', 'of', 'evolution']\n",
      "Normalized ['principles', 'of', 'succession', '[SEP]', '[SEP]', '[SEP]', '[SEP]']\n",
      "perplex -3.661372423171997\n",
      "Norm 0.9161912798881531\n",
      "normalized perplex -3.996296956263293\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['published']\n",
      "Normalized ['published']\n",
      "perplex -0.7957601547241211\n",
      "Norm 0.7957601547241211\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['when']\n",
      "Normalized ['how']\n",
      "perplex -1.0626983642578125\n",
      "Norm 0.5129594802856445\n",
      "normalized perplex -2.071700407342199\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Why is it easy to use an organism's absence to show the age of a formation?\n",
      "Target ['organism', \"'\", 's']\n",
      "Normalized ['principles', 'of', 'succession']\n",
      "perplex -5.617847442626953\n",
      "Norm 0.9003005027770996\n",
      "normalized perplex -6.239969238379782\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['absence']\n",
      "Normalized ['absence']\n",
      "perplex -0.2429065704345703\n",
      "Norm 0.2429065704345703\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['age']\n",
      "Normalized ['age']\n",
      "perplex -1.4342365264892578\n",
      "Norm 1.4342365264892578\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['it']\n",
      "Normalized ['it']\n",
      "perplex -1.1665430068969727\n",
      "Norm 1.1665430068969727\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['use']\n",
      "Normalized ['provide']\n",
      "perplex -4.878263473510742\n",
      "Norm 1.3992290496826172\n",
      "normalized perplex -3.4863937927941557\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['formation']\n",
      "Normalized ['it']\n",
      "perplex -5.358642578125\n",
      "Norm 1.1428098678588867\n",
      "normalized perplex -4.68900621952512\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['why']\n",
      "Normalized ['how']\n",
      "perplex -1.2822790145874023\n",
      "Norm 0.6079549789428711\n",
      "normalized perplex -2.109167716361275\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Why is the theory of evolution so complex?\n",
      "Target ['theory', 'of', 'evolution']\n",
      "Normalized ['principles', 'of', 'succession']\n",
      "perplex -3.188722610473633\n",
      "Norm 0.9003005027770996\n",
      "normalized perplex -3.5418425299525915\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['so', 'complex']\n",
      "Normalized ['independently', '[SEP]']\n",
      "perplex -3.9576117992401123\n",
      "Norm 0.8410077095031738\n",
      "normalized perplex -4.705797288800213\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['why']\n",
      "Normalized ['why']\n",
      "perplex -0.009103775024414062\n",
      "Norm 0.009103775024414062\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Where can sedimentary rock not be found at the same time?\n",
      "Target ['at', 'the', 'same', 'time']\n",
      "Normalized ['on', 'the', '[SEP]', '[SEP]']\n",
      "perplex -2.3053717613220215\n",
      "Norm 1.3014228343963623\n",
      "normalized perplex -1.7714240909191667\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['be', 'found']\n",
      "Normalized ['be', 'found']\n",
      "perplex -0.7915129661560059\n",
      "Norm 0.7915129661560059\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target ['sedimentary', 'rock']\n",
      "Normalized ['fossil', 'fossils']\n",
      "perplex -8.405275344848633\n",
      "Norm 0.8196568489074707\n",
      "normalized perplex -10.254627111396816\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['not']\n",
      "Normalized ['worldwide']\n",
      "perplex -4.417304992675781\n",
      "Norm 0.6023101806640625\n",
      "normalized perplex -7.333937121576774\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['where']\n",
      "Normalized ['where']\n",
      "perplex -1.1716604232788086\n",
      "Norm 1.1716604232788086\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Who wrote the principles of faunal succession?\n",
      "Target ['fauna', '##l', 'succession']\n",
      "Normalized ['principles', 'of', 'succession']\n",
      "perplex -4.8109893798828125\n",
      "Norm 0.9003005027770996\n",
      "normalized perplex -5.343759517008666\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['principles']\n",
      "Normalized ['fossils']\n",
      "perplex -5.020883560180664\n",
      "Norm 0.18271255493164062\n",
      "normalized perplex -27.47968557529699\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['wrote']\n",
      "Normalized ['developed']\n",
      "perplex -3.044300079345703\n",
      "Norm 1.0518131256103516\n",
      "normalized perplex -2.894335510007199\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Target ['who']\n",
      "Normalized ['who']\n",
      "perplex -0.10930728912353516\n",
      "Norm 0.10930728912353516\n",
      "normalized perplex -1.0\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "16.194401585117156 4.3052664277507775 32.91401385468727 9.965215378793467\n"
     ]
    }
   ],
   "source": [
    "contexts = random.sample(range(1203), 3)\n",
    "ans_e, unans_e = get_examples(contexts)\n",
    "ans_res, unans_res = {}, {}\n",
    "for context in contexts:\n",
    "    print(dataloader.dataset.contexts[context])\n",
    "    print(\"#\"*20)\n",
    "    print(\"Start Answerable\")\n",
    "    avg_ans_odds, max_ent_ans, r = get_avg_odds([e for e in ans_e if e[0] == context], eval_dataloader_ans)\n",
    "    ans_res.update(r)\n",
    "    print(\"Start Unanswerbale\")\n",
    "    avg_unans_odds, max_ent_unans, r = get_avg_odds([e for e in unans_e if e[0] == context], eval_dataloader_unans)\n",
    "    unans_res.update(r)\n",
    "    print(avg_ans_odds, avg_unans_odds, max_ent_ans, max_ent_unans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
