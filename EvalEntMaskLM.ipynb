{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"examples/\")\n",
    "\n",
    "import logging\n",
    "import argparse\n",
    "import json\n",
    "from tqdm import tqdm, trange\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForPreTraining \n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_chunk_lm_finetune import InputExample, random_word, InputFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args\n",
    "gradient_accumulation_steps = 1\n",
    "train_batch_size = 1\n",
    "eval_file = \"dataset/dev-v2.0.json\"\n",
    "max_seq_length=256\n",
    "on_memory = True\n",
    "bert_model = \"model_chunk/pytorch_model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, corpus_path, tokenizer, seq_len, encoding=\"utf-8\", on_memory=True, answerable_flag=True):\n",
    "        self.vocab = tokenizer.vocab\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        self.on_memory = on_memory\n",
    "        self.corpus_path = corpus_path\n",
    "        self.encoding = encoding\n",
    "\n",
    "        # for loading samples directly from file\n",
    "        self.sample_counter = 0  # used to keep track of full epochs on file\n",
    "        self.line_buffer = None  # keep second sentence of a pair in memory and use as first sentence in next pair\n",
    "\n",
    "        # for loading samples in memory\n",
    "        self.questions = []\n",
    "        self.contexts = []\n",
    "        self.examples = []\n",
    "\n",
    "        # load samples into memory\n",
    "        if on_memory:\n",
    "            # DANITER: Load Squad data\n",
    "            with open(corpus_path, 'r') as handle:\n",
    "                jdata = json.load(handle)\n",
    "                data = jdata['data']\n",
    "\n",
    "            for i in tqdm(range(len(data)), \"Loading Squad\", total=len(data)):\n",
    "                section = data[i]['paragraphs']\n",
    "                for sec in section:\n",
    "                    context = sec['context']\n",
    "                    self.contexts.append(context)\n",
    "                    qas = sec['qas']\n",
    "                    for j in range(len(qas)):\n",
    "                        question = qas[j]['question']\n",
    "                        unanswerable = qas[j]['is_impossible']\n",
    "                        self.questions.append(question)\n",
    "                        if unanswerable and answerable_flag:\n",
    "                            continue\n",
    "                        if not unanswerable and not answerable_flag:\n",
    "                            continue\n",
    "                        self.examples.append((len(self.contexts)-1, len(self.questions)-1))\n",
    "\n",
    "#             with open(\"../training_data_chunks.pkl\", \"rb\") as handle:\n",
    "#                 self.training_data_map = pickle.load(handle)\n",
    "\n",
    "\n",
    "        # load samples later lazily from disk\n",
    "        else:\n",
    "            raise Exception(\"No supported\")\n",
    "\n",
    "    def __len__(self):\n",
    "        # last line of doc won't be used, because there's no \"nextSentence\". Additionally, we start counting at 0.\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        cur_id = self.sample_counter\n",
    "        self.sample_counter += 1\n",
    "        if not self.on_memory:\n",
    "            raise Exception(\"No supported\")\n",
    "\n",
    "        while True:\n",
    "            t1, t2, target, is_next_label = self.get_example(item)\n",
    "\n",
    "            # tokenize\n",
    "            tokens_a = self.tokenizer.tokenize(t1)\n",
    "            tokens_b = self.tokenizer.tokenize(t2)\n",
    "            if len(tokens_a) + len(tokens_b) + 3 > self.seq_len :\n",
    "                item += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # combine to one sample\n",
    "        cur_example = InputExample(guid=cur_id, tokens_a=tokens_a, tokens_b=tokens_b, is_next=is_next_label, target=target)\n",
    "\n",
    "        # transform sample to features\n",
    "        cur_features = convert_example_to_features(cur_example, self.seq_len, self.tokenizer)\n",
    "\n",
    "        cur_tensors = (torch.tensor(cur_features.input_ids),\n",
    "                       torch.tensor(cur_features.input_mask),\n",
    "                       torch.tensor(cur_features.segment_ids),\n",
    "                       torch.tensor(cur_features.lm_label_ids),\n",
    "                       torch.tensor(cur_features.is_next))\n",
    "\n",
    "        return cur_tensors\n",
    "\n",
    "    def get_example(self, index):\n",
    "        \"\"\"\n",
    "        Get one sample from corpus consisting of two sentences. With prob. 50% these are two subsequent sentences\n",
    "        from one doc. With 50% the second sentence will be a random one from another doc.\n",
    "        :param index: int, index of sample.\n",
    "        :return: (str, str, int), sentence 1, sentence 2, isNextSentence Label\n",
    "        \"\"\"\n",
    "        t1, t2 = self.get_corpus_line(index)\n",
    "\n",
    "        target = (None, None) # keep same shape\n",
    "        # Daniter we do not do next sentence prediction\n",
    "\n",
    "        assert len(t1) > 0\n",
    "        assert len(t2) > 0\n",
    "        return t1, t2, target, 1\n",
    "\n",
    "    def get_corpus_line(self, item):\n",
    "        \"\"\"\n",
    "        Get one sample from corpus consisting of a pair of two subsequent lines from the same doc.\n",
    "        :param item: int, index of sample.\n",
    "        :return: (str, str), two subsequent sentences from corpus\n",
    "        \"\"\"\n",
    "        t1 = \"\"\n",
    "        t2 = \"\"\n",
    "        assert item < len(self.examples)\n",
    "        if self.on_memory:\n",
    "            # DANITER - get the context and question pair based on the example indexes\n",
    "            context_idx, question_idx = self.examples[item]\n",
    "            t1 = self.contexts[context_idx]\n",
    "            t2 = self.questions[question_idx]\n",
    "\n",
    "            # used later to avoid random nextSentence from same doc\n",
    "            return t1, t2\n",
    "        else:\n",
    "            raise Exception(\"No supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_features(example, max_seq_length, tokenizer):\n",
    "    \"\"\"\n",
    "    Convert a raw sample (pair of sentences as tokenized strings) into a proper training sample with\n",
    "    IDs, LM labels, input_mask, CLS and SEP tokens etc.\n",
    "    :param example: InputExample, containing sentence input as strings and is_next label\n",
    "    :param max_seq_length: int, maximum length of sequence.\n",
    "    :param tokenizer: Tokenizer\n",
    "    :return: InputFeatures, containing all inputs and labels of one sample as IDs (as used for model training)\n",
    "    \"\"\"\n",
    "    tokens_a = example.tokens_a\n",
    "    tokens_b = example.tokens_b\n",
    "    # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "    # length is less than the specified length.\n",
    "    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "    # _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "\n",
    "    t1_random, t1_label = random_word(tokens_a, tokenizer, question=False)\n",
    "    t2_random, t2_label = random_word(tokens_b, tokenizer, question=False)\n",
    "    # concatenate lm labels and account for CLS, SEP, SEP\n",
    "    lm_label_ids = ([-1] + t1_label + [-1] + t2_label + [-1])\n",
    "\n",
    "    # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "    # (b) For single sequences:\n",
    "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    #  type_ids: 0   0   0   0  0     0 0\n",
    "    #\n",
    "    # Where \"type_ids\" are used to indicate whether this is the first\n",
    "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "    # embedding vector (and position vector). This is not *strictly* necessary\n",
    "    # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "    # it easier for the model to learn the concept of sequences.\n",
    "    #\n",
    "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "    # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "    # the entire model is fine-tuned.\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    assert len(t2_random) > 0\n",
    "    for token in t2_random:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(1)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(1)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "    while len(lm_label_ids) < max_seq_length:\n",
    "        lm_label_ids.append(-1)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length, len(input_ids)\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "\n",
    "    features = InputFeatures(input_ids=input_ids,\n",
    "                             input_mask=input_mask,\n",
    "                             segment_ids=segment_ids,\n",
    "                             lm_label_ids=lm_label_ids,\n",
    "                             is_next=example.is_next)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/26/2019 15:46:11 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/daniter/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "Loading Squad: 100%|██████████| 35/35 [00:00<00:00, 4334.37it/s]\n",
      "Loading Squad: 100%|██████████| 35/35 [00:00<00:00, 3928.09it/s]\n",
      "02/26/2019 15:46:12 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/daniter/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "02/26/2019 15:46:12 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /Users/daniter/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/xx/8h5l1j614vv5wmbx9fbj69wm0000gn/T/tmpg230w1r_\n",
      "02/26/2019 15:46:15 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the vocab size: 30522\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "\n",
    "# Load eval_data\n",
    "eval_dataset_answerable = BERTDataset(eval_file, tokenizer, seq_len=max_seq_length,\n",
    "                            on_memory=on_memory, answerable_flag=True)\n",
    "eval_dataset_unanswerable = BERTDataset(eval_file, tokenizer, seq_len=max_seq_length,\n",
    "                           on_memory=on_memory, answerable_flag=False)\n",
    "\n",
    "# Prepare model\n",
    "model_state_dict = torch.load(bert_model, map_location='cpu') #TODO daniter: remove this map_location\n",
    "## TODO daniter: check if bert model is being loaded correctly\n",
    "model = BertForPreTraining.from_pretrained(\"bert-base-uncased\", state_dict=model_state_dict)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Prepare optimizer\n",
    "print(\"Checking the vocab size:\", len(tokenizer.vocab))\n",
    "# 768 is bert hidden size, 256 is GRU hidden size, 1 is the layers in the GRU\n",
    "\n",
    "# eval loader\n",
    "eval_sampler_ans = SequentialSampler(eval_dataset_answerable)\n",
    "eval_dataloader_ans = DataLoader(eval_dataset_answerable, sampler=eval_sampler_ans,\n",
    "                                 batch_size=train_batch_size)\n",
    "eval_sampler_unans = SequentialSampler(eval_dataset_unanswerable)\n",
    "eval_dataloader_unans = DataLoader(eval_dataset_unanswerable, sampler=eval_sampler_unans,\n",
    "                                   batch_size=train_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'the', 'norman', '##s', 'were', 'in', 'contact', 'with', 'england', 'from', 'an', 'early', 'date', '.', 'not', 'only', 'were', 'their', 'original', 'viking', 'brethren', 'still', 'ra', '##va', '##ging', 'the', 'english', 'coasts', ',', 'they', 'occupied', 'most', 'of', 'the', 'important', 'ports', 'opposite', 'england', 'across', 'the', 'english', 'channel', '.', 'this', 'relationship', 'eventually', 'produced', 'closer', 'ties', 'of', 'blood', 'through', 'the', 'marriage', 'of', 'emma', ',', 'sister', 'of', 'duke', 'richard', 'ii', 'of', 'normandy', ',', 'and', 'king', 'ethel', '##red', 'ii', 'of', 'england', '.', 'because', 'of', 'this', ',', 'ethel', '##red', 'fled', 'to', 'normandy', 'in', '101', '##3', ',', 'when', 'he', 'was', 'forced', 'from', 'his', 'kingdom', 'by', 'sw', '##ey', '##n', 'fork', '##be', '##ard', '.', 'his', 'stay', 'in', 'normandy', '(', 'until', '1016', ')', 'influenced', 'him', 'and', 'his', 'sons', 'by', 'emma', ',', 'who', 'stayed', 'in', 'normandy', 'after', 'cn', '##ut', 'the', 'great', \"'\", 's', 'conquest', 'of', 'the', 'isle', '.', '[SEP]', 'who', \"'\", 's', 'major', 'ports', 'were', 'controlled', 'by', 'the', '[MASK]', '?', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "target = 50\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss_ans = 0\n",
    "    for batch_i, eval_batch in enumerate(eval_dataloader_unans):\n",
    "        eval_batch = tuple(t.to(device) for t in eval_batch)\n",
    "        input_ids, input_mask, segment_ids, lm_label_ids, is_next = eval_batch\n",
    "        if batch_i != target:\n",
    "            continue\n",
    "        if batch_i == target:\n",
    "            input_ids[0][143] = 103\n",
    "            #input_ids[0][138] = 103\n",
    "            #input_ids[0][117] = 103\n",
    "            print(tokenizer.convert_ids_to_tokens(input_ids.data.numpy()[0]))\n",
    "        output, _ = model(input_ids, segment_ids, input_mask, None, None)\n",
    "        if batch_i == target:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['?', 'the', 'norman', '##s', 'were', 'in', 'contact', 'with', 'england', 'from', 'an', 'early', 'date', '.', 'not', 'only', 'were', 'their', 'original', 'viking', 'brethren', 'still', 'ra', '##va', '##ging', 'the', 'english', 'coasts', ',', 'they', 'occupied', 'most', 'of', 'the', 'important', 'ports', 'opposite', 'england', 'across', 'the', 'english', 'channel', '.', 'this', 'relationship', 'eventually', 'made', 'closer', 'ties', 'of', 'blood', 'through', 'the', 'marriage', 'of', 'emma', ',', 'sister', 'of', 'duke', 'richard', 'ii', 'of', 'normandy', ',', 'and', 'king', 'ethel', '##red', 'ii', 'of', 'england', '.', 'because', 'of', 'this', ',', 'ethel', '##red', 'fled', 'to', 'normandy', 'in', '101', '##3', ',', 'when', 'he', 'was', 'forced', 'from', 'his', 'kingdom', 'by', 'sw', '##ey', '##n', 'fork', '##be', '##ard', '.', 'his', 'stay', 'in', 'normandy', '(', 'until', '1016', ')', 'influenced', 'him', 'and', 'his', 'sons', 'by', 'emma', ',', 'who', 'stayed', 'in', 'normandy', 'after', 'cn', '##ut', 'the', 'great', \"'\", 's', 'conquest', 'of', 'the', 'isle', '.', 'vikings', 'who', \"'\", 's', 'major', 'ports', 'were', 'controlled', 'by', 'the', '[MASK]', '?', 'vikings', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s', '##s']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(np.argmax(output[0].data.numpy(), axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[CLS]', 0), ('the', 1), ('norman', 2), ('##s', 3), ('were', 4), ('in', 5), ('contact', 6), ('with', 7), ('england', 8), ('from', 9), ('an', 10), ('early', 11), ('date', 12), ('.', 13), ('not', 14), ('only', 15), ('were', 16), ('their', 17), ('original', 18), ('viking', 19), ('brethren', 20), ('still', 21), ('ra', 22), ('##va', 23), ('##ging', 24), ('the', 25), ('english', 26), ('coasts', 27), (',', 28), ('they', 29), ('occupied', 30), ('most', 31), ('of', 32), ('the', 33), ('important', 34), ('ports', 35), ('opposite', 36), ('england', 37), ('across', 38), ('the', 39), ('english', 40), ('channel', 41), ('.', 42), ('this', 43), ('relationship', 44), ('eventually', 45), ('produced', 46), ('closer', 47), ('ties', 48), ('of', 49), ('blood', 50), ('through', 51), ('the', 52), ('marriage', 53), ('of', 54), ('emma', 55), (',', 56), ('sister', 57), ('of', 58), ('duke', 59), ('richard', 60), ('ii', 61), ('of', 62), ('normandy', 63), (',', 64), ('and', 65), ('king', 66), ('ethel', 67), ('##red', 68), ('ii', 69), ('of', 70), ('england', 71), ('.', 72), ('because', 73), ('of', 74), ('this', 75), (',', 76), ('ethel', 77), ('##red', 78), ('fled', 79), ('to', 80), ('normandy', 81), ('in', 82), ('101', 83), ('##3', 84), (',', 85), ('when', 86), ('he', 87), ('was', 88), ('forced', 89), ('from', 90), ('his', 91), ('kingdom', 92), ('by', 93), ('sw', 94), ('##ey', 95), ('##n', 96), ('fork', 97), ('##be', 98), ('##ard', 99), ('.', 100), ('his', 101), ('stay', 102), ('in', 103), ('normandy', 104), ('(', 105), ('until', 106), ('1016', 107), (')', 108), ('influenced', 109), ('him', 110), ('and', 111), ('his', 112), ('sons', 113), ('by', 114), ('emma', 115), (',', 116), ('who', 117), ('stayed', 118), ('in', 119), ('normandy', 120), ('after', 121), ('cn', 122), ('##ut', 123), ('the', 124), ('great', 125), (\"'\", 126), ('s', 127), ('conquest', 128), ('of', 129), ('the', 130), ('isle', 131), ('.', 132), ('[SEP]', 133), ('who', 134), (\"'\", 135), ('s', 136), ('major', 137), ('ports', 138), ('were', 139), ('controlled', 140), ('by', 141), ('the', 142), ('english', 143), ('?', 144), ('[SEP]', 145), ('[PAD]', 146), ('[PAD]', 147), ('[PAD]', 148), ('[PAD]', 149), ('[PAD]', 150), ('[PAD]', 151), ('[PAD]', 152), ('[PAD]', 153), ('[PAD]', 154), ('[PAD]', 155), ('[PAD]', 156), ('[PAD]', 157), ('[PAD]', 158), ('[PAD]', 159), ('[MASK]', 160), ('[PAD]', 161), ('[PAD]', 162), ('[PAD]', 163), ('[PAD]', 164), ('[PAD]', 165), ('[PAD]', 166), ('[PAD]', 167), ('[PAD]', 168), ('[PAD]', 169), ('[PAD]', 170), ('[PAD]', 171), ('[PAD]', 172), ('[PAD]', 173), ('[PAD]', 174), ('[PAD]', 175), ('[PAD]', 176), ('[PAD]', 177), ('[PAD]', 178), ('[PAD]', 179), ('[PAD]', 180), ('[PAD]', 181), ('[PAD]', 182), ('[PAD]', 183), ('[PAD]', 184), ('[PAD]', 185), ('[PAD]', 186), ('[PAD]', 187), ('[PAD]', 188), ('[PAD]', 189), ('[PAD]', 190), ('[PAD]', 191), ('[PAD]', 192), ('[PAD]', 193), ('[PAD]', 194), ('[PAD]', 195), ('[PAD]', 196), ('[PAD]', 197), ('[PAD]', 198), ('[PAD]', 199), ('[PAD]', 200), ('[PAD]', 201), ('[PAD]', 202), ('[PAD]', 203), ('[PAD]', 204), ('[PAD]', 205), ('[PAD]', 206), ('[PAD]', 207), ('[PAD]', 208), ('[PAD]', 209), ('[PAD]', 210), ('[PAD]', 211), ('[PAD]', 212), ('[PAD]', 213), ('[PAD]', 214), ('[PAD]', 215), ('[PAD]', 216), ('[PAD]', 217), ('[PAD]', 218), ('[PAD]', 219), ('[PAD]', 220), ('[PAD]', 221), ('[PAD]', 222), ('[PAD]', 223), ('[PAD]', 224), ('[PAD]', 225), ('[PAD]', 226), ('[PAD]', 227), ('[PAD]', 228), ('[PAD]', 229), ('[PAD]', 230), ('[PAD]', 231), ('[PAD]', 232), ('[PAD]', 233), ('[PAD]', 234), ('[PAD]', 235), ('[PAD]', 236), ('[PAD]', 237), ('[PAD]', 238), ('[PAD]', 239), ('[PAD]', 240), ('[PAD]', 241), ('[PAD]', 242), ('[PAD]', 243), ('[PAD]', 244), ('[PAD]', 245), ('[PAD]', 246), ('[PAD]', 247), ('[PAD]', 248), ('[PAD]', 249), ('[PAD]', 250), ('[PAD]', 251), ('[PAD]', 252), ('[PAD]', 253), ('[PAD]', 254), ('[PAD]', 255)]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip(tokenizer.convert_ids_to_tokens(input_ids.data.numpy()[0]), range(256))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vikings'] tensor(16.1189)\n",
      "['english'] tensor(13.6539)\n",
      "['##s'] tensor(13.0520)\n",
      "['norman'] tensor(12.4569)\n",
      "['viking'] tensor(10.6945)\n",
      "['scandinavian'] tensor(10.4082)\n",
      "['germans'] tensor(9.7384)\n",
      "['british'] tensor(9.5046)\n",
      "['danes'] tensor(9.4454)\n",
      "['danish'] tensor(9.2061)\n",
      "['angles'] tensor(9.1156)\n",
      "['invaders'] tensor(9.0145)\n",
      "['french'] tensor(8.9796)\n",
      "['irish'] tensor(8.6128)\n",
      "['norse'] tensor(8.6118)\n",
      "['conquest'] tensor(8.5325)\n",
      "['saxons'] tensor(8.5170)\n",
      "['normandy'] tensor(8.4517)\n",
      "['sas'] tensor(8.3299)\n",
      "['welsh'] tensor(8.2382)\n",
      "['knights'] tensor(7.9403)\n",
      "['people'] tensor(7.7032)\n",
      "['humans'] tensor(7.6789)\n",
      "['spanish'] tensor(7.6453)\n",
      "['swedish'] tensor(7.6362)\n"
     ]
    }
   ],
   "source": [
    "c = Counter()\n",
    "for i, o in enumerate(output[0][145]):\n",
    "    c[i] = o\n",
    "for x, val in c.most_common(25):\n",
    "    print(tokenizer.convert_ids_to_tokens([x]), val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9586)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(output[0][120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'edgar'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.ids_to_tokens[9586] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* add end token to model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(context_ids.data.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(question_ids)\n",
    "print(tokenizer.convert_ids_to_tokens(question_ids.data.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = output.data.numpy()\n",
    "print(tokenizer.convert_ids_to_tokens(np.argmax(o[0], axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(np.argmax(o[0,i,:]), np.exp(np.max(o[0,i,:]))/ np.sum(np.exp(o[0,i,:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(o.shape[2]):\n",
    "    c[i] = np.exp(o[0,7,i])/ np.sum(np.exp(o[0,7,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, score in c.most_common()[:25]:\n",
    "    print(tokenizer.convert_ids_to_tokens([idx]), score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[2435] # normandy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss_ans = 0\n",
    "    for batch_i, eval_batch in enumerate(eval_dataloader_unans):\n",
    "        eids = eval_batch[-1]\n",
    "        eval_batch = tuple(t.to(device) for t in eval_batch[:-1])\n",
    "        question_ids, question_mask, context_ids, context_mask, targets = eval_batch\n",
    "        output, _ = model(context_ids, context_mask, question_ids, question_mask)\n",
    "        loss = criterion(output.view(-1, len(tokenizer.vocab)), question_ids.view(-1))\n",
    "        eval_loss_ans += loss.item()\n",
    "        break\n",
    "        if loss.item() > 0.01:\n",
    "            print(batch_i, eval_loss_ans)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(question_ids)\n",
    "print(tokenizer.convert_ids_to_tokens(question_ids.data.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = output.data.numpy()\n",
    "print(o)\n",
    "print(tokenizer.convert_ids_to_tokens(np.argmax(o[0], axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print(np.argmax(o[0,i,:]), np.exp(np.max(o[0,i,:]))/ np.sum(np.exp(o[0,i,:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interesting Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def print_details(context_ids, question_ids, output, loss, tokenizer):\n",
    "    print(\"Loss:\", loss.item())\n",
    "    print(\"CONTEXT\")\n",
    "    print(tokenizer.convert_ids_to_tokens(context_ids.data.numpy()[0]))\n",
    "    print(\"~\"*30)\n",
    "    print(\"QUESTION\")\n",
    "    q_ids = [i for i in question_ids.data.numpy()[0] if i != 0]\n",
    "    print(q_ids)\n",
    "    q_toks = [tok for tok in tokenizer.convert_ids_to_tokens(question_ids.data.numpy()[0]) if tok != '[PAD]']\n",
    "    print(q_toks)\n",
    "    print(\"~\"*30)\n",
    "    print(\"OUTPUT\")\n",
    "    o = output.data.numpy()\n",
    "    out_ids = [i for i in np.argmax(o[0], axis=1) if i != 0]\n",
    "    out_toks = [tok for tok in tokenizer.convert_ids_to_tokens(np.argmax(o[0], axis=1)) if tok != '[PAD]']\n",
    "    scores = [(np.argmax(o[0,i,:]), np.exp(np.max(o[0,i,:]))/ np.sum(np.exp(o[0,i,:]))) for i in range(len(out_toks))]\n",
    "    print(out_toks)\n",
    "    print(list(zip(out_toks, scores)))\n",
    "    print(\"~\"*30)\n",
    "    print(\"TOP K FOR INCORRECT TERMS:\")\n",
    "    for tok_i, (tar, out) in enumerate(zip(q_ids[1:], out_ids)):\n",
    "        if tar != out:\n",
    "            print (\"Output\", out_toks[tok_i], \"instead of \", q_toks[tok_i+1])\n",
    "            c = Counter()\n",
    "            for i in range(o.shape[2]):\n",
    "                c[i] = np.exp(o[0,tok_i,i])/ np.sum(np.exp(o[0,tok_i,:]))\n",
    "            for idx, score in c.most_common()[:10]:\n",
    "                print(\"- \\t\",tokenizer.convert_ids_to_tokens([idx]), score)\n",
    "    print(\"#\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore = {eval_dataloader_ans: [0,1], eval_dataloader_unans: [0, 15]}\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss_ans = 0\n",
    "    for dataloader in explore.keys():\n",
    "        for batch_i, eval_batch in enumerate(dataloader):\n",
    "            eids = eval_batch[-1]\n",
    "            eval_batch = tuple(t.to(device) for t in eval_batch[:-1])\n",
    "            question_ids, question_mask, context_ids, context_mask, targets = eval_batch\n",
    "            output, _ = model(context_ids, context_mask, question_ids, question_mask)\n",
    "            loss = criterion(output.view(-1, len(tokenizer.vocab)), targets.view(-1))\n",
    "            eval_loss_ans += loss.item()\n",
    "            if batch_i in explore[dataloader]:\n",
    "                if dataloader == eval_dataloader_ans:\n",
    "                    print(\"Answerable:\",batch_i)\n",
    "                else:\n",
    "                    print(\"Unanswerable:\",batch_i)\n",
    "                print_details(context_ids, question_ids, output, loss, tokenizer)\n",
    "            if batch_i > max(explore[dataloader]):\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "* LM was suprisingly good at guessing question type and general structure of the question. It may be because the BERT representation is leaky in terms of representation  \n",
    "    - may be interesting result of its own ... Q: How much does the feature of each BERT word contain information about the surrounding BERT words?  \n",
    "    - this is probably why the model is pretty good at guessing Q type from the [CLS] token  \n",
    "* the Sentence repr is not enough represent specific entities in the text  \n",
    "    - gets confused between normandy and france and mongolia and normans   \n",
    "    - this might be because I don't train the sentence repr which is probably a mistake and we should retrain this one with trained sentence repr and forward masking in the question repr\n",
    "    - Maybe we can do an attention over entities? \n",
    "    - There is a bias towards more common entities right now (ie. john and paris instead of rollo and normandy)\n",
    "    - the fact that stuff like Rollo and dates were predicted well means there is leakiness in the representation\n",
    "* rank may be more important than loss since sometimes the #1 option has very high prob but number 2 is really good (eg. arrive vs begin U15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
