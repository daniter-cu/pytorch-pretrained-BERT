{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"examples/\")\n",
    "\n",
    "import logging\n",
    "import argparse\n",
    "import json\n",
    "from tqdm import tqdm, trange\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForPreTraining \n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_chunk_lm_finetune import InputExample, random_word, InputFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args\n",
    "gradient_accumulation_steps = 1\n",
    "train_batch_size = 1\n",
    "eval_file = \"dataset/dev-v2.0.json\"\n",
    "max_seq_length=256\n",
    "on_memory = True\n",
    "bert_model = \"model_chunk/pytorch_model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, corpus_path, tokenizer, seq_len, encoding=\"utf-8\", on_memory=True, answerable_flag=True):\n",
    "        self.vocab = tokenizer.vocab\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        self.on_memory = on_memory\n",
    "        self.corpus_path = corpus_path\n",
    "        self.encoding = encoding\n",
    "\n",
    "        # for loading samples directly from file\n",
    "        self.sample_counter = 0  # used to keep track of full epochs on file\n",
    "        self.line_buffer = None  # keep second sentence of a pair in memory and use as first sentence in next pair\n",
    "\n",
    "        # for loading samples in memory\n",
    "        self.questions = []\n",
    "        self.contexts = []\n",
    "        self.examples = []\n",
    "\n",
    "        # load samples into memory\n",
    "        if on_memory:\n",
    "            # DANITER: Load Squad data\n",
    "            with open(corpus_path, 'r') as handle:\n",
    "                jdata = json.load(handle)\n",
    "                data = jdata['data']\n",
    "\n",
    "            for i in tqdm(range(len(data)), \"Loading Squad\", total=len(data)):\n",
    "                section = data[i]['paragraphs']\n",
    "                for sec in section:\n",
    "                    context = sec['context']\n",
    "                    self.contexts.append(context)\n",
    "                    qas = sec['qas']\n",
    "                    for j in range(len(qas)):\n",
    "                        question = qas[j]['question']\n",
    "                        unanswerable = qas[j]['is_impossible']\n",
    "                        self.questions.append(question)\n",
    "                        if unanswerable and answerable_flag:\n",
    "                            continue\n",
    "                        if not unanswerable and not answerable_flag:\n",
    "                            continue\n",
    "                        self.examples.append((len(self.contexts)-1, len(self.questions)-1))\n",
    "\n",
    "#             with open(\"../training_data_chunks.pkl\", \"rb\") as handle:\n",
    "#                 self.training_data_map = pickle.load(handle)\n",
    "\n",
    "\n",
    "        # load samples later lazily from disk\n",
    "        else:\n",
    "            raise Exception(\"No supported\")\n",
    "\n",
    "    def __len__(self):\n",
    "        # last line of doc won't be used, because there's no \"nextSentence\". Additionally, we start counting at 0.\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        cur_id = self.sample_counter\n",
    "        self.sample_counter += 1\n",
    "        if not self.on_memory:\n",
    "            raise Exception(\"No supported\")\n",
    "\n",
    "        while True:\n",
    "            t1, t2, target, is_next_label = self.get_example(item)\n",
    "\n",
    "            # tokenize\n",
    "            tokens_a = self.tokenizer.tokenize(t1)\n",
    "            tokens_b = self.tokenizer.tokenize(t2)\n",
    "            if len(tokens_a) + len(tokens_b) + 3 > self.seq_len :\n",
    "                item += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # combine to one sample\n",
    "        cur_example = InputExample(guid=cur_id, tokens_a=tokens_a, tokens_b=tokens_b, is_next=is_next_label, target=target)\n",
    "\n",
    "        # transform sample to features\n",
    "        cur_features = convert_example_to_features(cur_example, self.seq_len, self.tokenizer)\n",
    "\n",
    "        cur_tensors = (torch.tensor(cur_features.input_ids),\n",
    "                       torch.tensor(cur_features.input_mask),\n",
    "                       torch.tensor(cur_features.segment_ids),\n",
    "                       torch.tensor(cur_features.lm_label_ids),\n",
    "                       torch.tensor(cur_features.is_next))\n",
    "\n",
    "        return cur_tensors\n",
    "\n",
    "    def get_example(self, index):\n",
    "        \"\"\"\n",
    "        Get one sample from corpus consisting of two sentences. With prob. 50% these are two subsequent sentences\n",
    "        from one doc. With 50% the second sentence will be a random one from another doc.\n",
    "        :param index: int, index of sample.\n",
    "        :return: (str, str, int), sentence 1, sentence 2, isNextSentence Label\n",
    "        \"\"\"\n",
    "        t1, t2 = self.get_corpus_line(index)\n",
    "\n",
    "        target = (None, None) # keep same shape\n",
    "        # Daniter we do not do next sentence prediction\n",
    "\n",
    "        assert len(t1) > 0\n",
    "        assert len(t2) > 0\n",
    "        return t1, t2, target, 1\n",
    "\n",
    "    def get_corpus_line(self, item):\n",
    "        \"\"\"\n",
    "        Get one sample from corpus consisting of a pair of two subsequent lines from the same doc.\n",
    "        :param item: int, index of sample.\n",
    "        :return: (str, str), two subsequent sentences from corpus\n",
    "        \"\"\"\n",
    "        t1 = \"\"\n",
    "        t2 = \"\"\n",
    "        assert item < len(self.examples)\n",
    "        if self.on_memory:\n",
    "            # DANITER - get the context and question pair based on the example indexes\n",
    "            context_idx, question_idx = self.examples[item]\n",
    "            t1 = self.contexts[context_idx]\n",
    "            t2 = self.questions[question_idx]\n",
    "\n",
    "            # used later to avoid random nextSentence from same doc\n",
    "            return t1, t2\n",
    "        else:\n",
    "            raise Exception(\"No supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_features(example, max_seq_length, tokenizer):\n",
    "    \"\"\"\n",
    "    Convert a raw sample (pair of sentences as tokenized strings) into a proper training sample with\n",
    "    IDs, LM labels, input_mask, CLS and SEP tokens etc.\n",
    "    :param example: InputExample, containing sentence input as strings and is_next label\n",
    "    :param max_seq_length: int, maximum length of sequence.\n",
    "    :param tokenizer: Tokenizer\n",
    "    :return: InputFeatures, containing all inputs and labels of one sample as IDs (as used for model training)\n",
    "    \"\"\"\n",
    "    tokens_a = example.tokens_a\n",
    "    tokens_b = example.tokens_b\n",
    "    # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "    # length is less than the specified length.\n",
    "    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "    # _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "\n",
    "    t1_random, t1_label = random_word(tokens_a, tokenizer, question=False)\n",
    "    t2_random, t2_label = random_word(tokens_b, tokenizer, question=False)\n",
    "    # concatenate lm labels and account for CLS, SEP, SEP\n",
    "    lm_label_ids = ([-1] + t1_label + [-1] + t2_label + [-1])\n",
    "\n",
    "    # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "    # (b) For single sequences:\n",
    "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    #  type_ids: 0   0   0   0  0     0 0\n",
    "    #\n",
    "    # Where \"type_ids\" are used to indicate whether this is the first\n",
    "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "    # embedding vector (and position vector). This is not *strictly* necessary\n",
    "    # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "    # it easier for the model to learn the concept of sequences.\n",
    "    #\n",
    "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "    # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "    # the entire model is fine-tuned.\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    assert len(t2_random) > 0\n",
    "    for token in t2_random:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(1)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(1)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "    while len(lm_label_ids) < max_seq_length:\n",
    "        lm_label_ids.append(-1)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length, len(input_ids)\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "\n",
    "    features = InputFeatures(input_ids=input_ids,\n",
    "                             input_mask=input_mask,\n",
    "                             segment_ids=segment_ids,\n",
    "                             lm_label_ids=lm_label_ids,\n",
    "                             is_next=example.is_next)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/06/2019 14:52:03 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/daniter/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "Loading Squad: 100%|██████████| 35/35 [00:00<00:00, 3944.88it/s]\n",
      "Loading Squad: 100%|██████████| 35/35 [00:00<00:00, 3988.50it/s]\n",
      "03/06/2019 14:52:04 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/daniter/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "03/06/2019 14:52:04 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /Users/daniter/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/xx/8h5l1j614vv5wmbx9fbj69wm0000gn/T/tmpqrm18eq3\n",
      "03/06/2019 14:52:07 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the vocab size: 30522\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "\n",
    "# Load eval_data\n",
    "eval_dataset_answerable = BERTDataset(eval_file, tokenizer, seq_len=max_seq_length,\n",
    "                            on_memory=on_memory, answerable_flag=True)\n",
    "eval_dataset_unanswerable = BERTDataset(eval_file, tokenizer, seq_len=max_seq_length,\n",
    "                           on_memory=on_memory, answerable_flag=False)\n",
    "\n",
    "# Prepare model\n",
    "model_state_dict = torch.load(bert_model, map_location='cpu') #TODO daniter: remove this map_location\n",
    "## TODO daniter: check if bert model is being loaded correctly\n",
    "model = BertForPreTraining.from_pretrained(\"bert-base-uncased\", state_dict=model_state_dict)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Prepare optimizer\n",
    "print(\"Checking the vocab size:\", len(tokenizer.vocab))\n",
    "# 768 is bert hidden size, 256 is GRU hidden size, 1 is the layers in the GRU\n",
    "\n",
    "# eval loader\n",
    "eval_sampler_ans = SequentialSampler(eval_dataset_answerable)\n",
    "eval_dataloader_ans = DataLoader(eval_dataset_answerable, sampler=eval_sampler_ans,\n",
    "                                 batch_size=train_batch_size)\n",
    "eval_sampler_unans = SequentialSampler(eval_dataset_unanswerable)\n",
    "eval_dataloader_unans = DataLoader(eval_dataset_unanswerable, sampler=eval_sampler_unans,\n",
    "                                   batch_size=train_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'a', 'problem', 'is', 'regarded', 'as', 'inherently', 'difficult', 'if', 'its', 'solution', 'requires', 'significant', 'resources', ',', 'whatever', 'the', 'algorithm', 'used', '.', 'the', 'theory', 'formal', '##izes', 'this', 'intuition', ',', 'by', 'introducing', 'mathematical', 'models', 'of', 'computation', 'to', 'study', 'these', 'problems', 'and', 'quan', '##tify', '##ing', 'the', 'amount', 'of', 'resources', 'needed', 'to', 'solve', 'them', ',', 'such', 'as', 'time', 'and', 'storage', '.', 'other', 'complexity', 'measures', 'are', 'also', 'used', ',', 'such', 'as', 'the', 'amount', 'of', 'communication', '(', 'used', 'in', 'communication', 'complexity', ')', ',', 'the', 'number', 'of', 'gates', 'in', 'a', 'circuit', '(', 'used', 'in', 'circuit', 'complexity', ')', 'and', 'the', 'number', 'of', 'processors', '(', 'used', 'in', 'parallel', 'computing', ')', '.', 'one', 'of', 'the', 'roles', 'of', 'computational', 'complexity', 'theory', 'is', 'to', 'determine', 'the', 'practical', 'limits', 'on', 'what', 'computers', 'can', 'and', 'cannot', 'do', '.', '[SEP]', 'what', 'unit', 'is', 'measured', 'to', 'determine', '[MASK]', '[MASK]', '?', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "target = 120 #50\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss_ans = 0\n",
    "    for batch_i, eval_batch in enumerate(eval_dataloader_unans):\n",
    "        eval_batch = tuple(t.to(device) for t in eval_batch)\n",
    "        input_ids, input_mask, segment_ids, lm_label_ids, is_next = eval_batch\n",
    "        if batch_i != target:\n",
    "            continue\n",
    "        if batch_i == target:\n",
    "            input_ids[0][130] = 103\n",
    "            input_ids[0][131] = 103\n",
    "            #input_ids[0][117] = 103\n",
    "            print(tokenizer.convert_ids_to_tokens(input_ids.data.numpy()[0]))\n",
    "        output, _ = model(input_ids, segment_ids, input_mask, None, None)\n",
    "        if batch_i == target:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'a', 'problem', 'is', 'regarded', 'as', 'inherently', 'difficult', 'if', 'its', 'solution', 'requires', 'significant', 'resources', ',', 'whatever', 'the', 'algorithm', 'used', '.', 'the', 'theory', 'formal', '##izes', 'this', 'intuition', ',', 'by', 'introducing', 'mathematical', 'models', 'of', 'computation', 'to', 'study', 'these', 'problems', 'and', 'quan', '##tify', '##ing', 'the', 'amount', 'of', 'resources', 'needed', 'to', 'solve', 'them', ',', 'such', 'as', 'time', 'and', 'storage', '.', 'other', 'complexity', 'measures', 'are', 'also', 'used', ',', 'such', 'as', 'the', 'amount', 'of', 'communication', '(', 'used', 'in', 'communication', 'complexity', ')', ',', 'the', 'number', 'of', 'gates', 'in', 'a', 'circuit', '(', 'used', 'in', 'circuit', 'complexity', ')', 'and', 'the', 'number', 'of', 'processors', '(', 'used', 'in', 'parallel', 'computing', ')', '.', 'one', 'of', 'the', 'roles', 'of', 'computational', 'complexity', 'theory', 'is', 'to', 'determine', 'the', 'practical', 'limits', 'on', 'what', 'computers', 'can', 'and', 'cannot', 'do', '.', 'the', 'what', 'unit', 'is', 'measured', 'to', 'determine', '[MASK]', '[MASK]', '?', 'the', 'complexity', 'limits', 'limits', 'limits', 'limit', 'limit', 'use', 'use', 'use', 'use', 'use', 'use', 'problem', 'problem', 'limit', 'limits', 'limits', 'limits', 'limits', 'limits', 'limit', 'use', 'limit', 'limit', 'limit', 'use', 'work', 'limit', 'limits', 'limits', 'limits', 'limits', 'the', 'limits', 'limits', 'limits', 'problems', 'problems', 'problems', 'problems', 'problem', 'problems', 'limits', 'limits', 'limit', 'limit', 'limits', 'limits', 'limits', 'limit', 'limit', 'limit', 'limit', 'limit', 'problem', 'use', 'limit', 'limit', 'limit', 'limit', 'limit', 'limit', 'limits', 'limits', 'the', 'limits', 'limits', 'the', 'limits', 'limits', 'limits', 'limits', 'limit', 'problem', 'problem', 'problem', 'problem', 'problem', 'problem', 'problem', 'use', 'problem', 'problem', 'problem', 'problem', 'problem', 'problem', 'problem', 'problem', 'problem', 'problem', 'problem', 'problem', 'problem', 'problems', 'limits', 'limits', 'limits', 'limits', 'limits', 'limits', 'limits', 'limit', 'problem', 'problem', 'problem', 'problem', 'limit', 'limits', 'limits', 'limits', 'the', 'limits', 'limits', 'limits', 'limits', 'limit', 'complexity', 'complexity', 'complexity', 'problem', 'problem']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(np.argmax(output[0].data.numpy(), axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[CLS]', 0), ('a', 1), ('problem', 2), ('is', 3), ('regarded', 4), ('as', 5), ('inherently', 6), ('difficult', 7), ('if', 8), ('its', 9), ('solution', 10), ('requires', 11), ('significant', 12), ('resources', 13), (',', 14), ('whatever', 15), ('the', 16), ('algorithm', 17), ('used', 18), ('.', 19), ('the', 20), ('theory', 21), ('formal', 22), ('##izes', 23), ('this', 24), ('intuition', 25), (',', 26), ('by', 27), ('introducing', 28), ('mathematical', 29), ('models', 30), ('of', 31), ('computation', 32), ('to', 33), ('study', 34), ('these', 35), ('problems', 36), ('and', 37), ('quan', 38), ('##tify', 39), ('##ing', 40), ('the', 41), ('amount', 42), ('of', 43), ('resources', 44), ('needed', 45), ('to', 46), ('solve', 47), ('them', 48), (',', 49), ('such', 50), ('as', 51), ('time', 52), ('and', 53), ('storage', 54), ('.', 55), ('other', 56), ('complexity', 57), ('measures', 58), ('are', 59), ('also', 60), ('used', 61), (',', 62), ('such', 63), ('as', 64), ('the', 65), ('amount', 66), ('of', 67), ('communication', 68), ('(', 69), ('used', 70), ('in', 71), ('communication', 72), ('complexity', 73), (')', 74), (',', 75), ('the', 76), ('number', 77), ('of', 78), ('gates', 79), ('in', 80), ('a', 81), ('circuit', 82), ('(', 83), ('used', 84), ('in', 85), ('circuit', 86), ('complexity', 87), (')', 88), ('and', 89), ('the', 90), ('number', 91), ('of', 92), ('processors', 93), ('(', 94), ('used', 95), ('in', 96), ('parallel', 97), ('computing', 98), (')', 99), ('.', 100), ('one', 101), ('of', 102), ('the', 103), ('roles', 104), ('of', 105), ('computational', 106), ('complexity', 107), ('theory', 108), ('is', 109), ('to', 110), ('determine', 111), ('the', 112), ('practical', 113), ('limits', 114), ('on', 115), ('what', 116), ('computers', 117), ('can', 118), ('and', 119), ('cannot', 120), ('do', 121), ('.', 122), ('[SEP]', 123), ('what', 124), ('unit', 125), ('is', 126), ('measured', 127), ('to', 128), ('determine', 129), ('circuit', 130), ('simplicity', 131), ('?', 132), ('[SEP]', 133), ('[PAD]', 134), ('[PAD]', 135), ('[PAD]', 136), ('[PAD]', 137), ('[PAD]', 138), ('[PAD]', 139), ('[PAD]', 140), ('[PAD]', 141), ('[PAD]', 142), ('[PAD]', 143), ('[PAD]', 144), ('[PAD]', 145), ('[PAD]', 146), ('[PAD]', 147), ('[PAD]', 148), ('[PAD]', 149), ('[PAD]', 150), ('[PAD]', 151), ('[PAD]', 152), ('[PAD]', 153), ('[PAD]', 154), ('[PAD]', 155), ('[PAD]', 156), ('[PAD]', 157), ('[PAD]', 158), ('[PAD]', 159), ('[PAD]', 160), ('[PAD]', 161), ('[PAD]', 162), ('[PAD]', 163), ('[PAD]', 164), ('[PAD]', 165), ('[PAD]', 166), ('[PAD]', 167), ('[PAD]', 168), ('[PAD]', 169), ('[PAD]', 170), ('[PAD]', 171), ('[PAD]', 172), ('[PAD]', 173), ('[PAD]', 174), ('[PAD]', 175), ('[PAD]', 176), ('[PAD]', 177), ('[PAD]', 178), ('[PAD]', 179), ('[PAD]', 180), ('[PAD]', 181), ('[PAD]', 182), ('[PAD]', 183), ('[PAD]', 184), ('[PAD]', 185), ('[PAD]', 186), ('[PAD]', 187), ('[PAD]', 188), ('[PAD]', 189), ('[PAD]', 190), ('[PAD]', 191), ('[PAD]', 192), ('[PAD]', 193), ('[PAD]', 194), ('[PAD]', 195), ('[PAD]', 196), ('[PAD]', 197), ('[PAD]', 198), ('[PAD]', 199), ('[PAD]', 200), ('[PAD]', 201), ('[PAD]', 202), ('[PAD]', 203), ('[PAD]', 204), ('[PAD]', 205), ('[PAD]', 206), ('[PAD]', 207), ('[PAD]', 208), ('[PAD]', 209), ('[PAD]', 210), ('[PAD]', 211), ('[PAD]', 212), ('[PAD]', 213), ('[PAD]', 214), ('[PAD]', 215), ('[PAD]', 216), ('[PAD]', 217), ('[PAD]', 218), ('[PAD]', 219), ('[PAD]', 220), ('[PAD]', 221), ('[PAD]', 222), ('[PAD]', 223), ('[PAD]', 224), ('[PAD]', 225), ('[PAD]', 226), ('[PAD]', 227), ('[PAD]', 228), ('[PAD]', 229), ('[PAD]', 230), ('[PAD]', 231), ('[PAD]', 232), ('[PAD]', 233), ('[PAD]', 234), ('[PAD]', 235), ('[PAD]', 236), ('[PAD]', 237), ('[PAD]', 238), ('[PAD]', 239), ('[PAD]', 240), ('[PAD]', 241), ('[PAD]', 242), ('[PAD]', 243), ('[PAD]', 244), ('[PAD]', 245), ('[PAD]', 246), ('[PAD]', 247), ('[PAD]', 248), ('[PAD]', 249), ('[PAD]', 250), ('[PAD]', 251), ('[PAD]', 252), ('[PAD]', 253), ('[PAD]', 254), ('[PAD]', 255)]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip(tokenizer.convert_ids_to_tokens(input_ids.data.numpy()[0]), range(256))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['complexity'] tensor(9.9284)\n",
      "['the'] tensor(9.8887)\n",
      "['limits'] tensor(9.8850)\n",
      "['constraints'] tensor(9.7926)\n",
      "['problems'] tensor(8.9855)\n",
      "['limit'] tensor(8.6715)\n",
      "['limitations'] tensor(8.3517)\n",
      "['practical'] tensor(8.2851)\n",
      "['a'] tensor(8.0979)\n",
      "['problem'] tensor(7.9225)\n",
      "['implementation'] tensor(7.8894)\n",
      "['logic'] tensor(7.5877)\n",
      "['size'] tensor(7.5874)\n",
      "['rules'] tensor(7.4703)\n",
      "['issues'] tensor(7.4602)\n",
      "['computational'] tensor(7.3272)\n",
      "['restrictions'] tensor(7.2864)\n",
      "['theoretical'] tensor(7.1779)\n",
      "['speed'] tensor(7.1758)\n",
      "['efficiency'] tensor(7.1576)\n",
      "['time'] tensor(7.0341)\n",
      "['factors'] tensor(6.9810)\n",
      "['amount'] tensor(6.9632)\n",
      "['reduction'] tensor(6.9184)\n",
      "['##s'] tensor(6.8955)\n"
     ]
    }
   ],
   "source": [
    "c = Counter()\n",
    "for i, o in enumerate(output[0][134]):\n",
    "    c[i] = o\n",
    "for x, val in c.most_common(25):\n",
    "    print(tokenizer.convert_ids_to_tokens([x]), val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13298)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(output[0][120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'edgar'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.ids_to_tokens[9586] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* add end token to model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'when', 'considering', 'computational', 'problems', ',', 'a', 'problem', 'instance', 'is', 'a', 'string', 'over', 'an', 'alphabet', '.', 'usually', ',', 'the', 'alphabet', 'is', 'taken', 'to', 'be', 'the', 'binary', 'alphabet', '(', 'i', '.', 'e', '.', ',', 'the', 'set', '{', '0', ',', '1', '}', ')', ',', 'and', 'thus', 'the', 'strings', 'are', 'bits', '##tri', '##ng', '##s', '.', 'as', 'in', 'a', 'real', '-', 'world', 'computer', ',', 'mathematical', 'objects', 'other', 'than', 'bits', '##tri', '##ng', '##s', 'must', 'be', 'suit', '##ably', 'encoded', '.', 'for', 'example', ',', 'integers', 'can', 'be', 'represented', 'in', 'binary', 'notation', ',', 'and', 'graphs', 'can', 'be', 'encoded', 'directly', 'via', 'their', 'ad', '##ja', '##cen', '##cy', 'matrices', ',', 'or', 'by', 'encoding', 'their', 'ad', '##ja', '##cen', '##cy', 'lists', 'in', 'binary', '.', '[SEP]', 'what', 'is', 'one', 'way', 'in', 'which', '[MASK]', 'can', 'be', 'encoded', '?', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "target = 120 #50\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss_ans = 0\n",
    "    for batch_i, eval_batch in enumerate(eval_dataloader_ans):\n",
    "        eval_batch = tuple(t.to(device) for t in eval_batch)\n",
    "        input_ids, input_mask, segment_ids, lm_label_ids, is_next = eval_batch\n",
    "        if batch_i != target:\n",
    "            continue\n",
    "        if batch_i == target:\n",
    "            #input_ids[0][121] = 103\n",
    "            input_ids[0][118] = 103\n",
    "            #input_ids[0][117] = 103\n",
    "            print(tokenizer.convert_ids_to_tokens(input_ids.data.numpy()[0]))\n",
    "        output, _ = model(input_ids, segment_ids, input_mask, None, None)\n",
    "        if batch_i == target:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'when', 'considering', 'computational', 'problems', ',', 'a', 'problem', 'instance', 'is', 'a', 'string', 'over', 'an', 'alphabet', '.', 'usually', ',', 'the', 'alphabet', 'is', 'taken', 'to', 'be', 'the', 'binary', 'alphabet', '(', 'i', '.', 'e', '.', ',', 'the', 'set', '{', '0', ',', '1', '}', ')', ',', 'and', 'thus', 'the', 'strings', 'are', 'bits', '##tri', '##ng', '##s', '.', 'as', 'in', 'a', 'real', '-', 'world', 'computer', ',', 'mathematical', 'objects', 'other', 'than', 'bits', '##tri', '##ng', '##s', 'must', 'be', 'suit', '##ably', 'encoded', '.', 'for', 'example', ',', 'integers', 'can', 'be', 'represented', 'in', 'binary', 'notation', ',', 'and', 'graphs', 'can', 'be', 'encoded', 'directly', 'via', 'their', 'ad', '##ja', '##cen', '##cy', 'matrices', ',', 'or', 'by', 'encoding', 'their', 'ad', '##ja', '##cen', '##cy', 'lists', 'in', 'binary', '.', 'graphs', 'what', 'is', 'one', 'way', 'in', 'which', '[MASK]', 'can', 'be', 'encoded', '?', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'numbers', 'numbers', 'numbers', 'numbers', 'numbers', 'numbers', 'numbers', 'numbers', 'numbers', 'numbers', 'numbers', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'numbers', 'numbers', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'numbers', 'numbers', '##s', '##s', 'numbers', 'numbers', 'numbers', 'numbers', 'graphs', 'numbers', 'numbers', 'numbers', 'numbers', 'numbers', 'numbers', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'numbers', 'numbers', 'numbers', 'numbers', '##s', 'numbers', 'numbers', 'numbers', 'numbers', 'numbers', 'numbers', 'graphs', 'graphs', 'numbers', 'numbers', 'numbers', 'numbers', 'numbers', 'numbers', 'numbers', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'numbers', 'numbers', 'numbers', 'numbers', 'numbers', 'numbers', 'numbers', 'numbers', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'graphs', 'numbers', 'numbers', '##s', '##s', 'numbers', '##s', 'numbers', 'numbers', 'numbers', 'numbers', 'numbers', 'numbers', 'numbers', 'numbers', 'numbers']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(np.argmax(output[0].data.numpy(), axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[CLS]', 0), ('when', 1), ('considering', 2), ('computational', 3), ('problems', 4), (',', 5), ('a', 6), ('problem', 7), ('instance', 8), ('is', 9), ('a', 10), ('string', 11), ('over', 12), ('an', 13), ('alphabet', 14), ('.', 15), ('usually', 16), (',', 17), ('the', 18), ('alphabet', 19), ('is', 20), ('taken', 21), ('to', 22), ('be', 23), ('the', 24), ('binary', 25), ('alphabet', 26), ('(', 27), ('i', 28), ('.', 29), ('e', 30), ('.', 31), (',', 32), ('the', 33), ('set', 34), ('{', 35), ('0', 36), (',', 37), ('1', 38), ('}', 39), (')', 40), (',', 41), ('and', 42), ('thus', 43), ('the', 44), ('strings', 45), ('are', 46), ('bits', 47), ('##tri', 48), ('##ng', 49), ('##s', 50), ('.', 51), ('as', 52), ('in', 53), ('a', 54), ('real', 55), ('-', 56), ('world', 57), ('computer', 58), (',', 59), ('mathematical', 60), ('objects', 61), ('other', 62), ('than', 63), ('bits', 64), ('##tri', 65), ('##ng', 66), ('##s', 67), ('must', 68), ('be', 69), ('suit', 70), ('##ably', 71), ('encoded', 72), ('.', 73), ('for', 74), ('example', 75), (',', 76), ('integers', 77), ('can', 78), ('be', 79), ('represented', 80), ('in', 81), ('binary', 82), ('notation', 83), (',', 84), ('and', 85), ('graphs', 86), ('can', 87), ('be', 88), ('encoded', 89), ('directly', 90), ('via', 91), ('their', 92), ('ad', 93), ('##ja', 94), ('##cen', 95), ('##cy', 96), ('matrices', 97), (',', 98), ('or', 99), ('by', 100), ('encoding', 101), ('their', 102), ('ad', 103), ('##ja', 104), ('##cen', 105), ('##cy', 106), ('lists', 107), ('in', 108), ('binary', 109), ('.', 110), ('[SEP]', 111), ('what', 112), ('is', 113), ('one', 114), ('way', 115), ('in', 116), ('which', 117), ('graphs', 118), ('can', 119), ('be', 120), ('encoded', 121), ('?', 122), ('[SEP]', 123), ('[PAD]', 124), ('[PAD]', 125), ('[PAD]', 126), ('[PAD]', 127), ('[PAD]', 128), ('[PAD]', 129), ('[MASK]', 130), ('[MASK]', 131), ('[PAD]', 132), ('[PAD]', 133), ('[PAD]', 134), ('[PAD]', 135), ('[PAD]', 136), ('[PAD]', 137), ('[PAD]', 138), ('[PAD]', 139), ('[PAD]', 140), ('[PAD]', 141), ('[PAD]', 142), ('[PAD]', 143), ('[PAD]', 144), ('[PAD]', 145), ('[PAD]', 146), ('[PAD]', 147), ('[PAD]', 148), ('[PAD]', 149), ('[PAD]', 150), ('[PAD]', 151), ('[PAD]', 152), ('[PAD]', 153), ('[PAD]', 154), ('[PAD]', 155), ('[PAD]', 156), ('[PAD]', 157), ('[PAD]', 158), ('[PAD]', 159), ('[PAD]', 160), ('[PAD]', 161), ('[PAD]', 162), ('[PAD]', 163), ('[PAD]', 164), ('[PAD]', 165), ('[PAD]', 166), ('[PAD]', 167), ('[PAD]', 168), ('[PAD]', 169), ('[PAD]', 170), ('[PAD]', 171), ('[PAD]', 172), ('[PAD]', 173), ('[PAD]', 174), ('[PAD]', 175), ('[PAD]', 176), ('[PAD]', 177), ('[PAD]', 178), ('[PAD]', 179), ('[PAD]', 180), ('[PAD]', 181), ('[PAD]', 182), ('[PAD]', 183), ('[PAD]', 184), ('[PAD]', 185), ('[PAD]', 186), ('[PAD]', 187), ('[PAD]', 188), ('[PAD]', 189), ('[PAD]', 190), ('[PAD]', 191), ('[PAD]', 192), ('[PAD]', 193), ('[PAD]', 194), ('[PAD]', 195), ('[PAD]', 196), ('[PAD]', 197), ('[PAD]', 198), ('[PAD]', 199), ('[PAD]', 200), ('[PAD]', 201), ('[PAD]', 202), ('[PAD]', 203), ('[PAD]', 204), ('[PAD]', 205), ('[PAD]', 206), ('[PAD]', 207), ('[PAD]', 208), ('[PAD]', 209), ('[PAD]', 210), ('[PAD]', 211), ('[PAD]', 212), ('[PAD]', 213), ('[PAD]', 214), ('[PAD]', 215), ('[PAD]', 216), ('[PAD]', 217), ('[PAD]', 218), ('[PAD]', 219), ('[PAD]', 220), ('[PAD]', 221), ('[PAD]', 222), ('[PAD]', 223), ('[PAD]', 224), ('[PAD]', 225), ('[PAD]', 226), ('[PAD]', 227), ('[PAD]', 228), ('[PAD]', 229), ('[PAD]', 230), ('[PAD]', 231), ('[PAD]', 232), ('[PAD]', 233), ('[PAD]', 234), ('[PAD]', 235), ('[PAD]', 236), ('[PAD]', 237), ('[PAD]', 238), ('[PAD]', 239), ('[PAD]', 240), ('[PAD]', 241), ('[PAD]', 242), ('[PAD]', 243), ('[PAD]', 244), ('[PAD]', 245), ('[PAD]', 246), ('[PAD]', 247), ('[PAD]', 248), ('[PAD]', 249), ('[PAD]', 250), ('[PAD]', 251), ('[PAD]', 252), ('[PAD]', 253), ('[PAD]', 254), ('[PAD]', 255)]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip(tokenizer.convert_ids_to_tokens(input_ids.data.numpy()[0]), range(256))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'southern', 'california', 'is', 'home', 'to', 'many', 'major', 'business', 'districts', '.', 'central', 'business', 'districts', '(', 'cb', '##d', ')', 'include', 'downtown', 'los', 'angeles', ',', 'downtown', 'san', 'diego', ',', 'downtown', 'san', 'bernardino', ',', 'downtown', 'baker', '##sfield', ',', 'south', 'coast', 'metro', 'and', 'downtown', 'riverside', '.', '[SEP]', 'what', 'does', '[MASK]', '[MASK]', 'stand', 'for', '?', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "target = 420 #50\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss_ans = 0\n",
    "    for batch_i, eval_batch in enumerate(eval_dataloader_ans):\n",
    "        eval_batch = tuple(t.to(device) for t in eval_batch)\n",
    "        input_ids, input_mask, segment_ids, lm_label_ids, is_next = eval_batch\n",
    "        if batch_i != target:\n",
    "            continue\n",
    "        if batch_i == target:\n",
    "            input_ids[0][45] = 103\n",
    "            input_ids[0][46] = 103\n",
    "            #input_ids[0][117] = 103\n",
    "            print(tokenizer.convert_ids_to_tokens(input_ids.data.numpy()[0]))\n",
    "        output, _ = model(input_ids, segment_ids, input_mask, None, None)\n",
    "        if batch_i == target:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'southern', 'california', 'is', 'home', 'to', 'many', 'major', 'business', 'districts', '.', 'central', 'business', 'districts', '(', 'cb', '##d', ')', 'include', 'downtown', 'los', 'angeles', ',', 'downtown', 'san', 'diego', ',', 'downtown', 'san', 'bernardino', ',', 'downtown', 'baker', '##sfield', ',', 'south', 'coast', 'metro', 'and', 'downtown', 'riverside', '.', 'cb', 'what', 'does', '[MASK]', '[MASK]', 'stand', 'for', '?', 'cb', '##d', '##d', '##d', '##d', '##d', '##d', '##d', 'streets', '##d', '##d', 'streets', '##d', '##d', '##d', '##d', '##d', '##d', '##d', 'districts', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', 'streets', 'streets', 'streets', 'area', 'area', 'streets', 'streets', 'area', 'streets', '##d', '##d', '##d', 'area', 'area', 'districts', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', 'streets', 'streets', 'streets', 'area', 'streets', 'streets', '##d', '##d', '##d', '##d', '##d', '##d', 'streets', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', 'streets', 'streets', 'streets', 'streets', 'streets', 'area', '##d', 'streets', '##d', '##d', '##d', '##d', '##d', '##d', '##d', 'area', '##d', '##d', '##d', '##d', '##d', '##d', 'stand', '[MASK]', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', 'streets', '##d', 'streets', 'streets', '##d', 'streets', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '[MASK]', '##d', '[MASK]', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', 'streets', 'area', 'area', 'streets', 'streets', 'districts', 'area', '##d', 'area', 'area', 'area', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '[MASK]', '[MASK]', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', '##d', 'streets', 'streets']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(np.argmax(output[0].data.numpy(), axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[CLS]', 0), ('southern', 1), ('california', 2), ('is', 3), ('home', 4), ('to', 5), ('many', 6), ('major', 7), ('business', 8), ('districts', 9), ('.', 10), ('central', 11), ('business', 12), ('districts', 13), ('(', 14), ('cb', 15), ('##d', 16), (')', 17), ('include', 18), ('downtown', 19), ('los', 20), ('angeles', 21), (',', 22), ('downtown', 23), ('san', 24), ('diego', 25), (',', 26), ('downtown', 27), ('san', 28), ('bernardino', 29), (',', 30), ('downtown', 31), ('baker', 32), ('##sfield', 33), (',', 34), ('south', 35), ('coast', 36), ('metro', 37), ('and', 38), ('downtown', 39), ('riverside', 40), ('.', 41), ('[SEP]', 42), ('what', 43), ('does', 44), ('cb', 45), ('##d', 46), ('stand', 47), ('for', 48), ('?', 49), ('[SEP]', 50), ('[PAD]', 51), ('[PAD]', 52), ('[PAD]', 53), ('[PAD]', 54), ('[PAD]', 55), ('[PAD]', 56), ('[PAD]', 57), ('[PAD]', 58), ('[PAD]', 59), ('[PAD]', 60), ('[PAD]', 61), ('[PAD]', 62), ('[PAD]', 63), ('[PAD]', 64), ('[PAD]', 65), ('[PAD]', 66), ('[PAD]', 67), ('[PAD]', 68), ('[PAD]', 69), ('[PAD]', 70), ('[PAD]', 71), ('[PAD]', 72), ('[PAD]', 73), ('[PAD]', 74), ('[PAD]', 75), ('[PAD]', 76), ('[PAD]', 77), ('[PAD]', 78), ('[PAD]', 79), ('[PAD]', 80), ('[PAD]', 81), ('[PAD]', 82), ('[PAD]', 83), ('[PAD]', 84), ('[PAD]', 85), ('[PAD]', 86), ('[PAD]', 87), ('[PAD]', 88), ('[PAD]', 89), ('[PAD]', 90), ('[PAD]', 91), ('[PAD]', 92), ('[PAD]', 93), ('[PAD]', 94), ('[PAD]', 95), ('[PAD]', 96), ('[PAD]', 97), ('[PAD]', 98), ('[PAD]', 99), ('[PAD]', 100), ('[PAD]', 101), ('[PAD]', 102), ('[PAD]', 103), ('[PAD]', 104), ('[PAD]', 105), ('[PAD]', 106), ('[PAD]', 107), ('[PAD]', 108), ('[PAD]', 109), ('[PAD]', 110), ('[PAD]', 111), ('[PAD]', 112), ('[PAD]', 113), ('[PAD]', 114), ('[PAD]', 115), ('[PAD]', 116), ('[PAD]', 117), ('[MASK]', 118), ('[PAD]', 119), ('[PAD]', 120), ('[PAD]', 121), ('[PAD]', 122), ('[PAD]', 123), ('[PAD]', 124), ('[PAD]', 125), ('[PAD]', 126), ('[PAD]', 127), ('[PAD]', 128), ('[PAD]', 129), ('[PAD]', 130), ('[PAD]', 131), ('[PAD]', 132), ('[PAD]', 133), ('[PAD]', 134), ('[PAD]', 135), ('[PAD]', 136), ('[PAD]', 137), ('[PAD]', 138), ('[PAD]', 139), ('[PAD]', 140), ('[PAD]', 141), ('[PAD]', 142), ('[PAD]', 143), ('[PAD]', 144), ('[PAD]', 145), ('[PAD]', 146), ('[PAD]', 147), ('[PAD]', 148), ('[PAD]', 149), ('[PAD]', 150), ('[PAD]', 151), ('[PAD]', 152), ('[PAD]', 153), ('[PAD]', 154), ('[PAD]', 155), ('[PAD]', 156), ('[PAD]', 157), ('[PAD]', 158), ('[PAD]', 159), ('[PAD]', 160), ('[PAD]', 161), ('[PAD]', 162), ('[PAD]', 163), ('[PAD]', 164), ('[PAD]', 165), ('[PAD]', 166), ('[PAD]', 167), ('[PAD]', 168), ('[PAD]', 169), ('[PAD]', 170), ('[PAD]', 171), ('[PAD]', 172), ('[PAD]', 173), ('[PAD]', 174), ('[PAD]', 175), ('[PAD]', 176), ('[PAD]', 177), ('[PAD]', 178), ('[PAD]', 179), ('[PAD]', 180), ('[PAD]', 181), ('[PAD]', 182), ('[PAD]', 183), ('[PAD]', 184), ('[PAD]', 185), ('[PAD]', 186), ('[PAD]', 187), ('[PAD]', 188), ('[PAD]', 189), ('[PAD]', 190), ('[PAD]', 191), ('[PAD]', 192), ('[PAD]', 193), ('[PAD]', 194), ('[PAD]', 195), ('[PAD]', 196), ('[PAD]', 197), ('[PAD]', 198), ('[PAD]', 199), ('[PAD]', 200), ('[PAD]', 201), ('[PAD]', 202), ('[PAD]', 203), ('[PAD]', 204), ('[PAD]', 205), ('[PAD]', 206), ('[PAD]', 207), ('[PAD]', 208), ('[PAD]', 209), ('[PAD]', 210), ('[PAD]', 211), ('[PAD]', 212), ('[PAD]', 213), ('[PAD]', 214), ('[PAD]', 215), ('[PAD]', 216), ('[PAD]', 217), ('[PAD]', 218), ('[PAD]', 219), ('[PAD]', 220), ('[PAD]', 221), ('[PAD]', 222), ('[PAD]', 223), ('[PAD]', 224), ('[PAD]', 225), ('[PAD]', 226), ('[PAD]', 227), ('[PAD]', 228), ('[PAD]', 229), ('[PAD]', 230), ('[PAD]', 231), ('[PAD]', 232), ('[PAD]', 233), ('[PAD]', 234), ('[PAD]', 235), ('[PAD]', 236), ('[PAD]', 237), ('[PAD]', 238), ('[PAD]', 239), ('[PAD]', 240), ('[PAD]', 241), ('[PAD]', 242), ('[PAD]', 243), ('[PAD]', 244), ('[PAD]', 245), ('[PAD]', 246), ('[PAD]', 247), ('[PAD]', 248), ('[PAD]', 249), ('[PAD]', 250), ('[PAD]', 251), ('[PAD]', 252), ('[PAD]', 253), ('[PAD]', 254), ('[PAD]', 255)]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip(tokenizer.convert_ids_to_tokens(input_ids.data.numpy()[0]), range(256))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* add end character to prediction\n",
    "* eval only predicted masked terms\n",
    "* normalize over reasonable options\n",
    "    * handling multiple multi-token terms\n",
    "    * the model may prefer one entity but getting the probability of a secondary or third entity may be hard because each position is conditioned individually \n",
    "* designate different types of mask for different decision points (entity, attr, q-type)\n",
    "* implement automatic evaluation and exploration scripts\n",
    "\n",
    "## Main story / pitch\n",
    "* BERT is trained on huge amount of data and learns relationships / statistics of word correlations\n",
    "* This model is conditional generation where you are outputing either a copy of what is in the paragraph or a paraphrase\n",
    "* Most generated words should not be obvious by context but rather obvious by condition, and specific to context (ie. entities or attribtues)\n",
    "* The model should be more about learning patterns and relations rather than term co-occurrence statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'montpellier', 'was', 'among', 'the', 'most', 'important', 'of', 'the', '66', '\"', 'ville', '##s', 'de', 'sure', '##te', '\"', 'that', 'the', 'edict', 'of', '159', '##8', 'granted', 'to', 'the', 'hug', '##uen', '##ots', '.', 'the', 'city', \"'\", 's', 'political', 'institutions', 'and', 'the', 'university', 'were', 'all', 'handed', 'over', 'to', 'the', 'hug', '##uen', '##ots', '.', 'tension', 'with', 'paris', 'led', 'to', 'a', 'siege', 'by', 'the', 'royal', 'army', 'in', '1622', '.', 'peace', 'terms', 'called', 'for', 'the', 'di', '##sman', '##tling', 'of', 'the', 'city', \"'\", 's', 'fortifications', '.', 'a', 'royal', 'citadel', 'was', 'built', 'and', 'the', 'university', 'and', 'consulate', 'were', 'taken', 'over', 'by', 'the', 'catholic', 'party', '.', 'even', 'before', 'the', 'edict', 'of', 'ale', '##s', '(', '1629', ')', ',', 'protestant', 'rule', 'was', 'dead', 'and', 'the', 'ville', 'de', 'sure', '##te', 'was', 'no', 'more', '.', '[', 'citation', 'needed', ']', '[SEP]', 'in', 'what', 'year', 'did', '[MASK]', '[MASK]', 'in', 'montpellier', 'effectively', 'collapse', '?', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "target = 720 #50\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss_ans = 0\n",
    "    for batch_i, eval_batch in enumerate(eval_dataloader_unans):\n",
    "        eval_batch = tuple(t.to(device) for t in eval_batch)\n",
    "        input_ids, input_mask, segment_ids, lm_label_ids, is_next = eval_batch\n",
    "        if batch_i != target:\n",
    "            continue\n",
    "        if batch_i == target:\n",
    "            input_ids[0][130] = 103\n",
    "            input_ids[0][131] = 103\n",
    "            #input_ids[0][117] = 103\n",
    "            print(tokenizer.convert_ids_to_tokens(input_ids.data.numpy()[0]))\n",
    "        output, _ = model(input_ids, segment_ids, input_mask, None, None)\n",
    "        if batch_i == target:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[CLS]', 0), ('montpellier', 1), ('was', 2), ('among', 3), ('the', 4), ('most', 5), ('important', 6), ('of', 7), ('the', 8), ('66', 9), ('\"', 10), ('ville', 11), ('##s', 12), ('de', 13), ('sure', 14), ('##te', 15), ('\"', 16), ('that', 17), ('the', 18), ('edict', 19), ('of', 20), ('159', 21), ('##8', 22), ('granted', 23), ('to', 24), ('the', 25), ('hug', 26), ('##uen', 27), ('##ots', 28), ('.', 29), ('the', 30), ('city', 31), (\"'\", 32), ('s', 33), ('political', 34), ('institutions', 35), ('and', 36), ('the', 37), ('university', 38), ('were', 39), ('all', 40), ('handed', 41), ('over', 42), ('to', 43), ('the', 44), ('hug', 45), ('##uen', 46), ('##ots', 47), ('.', 48), ('tension', 49), ('with', 50), ('paris', 51), ('led', 52), ('to', 53), ('a', 54), ('siege', 55), ('by', 56), ('the', 57), ('royal', 58), ('army', 59), ('in', 60), ('1622', 61), ('.', 62), ('peace', 63), ('terms', 64), ('called', 65), ('for', 66), ('the', 67), ('di', 68), ('##sman', 69), ('##tling', 70), ('of', 71), ('the', 72), ('city', 73), (\"'\", 74), ('s', 75), ('fortifications', 76), ('.', 77), ('a', 78), ('royal', 79), ('citadel', 80), ('was', 81), ('built', 82), ('and', 83), ('the', 84), ('university', 85), ('and', 86), ('consulate', 87), ('were', 88), ('taken', 89), ('over', 90), ('by', 91), ('the', 92), ('catholic', 93), ('party', 94), ('.', 95), ('even', 96), ('before', 97), ('the', 98), ('edict', 99), ('of', 100), ('ale', 101), ('##s', 102), ('(', 103), ('1629', 104), (')', 105), (',', 106), ('protestant', 107), ('rule', 108), ('was', 109), ('dead', 110), ('and', 111), ('the', 112), ('ville', 113), ('de', 114), ('sure', 115), ('##te', 116), ('was', 117), ('no', 118), ('more', 119), ('.', 120), ('[', 121), ('citation', 122), ('needed', 123), (']', 124), ('[SEP]', 125), ('in', 126), ('what', 127), ('year', 128), ('did', 129), ('protestant', 130), ('rule', 131), ('in', 132), ('montpellier', 133), ('effectively', 134), ('collapse', 135), ('?', 136), ('[SEP]', 137), ('[PAD]', 138), ('[PAD]', 139), ('[PAD]', 140), ('[PAD]', 141), ('[PAD]', 142), ('[PAD]', 143), ('[PAD]', 144), ('[PAD]', 145), ('[PAD]', 146), ('[PAD]', 147), ('[PAD]', 148), ('[PAD]', 149), ('[PAD]', 150), ('[PAD]', 151), ('[PAD]', 152), ('[PAD]', 153), ('[PAD]', 154), ('[PAD]', 155), ('[PAD]', 156), ('[PAD]', 157), ('[PAD]', 158), ('[PAD]', 159), ('[PAD]', 160), ('[PAD]', 161), ('[PAD]', 162), ('[PAD]', 163), ('[PAD]', 164), ('[PAD]', 165), ('[PAD]', 166), ('[PAD]', 167), ('[PAD]', 168), ('[PAD]', 169), ('[PAD]', 170), ('[PAD]', 171), ('[PAD]', 172), ('[PAD]', 173), ('[PAD]', 174), ('[PAD]', 175), ('[PAD]', 176), ('[PAD]', 177), ('[PAD]', 178), ('[PAD]', 179), ('[PAD]', 180), ('[PAD]', 181), ('[PAD]', 182), ('[PAD]', 183), ('[PAD]', 184), ('[PAD]', 185), ('[PAD]', 186), ('[PAD]', 187), ('[PAD]', 188), ('[PAD]', 189), ('[PAD]', 190), ('[PAD]', 191), ('[PAD]', 192), ('[PAD]', 193), ('[PAD]', 194), ('[PAD]', 195), ('[PAD]', 196), ('[PAD]', 197), ('[PAD]', 198), ('[PAD]', 199), ('[PAD]', 200), ('[PAD]', 201), ('[PAD]', 202), ('[PAD]', 203), ('[PAD]', 204), ('[PAD]', 205), ('[PAD]', 206), ('[PAD]', 207), ('[PAD]', 208), ('[PAD]', 209), ('[PAD]', 210), ('[PAD]', 211), ('[PAD]', 212), ('[PAD]', 213), ('[PAD]', 214), ('[PAD]', 215), ('[PAD]', 216), ('[PAD]', 217), ('[PAD]', 218), ('[PAD]', 219), ('[PAD]', 220), ('[PAD]', 221), ('[PAD]', 222), ('[PAD]', 223), ('[PAD]', 224), ('[PAD]', 225), ('[PAD]', 226), ('[PAD]', 227), ('[PAD]', 228), ('[PAD]', 229), ('[PAD]', 230), ('[PAD]', 231), ('[PAD]', 232), ('[PAD]', 233), ('[PAD]', 234), ('[PAD]', 235), ('[PAD]', 236), ('[PAD]', 237), ('[PAD]', 238), ('[PAD]', 239), ('[PAD]', 240), ('[PAD]', 241), ('[PAD]', 242), ('[PAD]', 243), ('[PAD]', 244), ('[PAD]', 245), ('[PAD]', 246), ('[PAD]', 247), ('[PAD]', 248), ('[PAD]', 249), ('[PAD]', 250), ('[PAD]', 251), ('[PAD]', 252), ('[PAD]', 253), ('[PAD]', 254), ('[PAD]', 255)]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip(tokenizer.convert_ids_to_tokens(input_ids.data.numpy()[0]), range(256))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'montpellier', 'was', 'among', 'the', 'most', 'important', 'of', 'the', '66', '\"', 'ville', '##s', 'de', 'sure', '##te', '\"', 'that', 'the', 'edict', 'of', '159', '##8', 'granted', 'to', 'the', 'hug', '##uen', '##ots', '.', 'the', 'city', \"'\", 's', 'political', 'institutions', 'and', 'the', 'university', 'were', 'all', 'handed', 'over', 'to', 'the', 'hug', '##uen', '##ots', '.', 'tension', 'with', 'paris', 'led', 'to', 'a', 'siege', 'by', 'the', 'royal', 'army', 'in', '1622', '.', 'peace', 'terms', 'called', 'for', 'the', 'di', '##sman', '##tling', 'of', 'the', 'city', \"'\", 's', 'fortifications', '.', 'a', 'royal', 'citadel', 'was', 'built', 'and', 'the', 'university', 'and', 'consulate', 'were', 'taken', 'over', 'by', 'the', 'catholic', 'party', '.', 'even', 'before', 'the', 'edict', 'of', 'ale', '##s', '(', '1629', ')', ',', 'protestant', 'rule', 'was', 'dead', 'and', 'the', 'ville', 'de', 'sure', '##te', 'was', 'no', 'more', '.', '[', 'citation', 'needed', ']', 'the', 'in', 'what', 'year', 'did', '[MASK]', '[MASK]', 'in', 'montpellier', 'effectively', 'collapse', '?', 'the', 'government', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'regime', 'system', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', '[MASK]', 'rule', 'system', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', '##s', '##s', 'rule', 'rule', 'rule', 'rule', 'fortress', 'rule', 'rule', '##s', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule', 'rule']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(np.argmax(output[0].data.numpy(), axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['government'] tensor(12.8438)\n",
      "['rule'] tensor(12.0960)\n",
      "['monarchy'] tensor(11.6844)\n",
      "['regime'] tensor(9.7085)\n",
      "['democracy'] tensor(9.6858)\n",
      "['city'] tensor(9.6177)\n",
      "['system'] tensor(9.5993)\n",
      "['power'] tensor(9.5922)\n",
      "['institutions'] tensor(9.5776)\n",
      "['university'] tensor(9.5670)\n",
      "['society'] tensor(9.5314)\n",
      "['walls'] tensor(9.3728)\n",
      "['fortifications'] tensor(9.3701)\n",
      "['structure'] tensor(9.3055)\n",
      "['church'] tensor(9.2341)\n",
      "['religion'] tensor(9.1682)\n",
      "['order'] tensor(9.0762)\n",
      "['resistance'] tensor(9.0731)\n",
      "['castle'] tensor(8.7590)\n",
      "['republic'] tensor(8.6874)\n",
      "['catholicism'] tensor(8.6118)\n",
      "['economy'] tensor(8.5853)\n",
      "['institution'] tensor(8.5817)\n",
      "['constitution'] tensor(8.5799)\n",
      "['buildings'] tensor(8.4867)\n"
     ]
    }
   ],
   "source": [
    "c = Counter()\n",
    "for i, o in enumerate(output[0][138]):\n",
    "    c[i] = o\n",
    "for x, val in c.most_common(25):\n",
    "    print(tokenizer.convert_ids_to_tokens([x]), val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rule'] tensor(13.4089)\n",
      "['government'] tensor(12.4013)\n",
      "['monarchy'] tensor(11.1976)\n",
      "['system'] tensor(10.8194)\n",
      "['regime'] tensor(10.0926)\n",
      "['power'] tensor(9.8284)\n",
      "['structure'] tensor(9.3131)\n",
      "['control'] tensor(9.1633)\n",
      "['order'] tensor(9.0995)\n",
      "['city'] tensor(9.0672)\n",
      "['society'] tensor(8.9482)\n",
      "['institutions'] tensor(8.9404)\n",
      "['administration'] tensor(8.8783)\n",
      "['walls'] tensor(8.6860)\n",
      "['republic'] tensor(8.6619)\n",
      "['democracy'] tensor(8.5542)\n",
      "['resistance'] tensor(8.5440)\n",
      "['situation'] tensor(8.5289)\n",
      "['constitution'] tensor(8.5061)\n",
      "['state'] tensor(8.4401)\n",
      "['empire'] tensor(8.1867)\n",
      "['buildings'] tensor(8.1807)\n",
      "['institution'] tensor(8.1483)\n",
      "['castle'] tensor(8.1160)\n",
      "['laws'] tensor(8.0726)\n"
     ]
    }
   ],
   "source": [
    "c = Counter()\n",
    "for i, o in enumerate(output[0][139]):\n",
    "    c[i] = o\n",
    "for x, val in c.most_common(25):\n",
    "    print(tokenizer.convert_ids_to_tokens([x]), val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(context_ids.data.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(question_ids)\n",
    "print(tokenizer.convert_ids_to_tokens(question_ids.data.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = output.data.numpy()\n",
    "print(tokenizer.convert_ids_to_tokens(np.argmax(o[0], axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(np.argmax(o[0,i,:]), np.exp(np.max(o[0,i,:]))/ np.sum(np.exp(o[0,i,:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(o.shape[2]):\n",
    "    c[i] = np.exp(o[0,7,i])/ np.sum(np.exp(o[0,7,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, score in c.most_common()[:25]:\n",
    "    print(tokenizer.convert_ids_to_tokens([idx]), score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[2435] # normandy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss_ans = 0\n",
    "    for batch_i, eval_batch in enumerate(eval_dataloader_unans):\n",
    "        eids = eval_batch[-1]\n",
    "        eval_batch = tuple(t.to(device) for t in eval_batch[:-1])\n",
    "        question_ids, question_mask, context_ids, context_mask, targets = eval_batch\n",
    "        output, _ = model(context_ids, context_mask, question_ids, question_mask)\n",
    "        loss = criterion(output.view(-1, len(tokenizer.vocab)), question_ids.view(-1))\n",
    "        eval_loss_ans += loss.item()\n",
    "        break\n",
    "        if loss.item() > 0.01:\n",
    "            print(batch_i, eval_loss_ans)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(question_ids)\n",
    "print(tokenizer.convert_ids_to_tokens(question_ids.data.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = output.data.numpy()\n",
    "print(o)\n",
    "print(tokenizer.convert_ids_to_tokens(np.argmax(o[0], axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print(np.argmax(o[0,i,:]), np.exp(np.max(o[0,i,:]))/ np.sum(np.exp(o[0,i,:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interesting Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def print_details(context_ids, question_ids, output, loss, tokenizer):\n",
    "    print(\"Loss:\", loss.item())\n",
    "    print(\"CONTEXT\")\n",
    "    print(tokenizer.convert_ids_to_tokens(context_ids.data.numpy()[0]))\n",
    "    print(\"~\"*30)\n",
    "    print(\"QUESTION\")\n",
    "    q_ids = [i for i in question_ids.data.numpy()[0] if i != 0]\n",
    "    print(q_ids)\n",
    "    q_toks = [tok for tok in tokenizer.convert_ids_to_tokens(question_ids.data.numpy()[0]) if tok != '[PAD]']\n",
    "    print(q_toks)\n",
    "    print(\"~\"*30)\n",
    "    print(\"OUTPUT\")\n",
    "    o = output.data.numpy()\n",
    "    out_ids = [i for i in np.argmax(o[0], axis=1) if i != 0]\n",
    "    out_toks = [tok for tok in tokenizer.convert_ids_to_tokens(np.argmax(o[0], axis=1)) if tok != '[PAD]']\n",
    "    scores = [(np.argmax(o[0,i,:]), np.exp(np.max(o[0,i,:]))/ np.sum(np.exp(o[0,i,:]))) for i in range(len(out_toks))]\n",
    "    print(out_toks)\n",
    "    print(list(zip(out_toks, scores)))\n",
    "    print(\"~\"*30)\n",
    "    print(\"TOP K FOR INCORRECT TERMS:\")\n",
    "    for tok_i, (tar, out) in enumerate(zip(q_ids[1:], out_ids)):\n",
    "        if tar != out:\n",
    "            print (\"Output\", out_toks[tok_i], \"instead of \", q_toks[tok_i+1])\n",
    "            c = Counter()\n",
    "            for i in range(o.shape[2]):\n",
    "                c[i] = np.exp(o[0,tok_i,i])/ np.sum(np.exp(o[0,tok_i,:]))\n",
    "            for idx, score in c.most_common()[:10]:\n",
    "                print(\"- \\t\",tokenizer.convert_ids_to_tokens([idx]), score)\n",
    "    print(\"#\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore = {eval_dataloader_ans: [0,1], eval_dataloader_unans: [0, 15]}\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss_ans = 0\n",
    "    for dataloader in explore.keys():\n",
    "        for batch_i, eval_batch in enumerate(dataloader):\n",
    "            eids = eval_batch[-1]\n",
    "            eval_batch = tuple(t.to(device) for t in eval_batch[:-1])\n",
    "            question_ids, question_mask, context_ids, context_mask, targets = eval_batch\n",
    "            output, _ = model(context_ids, context_mask, question_ids, question_mask)\n",
    "            loss = criterion(output.view(-1, len(tokenizer.vocab)), targets.view(-1))\n",
    "            eval_loss_ans += loss.item()\n",
    "            if batch_i in explore[dataloader]:\n",
    "                if dataloader == eval_dataloader_ans:\n",
    "                    print(\"Answerable:\",batch_i)\n",
    "                else:\n",
    "                    print(\"Unanswerable:\",batch_i)\n",
    "                print_details(context_ids, question_ids, output, loss, tokenizer)\n",
    "            if batch_i > max(explore[dataloader]):\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "* LM was suprisingly good at guessing question type and general structure of the question. It may be because the BERT representation is leaky in terms of representation  \n",
    "    - may be interesting result of its own ... Q: How much does the feature of each BERT word contain information about the surrounding BERT words?  \n",
    "    - this is probably why the model is pretty good at guessing Q type from the [CLS] token  \n",
    "* the Sentence repr is not enough represent specific entities in the text  \n",
    "    - gets confused between normandy and france and mongolia and normans   \n",
    "    - this might be because I don't train the sentence repr which is probably a mistake and we should retrain this one with trained sentence repr and forward masking in the question repr\n",
    "    - Maybe we can do an attention over entities? \n",
    "    - There is a bias towards more common entities right now (ie. john and paris instead of rollo and normandy)\n",
    "    - the fact that stuff like Rollo and dates were predicted well means there is leakiness in the representation\n",
    "* rank may be more important than loss since sometimes the #1 option has very high prob but number 2 is really good (eg. arrive vs begin U15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
