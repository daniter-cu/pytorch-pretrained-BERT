{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"examples/\")\n",
    "\n",
    "import logging\n",
    "import argparse\n",
    "import json\n",
    "from tqdm import tqdm, trange\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertModel \n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_autoreg_eval import BERTDataset, RNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args\n",
    "gradient_accumulation_steps = 1\n",
    "train_batch_size = 1\n",
    "eval_file = \"dataset/dev-v2.0.json\"\n",
    "max_seq_length=128\n",
    "on_memory = True\n",
    "bert_model = \"autoreg_model_lm_noft/pytorch_model9.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/15/2019 11:12:14 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/daniter/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "Loading Squad: 100%|██████████| 35/35 [00:00<00:00, 715.28it/s]\n",
      "Loading Squad: 100%|██████████| 35/35 [00:00<00:00, 3620.51it/s]\n",
      "02/15/2019 11:12:16 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/daniter/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "02/15/2019 11:12:16 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /Users/daniter/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/xx/8h5l1j614vv5wmbx9fbj69wm0000gn/T/tmp2xfp9rsi\n",
      "02/15/2019 11:12:19 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "02/15/2019 11:12:21 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/daniter/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "02/15/2019 11:12:21 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /Users/daniter/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/xx/8h5l1j614vv5wmbx9fbj69wm0000gn/T/tmpkpnap916\n",
      "02/15/2019 11:12:24 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the vocab size: 30522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniter/anaconda3/envs/ptbert/lib/python3.7/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "\n",
    "# Load eval_data\n",
    "eval_dataset_answerable = BERTDataset(eval_file, tokenizer, seq_len=max_seq_length,\n",
    "                            on_memory=on_memory, answerable=True)\n",
    "eval_dataset_unanswerable = BERTDataset(eval_file, tokenizer, seq_len=max_seq_length,\n",
    "                           on_memory=on_memory, answerable=False)\n",
    "\n",
    "# Prepare model\n",
    "model_state_dict = torch.load(bert_model, map_location='cpu') #TODO daniter: remove this map_location\n",
    "## TODO daniter: check if bert model is being loaded correctly\n",
    "context_model = BertModel.from_pretrained(\"bert-base-uncased\")#, state_dict=model_state_dict)\n",
    "question_model = BertModel.from_pretrained(\"bert-base-uncased\")#, state_dict=model_state_dict)\n",
    "context_model.to(device)\n",
    "question_model.to(device)\n",
    "\n",
    "\n",
    "# Prepare optimizer\n",
    "print(\"Checking the vocab size:\", len(tokenizer.vocab))\n",
    "# 768 is bert hidden size, 256 is GRU hidden size, 1 is the layers in the GRU\n",
    "model = RNNModel(\"GRU\", len(tokenizer.vocab), 768, 768, 1, context_model, question_model, ngpu=n_gpu)\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.to(device)\n",
    "\n",
    "# eval loader\n",
    "eval_sampler_ans = SequentialSampler(eval_dataset_answerable)\n",
    "eval_dataloader_ans = DataLoader(eval_dataset_answerable, sampler=eval_sampler_ans,\n",
    "                                 batch_size=train_batch_size)\n",
    "eval_sampler_unans = SequentialSampler(eval_dataset_unanswerable)\n",
    "eval_dataloader_unans = DataLoader(eval_dataset_unanswerable, sampler=eval_sampler_unans,\n",
    "                                   batch_size=train_batch_size)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.init_hidden(train_batch_size)\n",
    "pass\n",
    "# with torch.no_grad():\n",
    "#     model.eval()\n",
    "\n",
    "#     eval_loss_ans = 0\n",
    "#     for batch_i, eval_batch in enumerate(eval_dataloader_ans):\n",
    "#         assert False\n",
    "#         if batch_i % 1000 == 0:\n",
    "#             print(\"#### DANITER completed answerable\", batch_i)\n",
    "#         eids = eval_batch[-1]\n",
    "#         eval_batch = tuple(t.to(device) for t in eval_batch[:-1])\n",
    "#         question_ids, question_mask, context_ids, context_mask, targets = eval_batch\n",
    "#         output, _ = model(context_ids, context_mask, question_ids, question_mask)\n",
    "#         loss = criterion(output.view(-1, len(tokenizer.vocab)), question_ids.view(-1))\n",
    "#         eval_loss_ans += loss.item()\n",
    "#     print(\"##### DANITER EVAL LOSS IS (ANSWERABLE) : \", eval_loss_ans)\n",
    "\n",
    "#     eval_loss_unans = 0\n",
    "#     for batch_i, eval_batch in enumerate(eval_dataloader_unans):\n",
    "#         if batch_i % 1000 == 0:\n",
    "#             print(\"#### DANITER completed unanswerable\", batch_i)\n",
    "#         eids = eval_batch[-1]\n",
    "#         eval_batch = tuple(t.to(device) for t in eval_batch[:-1])\n",
    "#         question_ids, question_mask, context_ids, context_mask, targets = eval_batch\n",
    "#         output, _ = model(context_ids, context_mask, question_ids, question_mask)\n",
    "#         loss = criterion(output.view(-1, len(tokenizer.vocab)), question_ids.view(-1))\n",
    "#         eval_loss_unans += loss.item()\n",
    "#     print(\"##### DANITER EVAL LOSS IS (UNANSWERABLE) : \", eval_loss_unans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss_ans = 0\n",
    "    for batch_i, eval_batch in enumerate(eval_dataloader_ans):\n",
    "        eids = eval_batch[-1]\n",
    "        eval_batch = tuple(t.to(device) for t in eval_batch[:-1])\n",
    "        question_ids, question_mask, context_ids, context_mask, targets = eval_batch\n",
    "        output, _ = model(context_ids, context_mask, question_ids, question_mask)\n",
    "        loss = criterion(output.view(-1, len(tokenizer.vocab)), targets.view(-1))\n",
    "        eval_loss_ans += loss.item()\n",
    "        if batch_i > 10:\n",
    "            break\n",
    "#         if loss.item() > 0.01:\n",
    "#             print(batch_i, eval_loss_ans)\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38543012738227844\n"
     ]
    }
   ],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'in', 'the', 'course', 'of', 'the', '10th', 'century', ',', 'the', 'initially', 'destructive', 'inc', '##urs', '##ions', 'of', 'norse', 'war', 'bands', 'into', 'the', 'rivers', 'of', 'france', 'evolved', 'into', 'more', 'permanent', 'en', '##camp', '##ments', 'that', 'included', 'local', 'women', 'and', 'personal', 'property', '.', 'the', 'duchy', 'of', 'normandy', ',', 'which', 'began', 'in', '911', 'as', 'a', 'fi', '##ef', '##dom', ',', 'was', 'established', 'by', 'the', 'treaty', 'of', 'saint', '-', 'clair', '-', 'sur', '-', 'ep', '##te', 'between', 'king', 'charles', 'iii', 'of', 'west', 'fran', '##cia', 'and', 'the', 'famed', 'viking', 'ruler', 'roll', '##o', ',', 'and', 'was', 'situated', 'in', 'the', 'former', 'frankish', 'kingdom', 'of', 'ne', '##ust', '##ria', '.', 'the', 'treaty', 'offered', 'roll', '##o', 'and', 'his', 'men', 'the', 'french', 'lands', 'between', 'the', 'river', 'ep', '##te', 'and', 'the', 'atlantic', 'coast', 'in', 'exchange', 'for', 'their', 'protection', 'against', 'further', 'viking', 'inc', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(context_ids.data.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2040,  2106,  4897,  2080,  3696,  1996,  5036,  1997,  3002,\n",
      "          1011, 17936,  1011,  7505,  1011,  4958,  2618,  2007,  1029,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "['[CLS]', 'who', 'did', 'roll', '##o', 'sign', 'the', 'treaty', 'of', 'saint', '-', 'clair', '-', 'sur', '-', 'ep', '##te', 'with', '?', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(question_ids)\n",
    "print(tokenizer.convert_ids_to_tokens(question_ids.data.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['who', 'did', 'john', '##o', 'establish', 'the', 'treaty', 'of', 'tie', '-', 'john', 'de', 'de', '-', 'en', '##te', '?', '?', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "o = output.data.numpy()\n",
    "print(tokenizer.convert_ids_to_tokens(np.argmax(o[0], axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013 0.42656258\n",
      "2029 0.99888664\n",
      "3032 0.98821425\n",
      "2106 0.96953803\n",
      "1996 0.7692266\n",
      "7658 0.3381953\n",
      "21754 0.91210896\n",
      "2013 0.755071\n",
      "0 0.99999976\n",
      "0 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(np.argmax(o[0,i,:]), np.exp(np.max(o[0,i,:]))/ np.sum(np.exp(o[0,i,:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(o.shape[2]):\n",
    "    c[i] = np.exp(o[0,7,i])/ np.sum(np.exp(o[0,7,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from'] 0.755071\n",
      "['?'] 0.24354506\n",
      "['between'] 0.00030850538\n",
      "['and'] 0.00017943446\n",
      "['in'] 0.00016526289\n",
      "['originate'] 0.00013509695\n",
      "['with'] 0.00012702325\n",
      "['out'] 5.2344498e-05\n",
      "['to'] 4.9995124e-05\n",
      "['after'] 4.5036806e-05\n",
      "['during'] 3.9964147e-05\n",
      "['for'] 3.017783e-05\n",
      "['primarily'] 2.6414084e-05\n",
      "['into'] 2.418289e-05\n",
      "['instead'] 1.4401603e-05\n",
      "['or'] 1.1509827e-05\n",
      "['by'] 9.858294e-06\n",
      "['north'] 8.104101e-06\n",
      "[','] 6.5176055e-06\n",
      "['towards'] 5.350587e-06\n",
      "['originating'] 5.0490876e-06\n",
      "['located'] 4.877459e-06\n",
      "['which'] 3.9955703e-06\n",
      "['that'] 3.991282e-06\n",
      "['acquired'] 3.5135242e-06\n"
     ]
    }
   ],
   "source": [
    "for idx, score in c.most_common()[:25]:\n",
    "    print(tokenizer.convert_ids_to_tokens([idx]), score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0006109915"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[2435] # normandy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss_ans = 0\n",
    "    for batch_i, eval_batch in enumerate(eval_dataloader_unans):\n",
    "        eids = eval_batch[-1]\n",
    "        eval_batch = tuple(t.to(device) for t in eval_batch[:-1])\n",
    "        question_ids, question_mask, context_ids, context_mask, targets = eval_batch\n",
    "        output, _ = model(context_ids, context_mask, question_ids, question_mask)\n",
    "        loss = criterion(output.view(-1, len(tokenizer.vocab)), question_ids.view(-1))\n",
    "        eval_loss_ans += loss.item()\n",
    "        break\n",
    "        if loss.item() > 0.01:\n",
    "            print(batch_i, eval_loss_ans)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0606985092163086e-05"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_loss_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2040,  2435,  2037,  2171,  2000, 13298,  1999,  1996,  6694,\n",
      "          1005,  1055,  1998, 22096,  1005,  1055,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "['[CLS]', 'who', 'gave', 'their', 'name', 'to', 'normandy', 'in', 'the', '1000', \"'\", 's', 'and', '1100', \"'\", 's', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(question_ids)\n",
    "print(tokenizer.convert_ids_to_tokens(question_ids.data.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 4.9845333  -0.5840121  -1.7072299  ... -0.8986035  -0.7701622\n",
      "   -2.4625373 ]\n",
      "  [ 5.5687795  -3.1512156  -1.5189477  ... -2.2867513  -0.81157726\n",
      "   -1.9473257 ]\n",
      "  [ 1.8056442  -2.4558816  -1.3853037  ... -1.5242887  -1.9538682\n",
      "   -2.677875  ]\n",
      "  ...\n",
      "  [33.50697    -1.898835   -3.0913079  ... -3.4296584  -3.0715473\n",
      "   -2.3606715 ]\n",
      "  [33.482018   -1.8573587  -3.0754821  ... -3.4108062  -3.087913\n",
      "   -2.3764603 ]\n",
      "  [33.43003    -1.8635046  -3.1077945  ... -3.40165    -3.1059961\n",
      "   -2.4096801 ]]]\n",
      "['[CLS]', 'who', 'gave', 'their', 'name', 'to', 'normandy', 'in', 'the', '1000', \"'\", 's', 'and', '1100', \"'\", 's', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "o = output.data.numpy()\n",
    "print(o)\n",
    "print(tokenizer.convert_ids_to_tokens(np.argmax(o[0], axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 1.0\n",
      "2040 0.99998236\n",
      "2435 0.99976426\n",
      "2037 0.99991757\n",
      "2171 0.999995\n",
      "2000 0.99999946\n",
      "13298 0.9999513\n",
      "1999 0.99999833\n",
      "1996 0.99999017\n",
      "6694 0.998527\n",
      "1005 0.99999917\n",
      "1055 0.999995\n",
      "1998 0.99999845\n",
      "22096 0.99808013\n",
      "1005 0.9999972\n",
      "1055 0.99999726\n",
      "0 0.99986583\n",
      "0 0.99999994\n",
      "0 1.0\n",
      "0 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(np.argmax(o[0,i,:]), np.exp(np.max(o[0,i,:]))/ np.sum(np.exp(o[0,i,:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interesting Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def print_details(context_ids, question_ids, output, loss, tokenizer):\n",
    "    print(\"Loss:\", loss.item())\n",
    "    print(\"CONTEXT\")\n",
    "    print(tokenizer.convert_ids_to_tokens(context_ids.data.numpy()[0]))\n",
    "    print(\"~\"*30)\n",
    "    print(\"QUESTION\")\n",
    "    q_ids = [i for i in question_ids.data.numpy()[0] if i != 0]\n",
    "    print(q_ids)\n",
    "    q_toks = [tok for tok in tokenizer.convert_ids_to_tokens(question_ids.data.numpy()[0]) if tok != '[PAD]']\n",
    "    print(q_toks)\n",
    "    print(\"~\"*30)\n",
    "    print(\"OUTPUT\")\n",
    "    o = output.data.numpy()\n",
    "    out_ids = [i for i in np.argmax(o[0], axis=1) if i != 0]\n",
    "    out_toks = [tok for tok in tokenizer.convert_ids_to_tokens(np.argmax(o[0], axis=1)) if tok != '[PAD]']\n",
    "    scores = [(np.argmax(o[0,i,:]), np.exp(np.max(o[0,i,:]))/ np.sum(np.exp(o[0,i,:]))) for i in range(len(out_toks))]\n",
    "    print(out_toks)\n",
    "    print(list(zip(out_toks, scores)))\n",
    "    print(\"~\"*30)\n",
    "    print(\"TOP K FOR INCORRECT TERMS:\")\n",
    "    for tok_i, (tar, out) in enumerate(zip(q_ids[1:], out_ids)):\n",
    "        if tar != out:\n",
    "            print (\"Output\", out_toks[tok_i], \"instead of \", q_toks[tok_i+1])\n",
    "            c = Counter()\n",
    "            for i in range(o.shape[2]):\n",
    "                c[i] = np.exp(o[0,tok_i,i])/ np.sum(np.exp(o[0,tok_i,:]))\n",
    "            for idx, score in c.most_common()[:10]:\n",
    "                print(\"- \\t\",tokenizer.convert_ids_to_tokens([idx]), score)\n",
    "    print(\"#\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answerable: 0\n",
      "Loss: 0.07345549762248993\n",
      "CONTEXT\n",
      "['[CLS]', 'the', 'norman', '##s', '(', 'norman', ':', 'no', '##ur', '##man', '##ds', ';', 'french', ':', 'norman', '##ds', ';', 'latin', ':', 'norman', '##ni', ')', 'were', 'the', 'people', 'who', 'in', 'the', '10th', 'and', '11th', 'centuries', 'gave', 'their', 'name', 'to', 'normandy', ',', 'a', 'region', 'in', 'france', '.', 'they', 'were', 'descended', 'from', 'norse', '(', '\"', 'norman', '\"', 'comes', 'from', '\"', 'norse', '##man', '\"', ')', 'raiders', 'and', 'pirates', 'from', 'denmark', ',', 'iceland', 'and', 'norway', 'who', ',', 'under', 'their', 'leader', 'roll', '##o', ',', 'agreed', 'to', 'swear', 'fe', '##al', '##ty', 'to', 'king', 'charles', 'iii', 'of', 'west', 'fran', '##cia', '.', 'through', 'generations', 'of', 'assimilation', 'and', 'mixing', 'with', 'the', 'native', 'frankish', 'and', 'roman', '-', 'gaul', '##ish', 'populations', ',', 'their', 'descendants', 'would', 'gradually', 'merge', 'with', 'the', 'carol', '##ing', '##ian', '-', 'based', 'cultures', 'of', 'west', 'fran', '##cia', '.', '[PAD]', '[PAD]']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "QUESTION\n",
      "[101, 1999, 2054, 2406, 2003, 13298, 2284, 1029]\n",
      "['[CLS]', 'in', 'what', 'country', 'is', 'normandy', 'located', '?']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "OUTPUT\n",
      "['in', 'what', 'country', 'is', 'paris', 'located', '?']\n",
      "[('in', (1999, 0.79822695)), ('what', (2054, 0.9998678)), ('country', (2406, 0.96969473)), ('is', (2003, 0.9404071)), ('paris', (3000, 0.23604622)), ('located', (2284, 0.9994947)), ('?', (1029, 0.9993846))]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "TOP K FOR INCORRECT TERMS:\n",
      "Output paris instead of  normandy\n",
      "- \t ['paris'] 0.23604622\n",
      "- \t ['fort'] 0.17937389\n",
      "- \t ['strasbourg'] 0.06697646\n",
      "- \t ['calais'] 0.04628686\n",
      "- \t ['sicily'] 0.0322827\n",
      "- \t ['hanover'] 0.021631142\n",
      "- \t ['alsace'] 0.021194316\n",
      "- \t ['palermo'] 0.015181807\n",
      "- \t ['the'] 0.012416687\n",
      "- \t ['villa'] 0.010885019\n",
      "##############################\n",
      "Answerable: 1\n",
      "Loss: 0.02413848042488098\n",
      "CONTEXT\n",
      "['[CLS]', 'the', 'norman', '##s', '(', 'norman', ':', 'no', '##ur', '##man', '##ds', ';', 'french', ':', 'norman', '##ds', ';', 'latin', ':', 'norman', '##ni', ')', 'were', 'the', 'people', 'who', 'in', 'the', '10th', 'and', '11th', 'centuries', 'gave', 'their', 'name', 'to', 'normandy', ',', 'a', 'region', 'in', 'france', '.', 'they', 'were', 'descended', 'from', 'norse', '(', '\"', 'norman', '\"', 'comes', 'from', '\"', 'norse', '##man', '\"', ')', 'raiders', 'and', 'pirates', 'from', 'denmark', ',', 'iceland', 'and', 'norway', 'who', ',', 'under', 'their', 'leader', 'roll', '##o', ',', 'agreed', 'to', 'swear', 'fe', '##al', '##ty', 'to', 'king', 'charles', 'iii', 'of', 'west', 'fran', '##cia', '.', 'through', 'generations', 'of', 'assimilation', 'and', 'mixing', 'with', 'the', 'native', 'frankish', 'and', 'roman', '-', 'gaul', '##ish', 'populations', ',', 'their', 'descendants', 'would', 'gradually', 'merge', 'with', 'the', 'carol', '##ing', '##ian', '-', 'based', 'cultures', 'of', 'west', 'fran', '##cia', '.', '[PAD]', '[PAD]']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "QUESTION\n",
      "[101, 2043, 2020, 1996, 5879, 2015, 1999, 13298, 1029]\n",
      "['[CLS]', 'when', 'were', 'the', 'norman', '##s', 'in', 'normandy', '?']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "OUTPUT\n",
      "['when', 'were', 'the', 'mongols', '##s', 'in', 'normandy', '?']\n",
      "[('when', (2043, 0.999721)), ('were', (2020, 0.97182703)), ('the', (1996, 0.99878526)), ('mongols', (22235, 0.1639758)), ('##s', (2015, 0.9999883)), ('in', (1999, 0.8997382)), ('normandy', (13298, 0.92034227)), ('?', (1029, 0.99988294))]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "TOP K FOR INCORRECT TERMS:\n",
      "Output mongols instead of  norman\n",
      "- \t ['mongols'] 0.1639758\n",
      "- \t ['germans'] 0.14179815\n",
      "- \t ['new'] 0.058425177\n",
      "- \t ['norman'] 0.056647338\n",
      "- \t ['mori'] 0.054002207\n",
      "- \t ['manchu'] 0.043946\n",
      "- \t ['french'] 0.036286015\n",
      "- \t ['um'] 0.030260703\n",
      "- \t ['40th'] 0.025814869\n",
      "- \t ['walden'] 0.024079964\n",
      "##############################\n",
      "Unanswerable: 0\n",
      "Loss: 0.25672146677970886\n",
      "CONTEXT\n",
      "['[CLS]', 'the', 'norman', '##s', '(', 'norman', ':', 'no', '##ur', '##man', '##ds', ';', 'french', ':', 'norman', '##ds', ';', 'latin', ':', 'norman', '##ni', ')', 'were', 'the', 'people', 'who', 'in', 'the', '10th', 'and', '11th', 'centuries', 'gave', 'their', 'name', 'to', 'normandy', ',', 'a', 'region', 'in', 'france', '.', 'they', 'were', 'descended', 'from', 'norse', '(', '\"', 'norman', '\"', 'comes', 'from', '\"', 'norse', '##man', '\"', ')', 'raiders', 'and', 'pirates', 'from', 'denmark', ',', 'iceland', 'and', 'norway', 'who', ',', 'under', 'their', 'leader', 'roll', '##o', ',', 'agreed', 'to', 'swear', 'fe', '##al', '##ty', 'to', 'king', 'charles', 'iii', 'of', 'west', 'fran', '##cia', '.', 'through', 'generations', 'of', 'assimilation', 'and', 'mixing', 'with', 'the', 'native', 'frankish', 'and', 'roman', '-', 'gaul', '##ish', 'populations', ',', 'their', 'descendants', 'would', 'gradually', 'merge', 'with', 'the', 'carol', '##ing', '##ian', '-', 'based', 'cultures', 'of', 'west', 'fran', '##cia', '.', '[PAD]', '[PAD]']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "QUESTION\n",
      "[101, 2040, 2435, 2037, 2171, 2000, 13298, 1999, 1996, 6694, 1005, 1055, 1998, 22096, 1005, 1055]\n",
      "['[CLS]', 'who', 'gave', 'their', 'name', 'to', 'normandy', 'in', 'the', '1000', \"'\", 's', 'and', '1100', \"'\", 's']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "OUTPUT\n",
      "['who', 'had', 'their', 'name', 'to', 'the', 'during', 'the', '1600', \"'\", 's', 'and', '1800', \"'\", 's', 's']\n",
      "[('who', (2040, 0.99445945)), ('had', (2018, 0.21659361)), ('their', (2037, 0.97804654)), ('name', (2171, 0.98356897)), ('to', (2000, 0.9999778)), ('the', (1996, 0.36654013)), ('during', (2076, 0.41664934)), ('the', (1996, 0.9971005)), ('1600', (14883, 0.7769362)), (\"'\", (1005, 0.9858804)), ('s', (1055, 0.99912935)), ('and', (1998, 0.82082427)), ('1800', (9807, 0.1144675)), (\"'\", (1005, 0.9999489)), ('s', (1055, 0.9999973)), ('s', (1055, 0.5097949))]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "TOP K FOR INCORRECT TERMS:\n",
      "Output had instead of  gave\n",
      "- \t ['had'] 0.21659361\n",
      "- \t ['released'] 0.09585799\n",
      "- \t ['noted'] 0.07460638\n",
      "- \t ['added'] 0.07255199\n",
      "- \t ['began'] 0.06861529\n",
      "- \t ['went'] 0.042246174\n",
      "- \t ['joined'] 0.0366144\n",
      "- \t ['ruled'] 0.026887074\n",
      "- \t ['came'] 0.025495239\n",
      "- \t ['signed'] 0.017666133\n",
      "Output the instead of  normandy\n",
      "- \t ['the'] 0.36654013\n",
      "- \t ['rome'] 0.16746594\n",
      "- \t ['sicily'] 0.105883166\n",
      "- \t ['normandy'] 0.09617493\n",
      "- \t ['france'] 0.023782434\n",
      "- \t ['they'] 0.020675704\n",
      "- \t ['bavaria'] 0.01654116\n",
      "- \t ['norman'] 0.015527311\n",
      "- \t ['ireland'] 0.01230613\n",
      "- \t ['new'] 0.012240116\n",
      "Output during instead of  in\n",
      "- \t ['during'] 0.41664934\n",
      "- \t ['from'] 0.34460765\n",
      "- \t ['in'] 0.11245022\n",
      "- \t ['after'] 0.057846554\n",
      "- \t ['normandy'] 0.01878583\n",
      "- \t ['before'] 0.009098401\n",
      "- \t ['to'] 0.008953253\n",
      "- \t ['by'] 0.0055678706\n",
      "- \t ['until'] 0.0049331654\n",
      "- \t ['when'] 0.0033710252\n",
      "Output 1600 instead of  1000\n",
      "- \t ['1600'] 0.7769362\n",
      "- \t ['1870'] 0.054665823\n",
      "- \t ['mid'] 0.025837086\n",
      "- \t ['9th'] 0.0139215635\n",
      "- \t ['1800'] 0.009900339\n",
      "- \t ['1960'] 0.008250613\n",
      "- \t ['french'] 0.007879415\n",
      "- \t ['10th'] 0.00509792\n",
      "- \t ['14th'] 0.0048008463\n",
      "- \t ['117'] 0.0047757397\n",
      "Output 1800 instead of  1100\n",
      "- \t ['1800'] 0.1144675\n",
      "- \t ['james'] 0.111359015\n",
      "- \t ['[PAD]'] 0.077546746\n",
      "- \t ['80'] 0.04499401\n",
      "- \t ['french'] 0.041729376\n",
      "- \t ['the'] 0.037435092\n",
      "- \t ['m'] 0.03666016\n",
      "- \t ['ce'] 0.036381252\n",
      "- \t ['ba'] 0.035938375\n",
      "- \t ['b'] 0.035113864\n",
      "##############################\n",
      "Unanswerable: 15\n",
      "Loss: 0.11361387372016907\n",
      "CONTEXT\n",
      "['[CLS]', 'before', 'roll', '##o', \"'\", 's', 'arrival', ',', 'its', 'populations', 'did', 'not', 'differ', 'from', 'pic', '##ard', '##y', 'or', 'the', 'ile', '-', 'de', '-', 'france', ',', 'which', 'were', 'considered', '\"', 'frankish', '\"', '.', 'earlier', 'viking', 'settlers', 'had', 'begun', 'arriving', 'in', 'the', '880', '##s', ',', 'but', 'were', 'divided', 'between', 'colonies', 'in', 'the', 'east', '(', 'ro', '##um', '##ois', 'and', 'pays', 'de', 'ca', '##ux', ')', 'around', 'the', 'low', 'seine', 'valley', 'and', 'in', 'the', 'west', 'in', 'the', 'cote', '##nti', '##n', 'peninsula', ',', 'and', 'were', 'separated', 'by', 'traditional', 'pa', '##gi', '##i', ',', 'where', 'the', 'population', 'remained', 'about', 'the', 'same', 'with', 'almost', 'no', 'foreign', 'settlers', '.', 'roll', '##o', \"'\", 's', 'contingent', '##s', 'who', 'raided', 'and', 'ultimately', 'settled', 'normandy', 'and', 'parts', 'of', 'the', 'atlantic', 'coast', 'included', 'danes', ',', 'norwegian', '##s', ',', 'norse', '–', 'gael', '[PAD]', '[PAD]']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "QUESTION\n",
      "[101, 2043, 2106, 4897, 2080, 4088, 2000, 7180, 1999, 13298, 1029]\n",
      "['[CLS]', 'when', 'did', 'roll', '##o', 'begin', 'to', 'arrive', 'in', 'normandy', '?']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "OUTPUT\n",
      "['when', 'did', 'john', '##o', 'arrive', 'to', 'arrive', 'in', 'normandy', '?']\n",
      "[('when', (2043, 0.99988675)), ('did', (2106, 0.99981487)), ('john', (2198, 0.33094674)), ('##o', (2080, 0.9996426)), ('arrive', (7180, 0.9563557)), ('to', (2000, 0.95665324)), ('arrive', (7180, 0.99976695)), ('in', (1999, 0.89688534)), ('normandy', (13298, 0.67361337)), ('?', (1029, 0.9920272))]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "TOP K FOR INCORRECT TERMS:\n",
      "Output john instead of  roll\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- \t ['john'] 0.33094674\n",
      "- \t ['joshua'] 0.21873909\n",
      "- \t ['oct'] 0.07884895\n",
      "- \t ['charles'] 0.033959374\n",
      "- \t ['edward'] 0.03276904\n",
      "- \t ['frederic'] 0.031260744\n",
      "- \t ['james'] 0.017703975\n",
      "- \t ['napoleon'] 0.013841213\n",
      "- \t ['alfred'] 0.01328788\n",
      "- \t ['ae'] 0.012359366\n",
      "Output arrive instead of  begin\n",
      "- \t ['arrive'] 0.9563557\n",
      "- \t ['begin'] 0.022727778\n",
      "- \t ['depart'] 0.013512615\n",
      "- \t ['start'] 0.003096774\n",
      "- \t ['return'] 0.00088552793\n",
      "- \t ['take'] 0.0006476474\n",
      "- \t ['come'] 0.0006314512\n",
      "- \t ['go'] 0.00036981778\n",
      "- \t ['become'] 0.00032174477\n",
      "- \t ['move'] 0.0002980802\n",
      "##############################\n"
     ]
    }
   ],
   "source": [
    "explore = {eval_dataloader_ans: [0,1], eval_dataloader_unans: [0, 15]}\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss_ans = 0\n",
    "    for dataloader in explore.keys():\n",
    "        for batch_i, eval_batch in enumerate(dataloader):\n",
    "            eids = eval_batch[-1]\n",
    "            eval_batch = tuple(t.to(device) for t in eval_batch[:-1])\n",
    "            question_ids, question_mask, context_ids, context_mask, targets = eval_batch\n",
    "            output, _ = model(context_ids, context_mask, question_ids, question_mask)\n",
    "            loss = criterion(output.view(-1, len(tokenizer.vocab)), targets.view(-1))\n",
    "            eval_loss_ans += loss.item()\n",
    "            if batch_i in explore[dataloader]:\n",
    "                if dataloader == eval_dataloader_ans:\n",
    "                    print(\"Answerable:\",batch_i)\n",
    "                else:\n",
    "                    print(\"Unanswerable:\",batch_i)\n",
    "                print_details(context_ids, question_ids, output, loss, tokenizer)\n",
    "            if batch_i > max(explore[dataloader]):\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "* LM was suprisingly good at guessing question type and general structure of the question. It may be because the BERT representation is leaky in terms of representation  \n",
    "    - may be interesting result of its own ... Q: How much does the feature of each BERT word contain information about the surrounding BERT words?  \n",
    "    - this is probably why the model is pretty good at guessing Q type from the [CLS] token  \n",
    "* the Sentence repr is not enough represent specific entities in the text  \n",
    "    - gets confused between normandy and france and mongolia and normans   \n",
    "    - this might be because I don't train the sentence repr which is probably a mistake and we should retrain this one with trained sentence repr and forward masking in the question repr\n",
    "    - Maybe we can do an attention over entities? \n",
    "    - There is a bias towards more common entities right now (ie. john and paris instead of rollo and normandy)\n",
    "    - the fact that stuff like Rollo and dates were predicted well means there is leakiness in the representation\n",
    "* rank may be more important than loss since sometimes the #1 option has very high prob but number 2 is really good (eg. arrive vs begin U15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
