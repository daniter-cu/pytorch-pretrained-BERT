{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"examples/\")\n",
    "\n",
    "import logging\n",
    "import argparse\n",
    "import json\n",
    "from tqdm import tqdm, trange\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertModel \n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_autoreg_eval import BERTDataset, RNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args\n",
    "gradient_accumulation_steps = 1\n",
    "train_batch_size = 1\n",
    "eval_file = \"dataset/dev-v2.0.json\"\n",
    "max_seq_length=128\n",
    "on_memory = True\n",
    "bert_model = \"autoreg_model/pytorch_model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/10/2019 13:19:15 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/daniter/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "Loading Squad: 100%|██████████| 35/35 [00:00<00:00, 3569.53it/s]\n",
      "Loading Squad: 100%|██████████| 35/35 [00:00<00:00, 821.61it/s]\n",
      "02/10/2019 13:19:17 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/daniter/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "02/10/2019 13:19:17 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /Users/daniter/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/xx/8h5l1j614vv5wmbx9fbj69wm0000gn/T/tmp1y1y0o4y\n",
      "02/10/2019 13:19:20 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "02/10/2019 13:19:23 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/daniter/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "02/10/2019 13:19:23 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /Users/daniter/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/xx/8h5l1j614vv5wmbx9fbj69wm0000gn/T/tmpf5_7njvy\n",
      "02/10/2019 13:19:26 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the vocab size: 30522\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "\n",
    "# Load eval_data\n",
    "eval_dataset_answerable = BERTDataset(eval_file, tokenizer, seq_len=max_seq_length,\n",
    "                            on_memory=on_memory, answerable=True)\n",
    "eval_dataset_unanswerable = BERTDataset(eval_file, tokenizer, seq_len=max_seq_length,\n",
    "                           on_memory=on_memory, answerable=False)\n",
    "\n",
    "# Prepare model\n",
    "model_state_dict = torch.load(bert_model, map_location='cpu') #TODO daniter: remove this map_location\n",
    "## TODO daniter: check if bert model is being loaded correctly\n",
    "context_model = BertModel.from_pretrained(\"bert-base-uncased\")#, state_dict=model_state_dict)\n",
    "question_model = BertModel.from_pretrained(\"bert-base-uncased\")#, state_dict=model_state_dict)\n",
    "context_model.to(device)\n",
    "question_model.to(device)\n",
    "\n",
    "\n",
    "# Prepare optimizer\n",
    "print(\"Checking the vocab size:\", len(tokenizer.vocab))\n",
    "# 768 is bert hidden size, 256 is GRU hidden size, 1 is the layers in the GRU\n",
    "model = RNNModel(\"GRU\", len(tokenizer.vocab), 768, 768, 1, context_model, question_model, ngpu=n_gpu)\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.to(device)\n",
    "\n",
    "# eval loader\n",
    "eval_sampler_ans = SequentialSampler(eval_dataset_answerable)\n",
    "eval_dataloader_ans = DataLoader(eval_dataset_answerable, sampler=eval_sampler_ans,\n",
    "                                 batch_size=train_batch_size)\n",
    "eval_sampler_unans = SequentialSampler(eval_dataset_unanswerable)\n",
    "eval_dataloader_unans = DataLoader(eval_dataset_unanswerable, sampler=eval_sampler_unans,\n",
    "                                   batch_size=train_batch_size)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.init_hidden(train_batch_size)\n",
    "pass\n",
    "# with torch.no_grad():\n",
    "#     model.eval()\n",
    "\n",
    "#     eval_loss_ans = 0\n",
    "#     for batch_i, eval_batch in enumerate(eval_dataloader_ans):\n",
    "#         assert False\n",
    "#         if batch_i % 1000 == 0:\n",
    "#             print(\"#### DANITER completed answerable\", batch_i)\n",
    "#         eids = eval_batch[-1]\n",
    "#         eval_batch = tuple(t.to(device) for t in eval_batch[:-1])\n",
    "#         question_ids, question_mask, context_ids, context_mask, targets = eval_batch\n",
    "#         output, _ = model(context_ids, context_mask, question_ids, question_mask)\n",
    "#         loss = criterion(output.view(-1, len(tokenizer.vocab)), question_ids.view(-1))\n",
    "#         eval_loss_ans += loss.item()\n",
    "#     print(\"##### DANITER EVAL LOSS IS (ANSWERABLE) : \", eval_loss_ans)\n",
    "\n",
    "#     eval_loss_unans = 0\n",
    "#     for batch_i, eval_batch in enumerate(eval_dataloader_unans):\n",
    "#         if batch_i % 1000 == 0:\n",
    "#             print(\"#### DANITER completed unanswerable\", batch_i)\n",
    "#         eids = eval_batch[-1]\n",
    "#         eval_batch = tuple(t.to(device) for t in eval_batch[:-1])\n",
    "#         question_ids, question_mask, context_ids, context_mask, targets = eval_batch\n",
    "#         output, _ = model(context_ids, context_mask, question_ids, question_mask)\n",
    "#         loss = criterion(output.view(-1, len(tokenizer.vocab)), question_ids.view(-1))\n",
    "#         eval_loss_unans += loss.item()\n",
    "#     print(\"##### DANITER EVAL LOSS IS (UNANSWERABLE) : \", eval_loss_unans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 0.10354693979024887\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss_ans = 0\n",
    "    for batch_i, eval_batch in enumerate(eval_dataloader_ans):\n",
    "        eids = eval_batch[-1]\n",
    "        eval_batch = tuple(t.to(device) for t in eval_batch[:-1])\n",
    "        question_ids, question_mask, context_ids, context_mask, targets = eval_batch\n",
    "        output, _ = model(context_ids, context_mask, question_ids, question_mask)\n",
    "        loss = criterion(output.view(-1, len(tokenizer.vocab)), question_ids.view(-1))\n",
    "        eval_loss_ans += loss.item()\n",
    "        if loss.item() > 0.01:\n",
    "            print(batch_i, eval_loss_ans)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2054,  2314,  2761, 10351,  1996, 11068,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "['[CLS]', 'what', 'river', 'originally', 'bounded', 'the', 'duchy', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(question_ids)\n",
    "print(tokenizer.convert_ids_to_tokens(question_ids.data.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 5.6281233  -0.69632524 -1.6773183  ... -1.089677   -0.66363454\n",
      "   -2.3050935 ]\n",
      "  [ 7.029129   -2.3007493  -2.2169943  ... -2.3561647  -1.9902968\n",
      "   -1.9757373 ]\n",
      "  [ 0.5491697  -2.3692703  -1.9706893  ... -2.0390208  -1.2088827\n",
      "   -3.4375327 ]\n",
      "  ...\n",
      "  [33.869183   -1.5653121  -2.9970436  ... -3.353264   -2.9366097\n",
      "   -2.3617918 ]\n",
      "  [33.857277   -1.5599437  -3.0104399  ... -3.3641667  -2.9220674\n",
      "   -2.3839917 ]\n",
      "  [33.84768    -1.5606     -3.0117958  ... -3.3346536  -2.9785056\n",
      "   -2.3785434 ]]]\n",
      "['[CLS]', 'what', 'river', 'originally', 'supervised', 'the', 'duchy', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "o = output.data.numpy()\n",
    "print(o)\n",
    "print(tokenizer.convert_ids_to_tokens(np.argmax(o[0], axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 1.0\n",
      "2054 0.99999976\n",
      "2314 0.9999653\n",
      "2761 0.99997437\n",
      "13588 0.06754731\n",
      "1996 0.9999825\n",
      "11068 0.9999676\n",
      "0 0.99984914\n",
      "0 1.0\n",
      "0 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(np.argmax(o[0,i,:]), np.exp(np.max(o[0,i,:]))/ np.sum(np.exp(o[0,i,:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10354693979024887"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_loss_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06754731"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(np.max(o[0,4,:]))/ np.sum(np.exp(o[0,4,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06754731"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(o[0,4,13588])/ np.sum(np.exp(o[0,4,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7623411e-06"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(o[0,4,10351])/ np.sum(np.exp(o[0,4,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(o.shape[2]):\n",
    "    c[i] = np.exp(o[0,4,i])/ np.sum(np.exp(o[0,4,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['supervised'] 0.06754731\n",
      "['brat'] 0.021551749\n",
      "['boundary'] 0.0153177455\n",
      "['##uve'] 0.013979059\n",
      "['bounds'] 0.0138610555\n",
      "['resided'] 0.012585191\n",
      "['regulated'] 0.011854893\n",
      "['soyuz'] 0.011402544\n",
      "['##gut'] 0.010964767\n",
      "['54th'] 0.01089891\n",
      "['yue'] 0.010824164\n",
      "['poems'] 0.010754741\n",
      "['##ht'] 0.009082668\n",
      "['poem'] 0.009074019\n",
      "['##ount'] 0.008436166\n",
      "['lowered'] 0.0067586447\n",
      "['deserved'] 0.0064462107\n",
      "['triggered'] 0.0064248405\n",
      "['corresponding'] 0.0061934637\n",
      "['packet'] 0.0059934203\n",
      "['##erted'] 0.00508144\n",
      "['took'] 0.004897214\n",
      "['bordered'] 0.0045819986\n",
      "['assessed'] 0.004541193\n",
      "['vacated'] 0.004289019\n"
     ]
    }
   ],
   "source": [
    "for idx, score in c.most_common()[:25]:\n",
    "    print(tokenizer.convert_ids_to_tokens([idx]), score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss_ans = 0\n",
    "    for batch_i, eval_batch in enumerate(eval_dataloader_unans):\n",
    "        eids = eval_batch[-1]\n",
    "        eval_batch = tuple(t.to(device) for t in eval_batch[:-1])\n",
    "        question_ids, question_mask, context_ids, context_mask, targets = eval_batch\n",
    "        output, _ = model(context_ids, context_mask, question_ids, question_mask)\n",
    "        loss = criterion(output.view(-1, len(tokenizer.vocab)), question_ids.view(-1))\n",
    "        eval_loss_ans += loss.item()\n",
    "        break\n",
    "        if loss.item() > 0.01:\n",
    "            print(batch_i, eval_loss_ans)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0606985092163086e-05"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_loss_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2040,  2435,  2037,  2171,  2000, 13298,  1999,  1996,  6694,\n",
      "          1005,  1055,  1998, 22096,  1005,  1055,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "['[CLS]', 'who', 'gave', 'their', 'name', 'to', 'normandy', 'in', 'the', '1000', \"'\", 's', 'and', '1100', \"'\", 's', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(question_ids)\n",
    "print(tokenizer.convert_ids_to_tokens(question_ids.data.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 4.9845333  -0.5840121  -1.7072299  ... -0.8986035  -0.7701622\n",
      "   -2.4625373 ]\n",
      "  [ 5.5687795  -3.1512156  -1.5189477  ... -2.2867513  -0.81157726\n",
      "   -1.9473257 ]\n",
      "  [ 1.8056442  -2.4558816  -1.3853037  ... -1.5242887  -1.9538682\n",
      "   -2.677875  ]\n",
      "  ...\n",
      "  [33.50697    -1.898835   -3.0913079  ... -3.4296584  -3.0715473\n",
      "   -2.3606715 ]\n",
      "  [33.482018   -1.8573587  -3.0754821  ... -3.4108062  -3.087913\n",
      "   -2.3764603 ]\n",
      "  [33.43003    -1.8635046  -3.1077945  ... -3.40165    -3.1059961\n",
      "   -2.4096801 ]]]\n",
      "['[CLS]', 'who', 'gave', 'their', 'name', 'to', 'normandy', 'in', 'the', '1000', \"'\", 's', 'and', '1100', \"'\", 's', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "o = output.data.numpy()\n",
    "print(o)\n",
    "print(tokenizer.convert_ids_to_tokens(np.argmax(o[0], axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 1.0\n",
      "2040 0.99998236\n",
      "2435 0.99976426\n",
      "2037 0.99991757\n",
      "2171 0.999995\n",
      "2000 0.99999946\n",
      "13298 0.9999513\n",
      "1999 0.99999833\n",
      "1996 0.99999017\n",
      "6694 0.998527\n",
      "1005 0.99999917\n",
      "1055 0.999995\n",
      "1998 0.99999845\n",
      "22096 0.99808013\n",
      "1005 0.9999972\n",
      "1055 0.99999726\n",
      "0 0.99986583\n",
      "0 0.99999994\n",
      "0 1.0\n",
      "0 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(np.argmax(o[0,i,:]), np.exp(np.max(o[0,i,:]))/ np.sum(np.exp(o[0,i,:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
