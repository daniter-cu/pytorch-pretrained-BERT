{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/train-v2.0.json\", 'r') as handle:\n",
    "    jdata = json.load(handle)\n",
    "    data = jdata['data']\n",
    "contexts = []\n",
    "questions = []\n",
    "unanswerable = []\n",
    "answerable = []\n",
    "for i in range(len(data)):\n",
    "    section = data[i]['paragraphs']\n",
    "    for sec in section:\n",
    "        context = sec['context']\n",
    "        contexts.append(context)\n",
    "        qas = sec['qas']\n",
    "        for j in range(len(qas)):\n",
    "            question = qas[j]['question']\n",
    "            questions.append(question)\n",
    "            label = qas[j]['is_impossible']\n",
    "            if label:\n",
    "                unanswerable.append((len(contexts)-1, len(questions)-1))\n",
    "            else:\n",
    "                answerable.append((len(contexts)-1, len(questions)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_questions = list(questions[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_questions = []\n",
    "for q in candidate_questions:\n",
    "    parsed_questions.append(nlp(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When did Beyonce start becoming popular?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Beyonce]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (parsed_questions[0])\n",
    "[t for t in parsed_questions[0].noun_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.parts_of_speech.ADV == parsed_questions[0][0].pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which magazine declared her the most dominant woman musician?\n",
      "[Which magazine]\n",
      "Where did Beyonce get her name from?\n",
      "[Beyonce, her name]\n",
      "How many records has Beyonce sold in her 19 year career?\n",
      "[How many records, Beyonce, her 19 year career]\n",
      "In which decade did Beyonce become famous?\n",
      "[which decade, Beyonce]\n",
      "When did Destiny's Child end their group act?\n",
      "[Destiny's Child, their group act]\n"
     ]
    }
   ],
   "source": [
    "for i in np.random.choice(50, 5):\n",
    "    print(parsed_questions[i])\n",
    "    print ([t for t in parsed_questions[i].noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which magazine declared her the most dominant woman musician?\n",
    "# [Which] [magazine] [declared] [her] [the [most dominant woman musician] ]\n",
    "q  = [q for q in parsed_questions if \"magazine declared her the most dominant\" in q.text][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Which magazine]\n",
      "Which ADJ det WDT\n",
      "magazine NOUN nsubj NN\n",
      "declared VERB ROOT VBD\n",
      "her PRON poss PRP\n",
      "the DET det DT\n",
      "most ADV advmod RBS\n",
      "dominant ADJ amod JJ\n",
      "woman NOUN compound NN\n",
      "musician NOUN ccomp NN\n",
      "? PUNCT punct .\n"
     ]
    }
   ],
   "source": [
    "print([c for c in q.noun_chunks])\n",
    "for t in q:\n",
    "    print (t, t.pos_, t.dep_, t.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan\n",
    "1) Algorithm to convert sentences to nested spans  \n",
    "2) Algorithm to map span to bert tokens  \n",
    "3) generate dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['which',\n",
       " 'magazine',\n",
       " 'declared',\n",
       " 'her',\n",
       " 'the',\n",
       " 'most',\n",
       " 'dominant',\n",
       " 'woman',\n",
       " 'musician',\n",
       " '?']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(q.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tested = 0\n",
    "for q in parsed_questions:\n",
    "    tokenized = tokenizer.tokenize(q.text)\n",
    "    j = [0,0]\n",
    "    for i in range(len(q.text)):\n",
    "        if q.text[i] == \" \":\n",
    "            continue\n",
    "        while tokenized[j[0]][j[1]] == \"#\":\n",
    "            j[1] += 1\n",
    "        if ord(q.text[i]) < 128:\n",
    "            assert q.text[i].lower() == tokenized[j[0]][j[1]],(q, i, j, q.text[i:] )\n",
    "        if (j[1] + 1) == len(tokenized[j[0]]):\n",
    "            j[0] += 1\n",
    "            j[1] = 0\n",
    "        else:\n",
    "            j[1] += 1\n",
    "    tested += 1\n",
    "tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_tokens_by_chunk(tokens, text, span):\n",
    "    masked_tokens = list(tokens)\n",
    "    target = list(tokens)\n",
    "    tokens_to_mask = set()\n",
    "    j = [0,0]\n",
    "    for i in range(len(text)):\n",
    "        if text[i] == \" \":\n",
    "            continue\n",
    "        while tokens[j[0]][j[1]] == \"#\":\n",
    "            j[1] += 1\n",
    "            if j[1] == len(tokens[j[0]]):\n",
    "                return None, None\n",
    "        if ord(text[i]) < 128:\n",
    "            if text[i].lower() != tokens[j[0]][j[1]]:#,(text, i, j, text[i:], tokens )\n",
    "                return None,None\n",
    "        if i >= span.start_char and i < span.end_char:\n",
    "            tokens_to_mask.add(j[0])\n",
    "        if (j[1] + 1) == len(tokens[j[0]]):\n",
    "            j[0] += 1\n",
    "            j[1] = 0\n",
    "        else:\n",
    "            j[1] += 1\n",
    "    for i in tokens_to_mask:\n",
    "        masked_tokens[i] = '[MASK]'\n",
    "        target[i] = '[MASK]'\n",
    "        target.append(tokens[i])\n",
    "    return masked_tokens, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['for', 'what', 'does', '[MASK]', 'receive', 'praise', '?'], ['for', 'what', 'does', '[MASK]', 'receive', 'praise', '?', 'beyonce'])\n"
     ]
    }
   ],
   "source": [
    "testq = q\n",
    "test_tokens = tokenizer.tokenize(q.text)\n",
    "test_text = q.text\n",
    "test_span = list(q.noun_chunks)[1]\n",
    "print(mask_tokens_by_chunk(test_tokens, test_text, test_span))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who did BeyoncÃ© sing a duet with for \"The Best Man\" film?\n",
      "['[MASK]', 'did', 'beyonce', 'sing', 'a', 'duet', 'with', 'for', '\"', 'the', 'best', 'man', '\"', 'film', '?']\n",
      "['[MASK]', 'did', 'beyonce', 'sing', 'a', 'duet', 'with', 'for', '\"', 'the', 'best', 'man', '\"', 'film', '?', 'who']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['who', 'did', '[MASK]', 'sing', 'a', 'duet', 'with', 'for', '\"', 'the', 'best', 'man', '\"', 'film', '?']\n",
      "['who', 'did', '[MASK]', 'sing', 'a', 'duet', 'with', 'for', '\"', 'the', 'best', 'man', '\"', 'film', '?', 'beyonce']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['who', 'did', 'beyonce', 'sing', '[MASK]', '[MASK]', 'with', 'for', '\"', 'the', 'best', 'man', '\"', 'film', '?']\n",
      "['who', 'did', 'beyonce', 'sing', '[MASK]', '[MASK]', 'with', 'for', '\"', 'the', 'best', 'man', '\"', 'film', '?', 'a', 'duet']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['who', 'did', 'beyonce', 'sing', 'a', 'duet', 'with', 'for', '\"', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '?']\n",
      "['who', 'did', 'beyonce', 'sing', 'a', 'duet', 'with', 'for', '\"', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '?', 'the', 'best', 'man', '\"', 'film']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['who', 'did', '[MASK]', 'sing', 'a', 'duet', 'with', 'for', '\"', 'the', 'best', 'man', '\"', 'film', '?']\n",
      "['who', 'did', '[MASK]', 'sing', 'a', 'duet', 'with', 'for', '\"', 'the', 'best', 'man', '\"', 'film', '?', 'beyonce']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['who', 'did', 'beyonce', 'sing', 'a', 'duet', 'with', 'for', '\"', '[MASK]', '[MASK]', '[MASK]', '\"', 'film', '?']\n",
      "['who', 'did', 'beyonce', 'sing', 'a', 'duet', 'with', 'for', '\"', '[MASK]', '[MASK]', '[MASK]', '\"', 'film', '?', 'the', 'best', 'man']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "##############################\n",
      "Which song did Beyonce sing to win a competition at age 7?\n",
      "['[MASK]', '[MASK]', 'did', 'beyonce', 'sing', 'to', 'win', 'a', 'competition', 'at', 'age', '7', '?']\n",
      "['[MASK]', '[MASK]', 'did', 'beyonce', 'sing', 'to', 'win', 'a', 'competition', 'at', 'age', '7', '?', 'which', 'song']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['which', 'song', 'did', '[MASK]', 'sing', 'to', 'win', 'a', 'competition', 'at', 'age', '7', '?']\n",
      "['which', 'song', 'did', '[MASK]', 'sing', 'to', 'win', 'a', 'competition', 'at', 'age', '7', '?', 'beyonce']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['which', 'song', 'did', 'beyonce', 'sing', 'to', 'win', '[MASK]', '[MASK]', 'at', 'age', '7', '?']\n",
      "['which', 'song', 'did', 'beyonce', 'sing', 'to', 'win', '[MASK]', '[MASK]', 'at', 'age', '7', '?', 'competition', 'a']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['which', 'song', 'did', 'beyonce', 'sing', 'to', 'win', 'a', 'competition', 'at', '[MASK]', '7', '?']\n",
      "['which', 'song', 'did', 'beyonce', 'sing', 'to', 'win', 'a', 'competition', 'at', '[MASK]', '7', '?', 'age']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['which', 'song', 'did', '[MASK]', 'sing', 'to', 'win', 'a', 'competition', 'at', 'age', '7', '?']\n",
      "['which', 'song', 'did', '[MASK]', 'sing', 'to', 'win', 'a', 'competition', 'at', 'age', '7', '?', 'beyonce']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['which', 'song', 'did', 'beyonce', 'sing', 'to', 'win', 'a', 'competition', 'at', '[MASK]', '[MASK]', '?']\n",
      "['which', 'song', 'did', 'beyonce', 'sing', 'to', 'win', 'a', 'competition', 'at', '[MASK]', '[MASK]', '?', 'age', '7']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "##############################\n",
      "Which magazine did Beyonce pose on the cover for in August of 2015?\n",
      "['[MASK]', '[MASK]', 'did', 'beyonce', 'pose', 'on', 'the', 'cover', 'for', 'in', 'august', 'of', '2015', '?']\n",
      "['[MASK]', '[MASK]', 'did', 'beyonce', 'pose', 'on', 'the', 'cover', 'for', 'in', 'august', 'of', '2015', '?', 'which', 'magazine']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['which', 'magazine', 'did', '[MASK]', 'pose', 'on', 'the', 'cover', 'for', 'in', 'august', 'of', '2015', '?']\n",
      "['which', 'magazine', 'did', '[MASK]', 'pose', 'on', 'the', 'cover', 'for', 'in', 'august', 'of', '2015', '?', 'beyonce']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['which', 'magazine', 'did', 'beyonce', 'pose', 'on', '[MASK]', '[MASK]', 'for', 'in', 'august', 'of', '2015', '?']\n",
      "['which', 'magazine', 'did', 'beyonce', 'pose', 'on', '[MASK]', '[MASK]', 'for', 'in', 'august', 'of', '2015', '?', 'the', 'cover']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['which', 'magazine', 'did', 'beyonce', 'pose', 'on', 'the', 'cover', 'for', 'in', '[MASK]', 'of', '2015', '?']\n",
      "['which', 'magazine', 'did', 'beyonce', 'pose', 'on', 'the', 'cover', 'for', 'in', '[MASK]', 'of', '2015', '?', 'august']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['which', 'magazine', 'did', '[MASK]', 'pose', 'on', 'the', 'cover', 'for', 'in', 'august', 'of', '2015', '?']\n",
      "['which', 'magazine', 'did', '[MASK]', 'pose', 'on', 'the', 'cover', 'for', 'in', 'august', 'of', '2015', '?', 'beyonce']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['which', 'magazine', 'did', 'beyonce', 'pose', 'on', 'the', 'cover', 'for', 'in', '[MASK]', '[MASK]', '[MASK]', '?']\n",
      "['which', 'magazine', 'did', 'beyonce', 'pose', 'on', 'the', 'cover', 'for', 'in', '[MASK]', '[MASK]', '[MASK]', '?', 'august', 'of', '2015']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "##############################\n",
      "Where did BeyoncÃ© get pregnant?\n",
      "['where', 'did', '[MASK]', 'get', 'pregnant', '?']\n",
      "['where', 'did', '[MASK]', 'get', 'pregnant', '?', 'beyonce']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['where', 'did', '[MASK]', 'get', 'pregnant', '?']\n",
      "['where', 'did', '[MASK]', 'get', 'pregnant', '?', 'beyonce']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "##############################\n",
      "What genre of film was the movie, Obsessed, in which Beyonce starred in?\n",
      "['[MASK]', '[MASK]', 'of', 'film', 'was', 'the', 'movie', ',', 'obsessed', ',', 'in', 'which', 'beyonce', 'starred', 'in', '?']\n",
      "['[MASK]', '[MASK]', 'of', 'film', 'was', 'the', 'movie', ',', 'obsessed', ',', 'in', 'which', 'beyonce', 'starred', 'in', '?', 'what', 'genre']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['what', 'genre', 'of', '[MASK]', 'was', 'the', 'movie', ',', 'obsessed', ',', 'in', 'which', 'beyonce', 'starred', 'in', '?']\n",
      "['what', 'genre', 'of', '[MASK]', 'was', 'the', 'movie', ',', 'obsessed', ',', 'in', 'which', 'beyonce', 'starred', 'in', '?', 'film']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['what', 'genre', 'of', 'film', 'was', '[MASK]', '[MASK]', ',', 'obsessed', ',', 'in', 'which', 'beyonce', 'starred', 'in', '?']\n",
      "['what', 'genre', 'of', 'film', 'was', '[MASK]', '[MASK]', ',', 'obsessed', ',', 'in', 'which', 'beyonce', 'starred', 'in', '?', 'the', 'movie']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['what', 'genre', 'of', 'film', 'was', 'the', 'movie', ',', 'obsessed', ',', 'in', 'which', '[MASK]', 'starred', 'in', '?']\n",
      "['what', 'genre', 'of', 'film', 'was', 'the', 'movie', ',', 'obsessed', ',', 'in', 'which', '[MASK]', 'starred', 'in', '?', 'beyonce']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['what', 'genre', 'of', 'film', 'was', 'the', 'movie', ',', '[MASK]', ',', 'in', 'which', 'beyonce', 'starred', 'in', '?']\n",
      "['what', 'genre', 'of', 'film', 'was', 'the', 'movie', ',', '[MASK]', ',', 'in', 'which', 'beyonce', 'starred', 'in', '?', 'obsessed']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['what', 'genre', 'of', 'film', 'was', 'the', 'movie', ',', 'obsessed', ',', 'in', 'which', '[MASK]', 'starred', 'in', '?']\n",
      "['what', 'genre', 'of', 'film', 'was', 'the', 'movie', ',', 'obsessed', ',', 'in', 'which', '[MASK]', 'starred', 'in', '?', 'beyonce']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "##############################\n",
      "Beyonce along with Jay Z met with whom's family after their death?\n",
      "['[MASK]', 'along', 'with', 'jay', 'z', 'met', 'with', 'whom', \"'\", 's', 'family', 'after', 'their', 'death', '?']\n",
      "['[MASK]', 'along', 'with', 'jay', 'z', 'met', 'with', 'whom', \"'\", 's', 'family', 'after', 'their', 'death', '?', 'beyonce']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['beyonce', 'along', 'with', '[MASK]', '[MASK]', 'met', 'with', 'whom', \"'\", 's', 'family', 'after', 'their', 'death', '?']\n",
      "['beyonce', 'along', 'with', '[MASK]', '[MASK]', 'met', 'with', 'whom', \"'\", 's', 'family', 'after', 'their', 'death', '?', 'jay', 'z']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['beyonce', 'along', 'with', 'jay', 'z', 'met', 'with', '[MASK]', \"'\", 's', 'family', 'after', 'their', 'death', '?']\n",
      "['beyonce', 'along', 'with', 'jay', 'z', 'met', 'with', '[MASK]', \"'\", 's', 'family', 'after', 'their', 'death', '?', 'whom']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['beyonce', 'along', 'with', 'jay', 'z', 'met', 'with', 'whom', \"'\", 's', '[MASK]', 'after', 'their', 'death', '?']\n",
      "['beyonce', 'along', 'with', 'jay', 'z', 'met', 'with', 'whom', \"'\", 's', '[MASK]', 'after', 'their', 'death', '?', 'family']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['beyonce', 'along', 'with', 'jay', 'z', 'met', 'with', 'whom', \"'\", 's', 'family', 'after', '[MASK]', '[MASK]', '?']\n",
      "['beyonce', 'along', 'with', 'jay', 'z', 'met', 'with', 'whom', \"'\", 's', 'family', 'after', '[MASK]', '[MASK]', '?', 'their', 'death']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['[MASK]', 'along', 'with', 'jay', 'z', 'met', 'with', 'whom', \"'\", 's', 'family', 'after', 'their', 'death', '?']\n",
      "['[MASK]', 'along', 'with', 'jay', 'z', 'met', 'with', 'whom', \"'\", 's', 'family', 'after', 'their', 'death', '?', 'beyonce']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['beyonce', 'along', 'with', '[MASK]', '[MASK]', 'met', 'with', 'whom', \"'\", 's', 'family', 'after', 'their', 'death', '?']\n",
      "['beyonce', 'along', 'with', '[MASK]', '[MASK]', 'met', 'with', 'whom', \"'\", 's', 'family', 'after', 'their', 'death', '?', 'jay', 'z']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "##############################\n",
      "Beyonce's group changed their name to Destiny's Child in what year?\n",
      "['[MASK]', '[MASK]', '[MASK]', '[MASK]', 'changed', 'their', 'name', 'to', 'destiny', \"'\", 's', 'child', 'in', 'what', 'year', '?']\n",
      "['[MASK]', '[MASK]', '[MASK]', '[MASK]', 'changed', 'their', 'name', 'to', 'destiny', \"'\", 's', 'child', 'in', 'what', 'year', '?', 'beyonce', \"'\", 's', 'group']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['beyonce', \"'\", 's', 'group', 'changed', '[MASK]', '[MASK]', 'to', 'destiny', \"'\", 's', 'child', 'in', 'what', 'year', '?']\n",
      "['beyonce', \"'\", 's', 'group', 'changed', '[MASK]', '[MASK]', 'to', 'destiny', \"'\", 's', 'child', 'in', 'what', 'year', '?', 'their', 'name']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['beyonce', \"'\", 's', 'group', 'changed', 'their', 'name', 'to', '[MASK]', '[MASK]', '[MASK]', '[MASK]', 'in', 'what', 'year', '?']\n",
      "['beyonce', \"'\", 's', 'group', 'changed', 'their', 'name', 'to', '[MASK]', '[MASK]', '[MASK]', '[MASK]', 'in', 'what', 'year', '?', 'destiny', \"'\", 's', 'child']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['beyonce', \"'\", 's', 'group', 'changed', 'their', 'name', 'to', 'destiny', \"'\", 's', 'child', 'in', '[MASK]', '[MASK]', '?']\n",
      "['beyonce', \"'\", 's', 'group', 'changed', 'their', 'name', 'to', 'destiny', \"'\", 's', 'child', 'in', '[MASK]', '[MASK]', '?', 'what', 'year']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['[MASK]', \"'\", 's', 'group', 'changed', 'their', 'name', 'to', 'destiny', \"'\", 's', 'child', 'in', 'what', 'year', '?']\n",
      "['[MASK]', \"'\", 's', 'group', 'changed', 'their', 'name', 'to', 'destiny', \"'\", 's', 'child', 'in', 'what', 'year', '?', 'beyonce']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['beyonce', \"'\", 's', 'group', 'changed', 'their', 'name', 'to', '[MASK]', '[MASK]', '[MASK]', '[MASK]', 'in', 'what', 'year', '?']\n",
      "['beyonce', \"'\", 's', 'group', 'changed', 'their', 'name', 'to', '[MASK]', '[MASK]', '[MASK]', '[MASK]', 'in', 'what', 'year', '?', 'destiny', \"'\", 's', 'child']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['beyonce', \"'\", 's', 'group', 'changed', 'their', 'name', 'to', 'destiny', \"'\", 's', 'child', 'in', '[MASK]', '[MASK]', '?']\n",
      "['beyonce', \"'\", 's', 'group', 'changed', 'their', 'name', 'to', 'destiny', \"'\", 's', 'child', 'in', '[MASK]', '[MASK]', '?', 'what', 'year']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "##############################\n",
      "To whom did Beyonce credit as her major influence on her music?\n",
      "['to', '[MASK]', 'did', 'beyonce', 'credit', 'as', 'her', 'major', 'influence', 'on', 'her', 'music', '?']\n",
      "['to', '[MASK]', 'did', 'beyonce', 'credit', 'as', 'her', 'major', 'influence', 'on', 'her', 'music', '?', 'whom']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['to', 'whom', 'did', '[MASK]', '[MASK]', 'as', 'her', 'major', 'influence', 'on', 'her', 'music', '?']\n",
      "['to', 'whom', 'did', '[MASK]', '[MASK]', 'as', 'her', 'major', 'influence', 'on', 'her', 'music', '?', 'beyonce', 'credit']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['to', 'whom', 'did', 'beyonce', 'credit', 'as', '[MASK]', '[MASK]', '[MASK]', 'on', 'her', 'music', '?']\n",
      "['to', 'whom', 'did', 'beyonce', 'credit', 'as', '[MASK]', '[MASK]', '[MASK]', 'on', 'her', 'music', '?', 'influence', 'her', 'major']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['to', 'whom', 'did', 'beyonce', 'credit', 'as', 'her', 'major', 'influence', 'on', '[MASK]', '[MASK]', '?']\n",
      "['to', 'whom', 'did', 'beyonce', 'credit', 'as', 'her', 'major', 'influence', 'on', '[MASK]', '[MASK]', '?', 'her', 'music']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['to', 'whom', 'did', '[MASK]', 'credit', 'as', 'her', 'major', 'influence', 'on', 'her', 'music', '?']\n",
      "['to', 'whom', 'did', '[MASK]', 'credit', 'as', 'her', 'major', 'influence', 'on', 'her', 'music', '?', 'beyonce']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "##############################\n",
      "What singer did Beyonce record a song with for the movie, ''The Best Man\"?\n",
      "['[MASK]', '[MASK]', 'did', 'beyonce', 'record', 'a', 'song', 'with', 'for', 'the', 'movie', ',', \"'\", \"'\", 'the', 'best', 'man', '\"', '?']\n",
      "['[MASK]', '[MASK]', 'did', 'beyonce', 'record', 'a', 'song', 'with', 'for', 'the', 'movie', ',', \"'\", \"'\", 'the', 'best', 'man', '\"', '?', 'what', 'singer']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['what', 'singer', 'did', '[MASK]', 'record', 'a', 'song', 'with', 'for', 'the', 'movie', ',', \"'\", \"'\", 'the', 'best', 'man', '\"', '?']\n",
      "['what', 'singer', 'did', '[MASK]', 'record', 'a', 'song', 'with', 'for', 'the', 'movie', ',', \"'\", \"'\", 'the', 'best', 'man', '\"', '?', 'beyonce']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['what', 'singer', 'did', 'beyonce', 'record', '[MASK]', '[MASK]', 'with', 'for', 'the', 'movie', ',', \"'\", \"'\", 'the', 'best', 'man', '\"', '?']\n",
      "['what', 'singer', 'did', 'beyonce', 'record', '[MASK]', '[MASK]', 'with', 'for', 'the', 'movie', ',', \"'\", \"'\", 'the', 'best', 'man', '\"', '?', 'a', 'song']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['what', 'singer', 'did', 'beyonce', 'record', 'a', 'song', 'with', 'for', '[MASK]', '[MASK]', ',', \"'\", \"'\", 'the', 'best', 'man', '\"', '?']\n",
      "['what', 'singer', 'did', 'beyonce', 'record', 'a', 'song', 'with', 'for', '[MASK]', '[MASK]', ',', \"'\", \"'\", 'the', 'best', 'man', '\"', '?', 'the', 'movie']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['what', 'singer', 'did', 'beyonce', 'record', 'a', 'song', 'with', 'for', 'the', 'movie', ',', \"'\", '[MASK]', '[MASK]', '[MASK]', '[MASK]', '\"', '?']\n",
      "['what', 'singer', 'did', 'beyonce', 'record', 'a', 'song', 'with', 'for', 'the', 'movie', ',', \"'\", '[MASK]', '[MASK]', '[MASK]', '[MASK]', '\"', '?', 'man', \"'\", 'the', 'best']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['what', 'singer', 'did', '[MASK]', 'record', 'a', 'song', 'with', 'for', 'the', 'movie', ',', \"'\", \"'\", 'the', 'best', 'man', '\"', '?']\n",
      "['what', 'singer', 'did', '[MASK]', 'record', 'a', 'song', 'with', 'for', 'the', 'movie', ',', \"'\", \"'\", 'the', 'best', 'man', '\"', '?', 'beyonce']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['what', 'singer', 'did', 'beyonce', 'record', 'a', 'song', 'with', 'for', 'the', 'movie', ',', \"'\", \"'\", '[MASK]', '[MASK]', '[MASK]', '\"', '?']\n",
      "['what', 'singer', 'did', 'beyonce', 'record', 'a', 'song', 'with', 'for', 'the', 'movie', ',', \"'\", \"'\", '[MASK]', '[MASK]', '[MASK]', '\"', '?', 'man', 'the', 'best']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "##############################\n",
      "When did BeyoncÃ© rise to fame?\n",
      "['when', 'did', '[MASK]', 'rise', 'to', 'fame', '?']\n",
      "['when', 'did', '[MASK]', 'rise', 'to', 'fame', '?', 'beyonce']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['when', 'did', 'beyonce', 'rise', 'to', '[MASK]', '?']\n",
      "['when', 'did', 'beyonce', 'rise', 'to', '[MASK]', '?', 'fame']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "['when', 'did', '[MASK]', 'rise', 'to', 'fame', '?']\n",
      "['when', 'did', '[MASK]', 'rise', 'to', 'fame', '?', 'beyonce']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "##############################\n"
     ]
    }
   ],
   "source": [
    "for q in np.random.choice(parsed_questions, 10):\n",
    "    print(q)\n",
    "    tokens = tokenizer.tokenize(q.text)\n",
    "    for chunk in q.noun_chunks:\n",
    "        masked, target = mask_tokens_by_chunk(tokens, q.text, chunk)\n",
    "        print (masked)\n",
    "        print (target)\n",
    "        print(\"~\"*30)\n",
    "    for chunk in q.ents:\n",
    "        masked, target = mask_tokens_by_chunk(tokens, q.text, chunk)\n",
    "        print (masked)\n",
    "        print (target)\n",
    "        print(\"~\"*30)\n",
    "    print(\"#\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 Âµs, sys: 0 ns, total: 3 Âµs\n",
      "Wall time: 8.11 Âµs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "docs = []\n",
    "for doc in nlp.pipe(questions, batch_size=500, n_threads=16):\n",
    "     docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ADJ'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[-1][0].pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130319"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_sizes = []\n",
    "for c in contexts:\n",
    "    tokens = tokenizer.tokenize(c)\n",
    "    context_sizes.append(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "853 25 151.62348305752562\n"
     ]
    }
   ],
   "source": [
    "print(max(context_sizes), min(context_sizes), np.mean(context_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1288\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(np.array(context_sizes) > 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sizes = []\n",
    "for ci, qi, in answerable+ unanswerable:\n",
    "    size = context_sizes[ci]\n",
    "    tokens = tokenizer.tokenize(questions[qi])\n",
    "    size += len(tokens)\n",
    "    total_sizes.append(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "867 32 167.72663234064103\n"
     ]
    }
   ],
   "source": [
    "print(max(total_sizes), min(total_sizes), np.mean(total_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08909675488608722\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(np.array(total_sizes) > 256) / len(total_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = {}\n",
    "for i, doc in enumerate(docs):\n",
    "    training_data[i] = []\n",
    "    tokens = tokenizer.tokenize(doc.text)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        masked, target = mask_tokens_by_chunk(tokens, doc.text, chunk)\n",
    "        training_data[i].append([masked, target])\n",
    "    for chunk in doc.ents:\n",
    "        masked, target = mask_tokens_by_chunk(tokens, doc.text, chunk)\n",
    "        training_data[i].append([masked, target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517932 159\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "nones = 0\n",
    "for k, v in training_data.items():\n",
    "    c += len(v)\n",
    "    for vv in v:\n",
    "        if vv[0] is None:\n",
    "            nones += 1\n",
    "print(c, nones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "1) make test data  \n",
    "2) build finetuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/dev-v2.0.json\", 'r') as handle:\n",
    "    jdata = json.load(handle)\n",
    "    data = jdata['data']\n",
    "val_contexts = []\n",
    "val_questions = []\n",
    "val_unanswerable = []\n",
    "val_answerable = []\n",
    "for i in range(len(data)):\n",
    "    section = data[i]['paragraphs']\n",
    "    for sec in section:\n",
    "        context = sec['context']\n",
    "        val_contexts.append(context)\n",
    "        qas = sec['qas']\n",
    "        for j in range(len(qas)):\n",
    "            question = qas[j]['question']\n",
    "            val_questions.append(question)\n",
    "            label = qas[j]['is_impossible']\n",
    "            if label:\n",
    "                val_unanswerable.append((len(contexts)-1, len(questions)-1))\n",
    "            else:\n",
    "                val_answerable.append((len(contexts)-1, len(questions)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_docs = []\n",
    "for doc in nlp.pipe(val_questions, batch_size=500, n_threads=16):\n",
    "     val_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = {}\n",
    "for i, doc in enumerate(val_docs):\n",
    "    val_data[i] = []\n",
    "    tokens = tokenizer.tokenize(doc.text)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        masked, target = mask_tokens_by_chunk(tokens, doc.text, chunk)\n",
    "        val_data[i].append([masked, target])\n",
    "    for chunk in doc.ents:\n",
    "        masked, target = mask_tokens_by_chunk(tokens, doc.text, chunk)\n",
    "        val_data[i].append([masked, target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45674 0\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "nones = 0\n",
    "for k, v in val_data.items():\n",
    "    c += len(v)\n",
    "    for vv in v:\n",
    "        if vv[0] is None:\n",
    "            nones += 1\n",
    "print(c, nones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['[MASK]',\n",
       "   'was',\n",
       "   'the',\n",
       "   'duke',\n",
       "   'in',\n",
       "   'the',\n",
       "   'battle',\n",
       "   'of',\n",
       "   'hastings',\n",
       "   '?'],\n",
       "  ['[MASK]',\n",
       "   'was',\n",
       "   'the',\n",
       "   'duke',\n",
       "   'in',\n",
       "   'the',\n",
       "   'battle',\n",
       "   'of',\n",
       "   'hastings',\n",
       "   '?',\n",
       "   'who']],\n",
       " [['who',\n",
       "   'was',\n",
       "   '[MASK]',\n",
       "   '[MASK]',\n",
       "   'in',\n",
       "   'the',\n",
       "   'battle',\n",
       "   'of',\n",
       "   'hastings',\n",
       "   '?'],\n",
       "  ['who',\n",
       "   'was',\n",
       "   '[MASK]',\n",
       "   '[MASK]',\n",
       "   'in',\n",
       "   'the',\n",
       "   'battle',\n",
       "   'of',\n",
       "   'hastings',\n",
       "   '?',\n",
       "   'the',\n",
       "   'duke']],\n",
       " [['who',\n",
       "   'was',\n",
       "   'the',\n",
       "   'duke',\n",
       "   'in',\n",
       "   '[MASK]',\n",
       "   '[MASK]',\n",
       "   'of',\n",
       "   'hastings',\n",
       "   '?'],\n",
       "  ['who',\n",
       "   'was',\n",
       "   'the',\n",
       "   'duke',\n",
       "   'in',\n",
       "   '[MASK]',\n",
       "   '[MASK]',\n",
       "   'of',\n",
       "   'hastings',\n",
       "   '?',\n",
       "   'the',\n",
       "   'battle']],\n",
       " [['who', 'was', 'the', 'duke', 'in', 'the', 'battle', 'of', '[MASK]', '?'],\n",
       "  ['who',\n",
       "   'was',\n",
       "   'the',\n",
       "   'duke',\n",
       "   'in',\n",
       "   'the',\n",
       "   'battle',\n",
       "   'of',\n",
       "   '[MASK]',\n",
       "   '?',\n",
       "   'hastings']],\n",
       " [['who', 'was', 'the', 'duke', 'in', 'the', 'battle', 'of', '[MASK]', '?'],\n",
       "  ['who',\n",
       "   'was',\n",
       "   'the',\n",
       "   'duke',\n",
       "   'in',\n",
       "   'the',\n",
       "   'battle',\n",
       "   'of',\n",
       "   '[MASK]',\n",
       "   '?',\n",
       "   'hastings']]]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_data_chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(training_data, f)\n",
    "with open(\"val_data_chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop list = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "\n",
    "def overlapping_spans(c, q):\n",
    "    qt = tokenizer.tokenize(q)\n",
    "    ct = tokenizer.tokenize(c)\n",
    "    # build index\n",
    "    output_set = set()\n",
    "    context_ngram_set = set()\n",
    "    for i in range(len(ct)):\n",
    "        for j in range(10):\n",
    "            if j == 0:\n",
    "                continue\n",
    "            context_ngram_set.add(tuple(ct[i:i+j]))\n",
    "    skip = 0\n",
    "    for i in range(len(qt)):\n",
    "        longest = None\n",
    "        if skip:\n",
    "            skip -= 1\n",
    "            continue\n",
    "        for j in range(10):\n",
    "            if j == 0 or len(qt[i:i+j]) < j:\n",
    "                continue\n",
    "            span = tuple(qt[i:i+j])\n",
    "            if span in context_ngram_set:\n",
    "                longest = span\n",
    "            if span not in context_ngram_set and longest:\n",
    "                output_set.add(longest)\n",
    "                skip = len(longest) - 1 \n",
    "                break\n",
    "    return output_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the 9th to 11th century, Armenian architecture underwent a revival under the patronage of the Bagratid Dynasty with a great deal of building done in the area of Lake Van, this included both traditional styles and new innovations. Ornately carved Armenian Khachkars were developed during this time. Many new cities and churches were built during this time, including a new capital at Lake Van and a new Cathedral on Akdamar Island to match. The Cathedral of Ani was also completed during this dynasty. It was during this time that the first major monasteries, such as Haghpat and Haritchavank were built. This period was ended by the Seljuk invasion.\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "What Armenian monasteries were built in the 11th century?\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "{('in', 'the'), ('11th', 'century'), ('were', 'built'), ('monasteries',), ('armenian',)}\n"
     ]
    }
   ],
   "source": [
    "cid, qid = random.sample(answerable, 1)[0]\n",
    "q = questions[qid]\n",
    "c = contexts[cid]\n",
    "print(c)\n",
    "print(\"~\"*20)\n",
    "print(q)\n",
    "print(\"~\"*20)\n",
    "print(overlapping_spans(c,q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'free', 'officers', \"'\", 'intention', 'was', 'not', 'to', 'install', 'themselves', 'in', 'government', ',', 'but', 'to', 're', '-', 'establish', 'a', 'parliamentary', 'democracy', '.', 'nasser', 'did', 'not', 'believe', 'that', 'a', 'low', '-', 'ranking', 'officer', 'like', 'himself', '(', 'a', 'lieutenant', 'colonel', ')', 'would', 'be', 'accepted', 'by', 'the', 'egyptian', 'people', ',', 'and', 'so', 'selected', 'general', 'na', '##gui', '##b', 'to', 'be', 'his', '\"', 'boss', '\"', 'and', 'lead', 'the', 'coup', 'in', 'name', '.', 'the', 'revolution', 'they', 'had', 'long', 'sought', 'was', 'launched', 'on', '22', 'july', 'and', 'was', 'declared', 'a', 'success', 'the', 'next', 'day', '.', 'the', 'free', 'officers', 'seized', 'control', 'of', 'all', 'government', 'buildings', ',', 'radio', 'stations', ',', 'and', 'police', 'stations', ',', 'as', 'well', 'as', 'army', 'headquarters', 'in', 'cairo', '.', 'while', 'many', 'of', 'the', 'rebel', 'officers', 'were', 'leading', 'their', 'units', ',', 'nasser', 'don', '##ned', 'civilian', 'clothing', 'to', 'avoid', 'detection', 'by', 'royalist', '##s', 'and', 'moved', 'around', 'cairo', 'monitoring', 'the', 'situation', '.', 'in', 'a', 'move', 'to', 'st', '##ave', 'off', 'foreign', 'intervention', 'two', 'days', 'before', 'the', 'revolution', ',', 'nasser', 'had', 'notified', 'the', 'american', 'and', 'british', 'governments', 'of', 'his', 'intentions', ',', 'and', 'both', 'had', 'agreed', 'not', 'to', 'aid', 'far', '##ouk', '.', 'under', 'pressure', 'from', 'the', 'americans', ',', 'nasser', 'had', 'agreed', 'to', 'exile', 'the', 'deposed', 'king', 'with', 'an', 'honorary', 'ceremony', '.']\n",
      "['what', 'type', 'of', 'government', 'did', 'the', 'free', 'officer', \"'\", 's', 'want', 'to', 'establish', '?']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(c))\n",
    "print(tokenizer.tokenize(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues\n",
    "* verb / noun lemmatization might help (violated vs violating)\n",
    "* stop words - partial and full (eg. \"The administration\", \"in the\")\n",
    "* quotes interfere with matching (eg. \"pristine\" vs pristine)\n",
    "* there are nouns phrases that don't match (eg. Everton Football Clubs vs Everton team etc)\n",
    "* plural / possessive mistakes (free officer's vs free officers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = nlp(\"What Armenian monasteries were built in the 11th century?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What dobj\n",
      "Armenian amod\n",
      "monasteries nsubjpass\n",
      "were auxpass\n",
      "built ROOT\n",
      "in prep\n",
      "the det\n",
      "11th amod\n",
      "century pobj\n",
      "? punct\n"
     ]
    }
   ],
   "source": [
    "for token in s:\n",
    "    print(token, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What dobj NOUN\n",
      "Armenian monasteries nsubjpass NOUN\n",
      "the 11th century pobj NOUN\n"
     ]
    }
   ],
   "source": [
    "for c in s.noun_chunks:\n",
    "    print(c, c.root.dep_, c.root.pos_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '_recalculate_indices',\n",
       " '_vector',\n",
       " '_vector_norm',\n",
       " 'as_doc',\n",
       " 'doc',\n",
       " 'end',\n",
       " 'end_char',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ents',\n",
       " 'get_extension',\n",
       " 'get_lca_matrix',\n",
       " 'has_extension',\n",
       " 'has_vector',\n",
       " 'label',\n",
       " 'label_',\n",
       " 'lefts',\n",
       " 'lemma_',\n",
       " 'lower_',\n",
       " 'merge',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'noun_chunks',\n",
       " 'orth_',\n",
       " 'remove_extension',\n",
       " 'rights',\n",
       " 'root',\n",
       " 'sent',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'similarity',\n",
       " 'start',\n",
       " 'start_char',\n",
       " 'string',\n",
       " 'subtree',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'to_array',\n",
       " 'upper_',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Armenian\n",
      "the 11th century\n"
     ]
    }
   ],
   "source": [
    "for e in s.ents:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Parsing types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_questions = random.sample(questions, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What did employees discuss at the  second Jam event put on by PwC consulting in 2002?\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "########################################\n",
      "The interceptors were no longer built because of the shift of the bombing role to what?\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "########################################\n",
      " When did the demand for new NES software get boosted?\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "########################################\n",
      "What was proposed at the beginning of the 1991 season?\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "########################################\n",
      "Who was first to invade Manchuria?\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "########################################\n",
      "When did The Dead End Guys run?\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "########################################\n",
      "What is the name of the Presbyterian church in Brazil with Dutch origins?\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "########################################\n",
      "How many USB ports may a host controller provide?\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "########################################\n",
      "For what kind of aircraft is deicing fluid sprayed on the airfield?\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "########################################\n",
      "What state does the intensive theory conceive pain as being?\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "########################################\n",
      "What is the population of San Diego's urgan area?\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "########################################\n",
      "What political party did the Tories have to form a coalition with in 2010?\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "########################################\n",
      "Where was the Massey conference held?\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "########################################\n",
      "What was the average temperature for July 2011?\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "########################################\n",
      "What might stop a switch from transitioning quickly? \n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "########################################\n",
      "What are Tucson's typical winter high temperatures?\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "########################################\n",
      "Which stateroom has a small bow as a feature of the facade?\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "########################################\n",
      "What were Men of God expected to stay behind?\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "########################################\n",
      " What is the reason for never using capacitors in power factor correction?\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "########################################\n",
      "For how many years has Melbourne been considered the world's most liveable city?\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "########################################\n"
     ]
    }
   ],
   "source": [
    "for q in rand_questions:\n",
    "    print(q)\n",
    "    print(\"~~\"*20)\n",
    "    print()\n",
    "    print(\"#\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did employees discuss at the  second Jam event put on by PwC consulting in 2002?\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "########################################\n",
    "The interceptors were no longer built because of the shift of the bombing role to what?\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "########################################\n",
    " When did the demand for new NES software get boosted?\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "########################################\n",
    "What was proposed at the beginning of the 1991 season?\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "########################################\n",
    "Who was first to invade Manchuria?\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "########################################\n",
    "When did The Dead End Guys run?\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "########################################\n",
    "What is the name of the Presbyterian church in Brazil with Dutch origins?\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "########################################\n",
    "How many USB ports may a host controller provide?\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "########################################\n",
    "For what kind of aircraft is deicing fluid sprayed on the airfield?\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "########################################\n",
    "What state does the intensive theory conceive pain as being?\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "########################################\n",
    "What is the population of San Diego's urgan area?\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "########################################\n",
    "What political party did the Tories have to form a coalition with in 2010?\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "########################################\n",
    "Where was the Massey conference held?\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "########################################\n",
    "What was the average temperature for July 2011?\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "########################################\n",
    "What might stop a switch from transitioning quickly? \n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "########################################\n",
    "What are Tucson's typical winter high temperatures?\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "########################################\n",
    "Which stateroom has a small bow as a feature of the facade?\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "########################################\n",
    "What were Men of God expected to stay behind?\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "########################################\n",
    " What is the reason for never using capacitors in power factor correction?\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "########################################\n",
    "For how many years has Melbourne been considered the world's most liveable city?\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "########################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
