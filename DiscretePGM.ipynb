{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/train-v2.0.json\", 'r') as handle:\n",
    "    jdata = json.load(handle)\n",
    "    data = jdata['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []\n",
    "questions = []\n",
    "unanswerable = []\n",
    "answerable = []\n",
    "for i in range(len(data)):\n",
    "    section = data[i]['paragraphs']\n",
    "    for sec in section:\n",
    "        context = sec['context']\n",
    "        contexts.append(context)\n",
    "        qas = sec['qas']\n",
    "        for j in range(len(qas)):\n",
    "            question = qas[j]['question']\n",
    "            questions.append(question)\n",
    "            label = qas[j]['is_impossible']\n",
    "            if label:\n",
    "                unanswerable.append((len(contexts)-1, len(questions)-1))\n",
    "            else:\n",
    "                answerable.append((len(contexts)-1, len(questions)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1989 through 1996, the total area of the US was listed as 9,372,610 km2 (3,618,780 sq mi) (land + inland water only). The listed total area changed to 9,629,091 km2 (3,717,813 sq mi) in 1997 (Great Lakes area and coastal waters added), to 9,631,418 km2 (3,718,711 sq mi) in 2004, to 9,631,420 km2 (3,718,710 sq mi) in 2006, and to 9,826,630 km2 (3,794,080 sq mi) in 2007 (territorial waters added). Currently, the CIA World Factbook gives 9,826,675 km2 (3,794,100 sq mi), the United Nations Statistics Division gives 9,629,091 km2 (3,717,813 sq mi), and the Encyclopædia Britannica gives 9,522,055 km2 (3,676,486 sq mi)(Great Lakes area included but not coastal waters). These source consider only the 50 states and the Federal District, and exclude overseas territories.\n",
      "####################\n",
      "According to the Encyclopedia Britannica, what is the total area of the US in miles?\n"
     ]
    }
   ],
   "source": [
    "i = np.random.choice(range(len(answerable)))\n",
    "c_i, q_i = answerable[i]\n",
    "print(contexts[c_i])\n",
    "print(\"##\"*10)\n",
    "print(questions[q_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "def test(sent1, sent2):\n",
    "    # Tokenized input\n",
    "    sent1_tokens =  tokenizer.tokenize(sent1)\n",
    "    sent2_tokens =  tokenizer.tokenize(sent2)\n",
    "    tokenized_text = tokenizer.tokenize(sent1 + \" \" + sent2)\n",
    "    print(len(tokenized_text))\n",
    "\n",
    "    # Convert token to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "    segments_ids = [0]*len(sent1_tokens) + [1]*len(sent2_tokens)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    model.eval()\n",
    "\n",
    "    # Predict all tokens\n",
    "    output = model(tokens_tensor,  segments_tensors)\n",
    "    return output,tokenized_text\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287\n"
     ]
    }
   ],
   "source": [
    "output, tokenized_text = test(contexts[c_i], questions[q_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1989 through 1996, the total area of the US was listed as 9,372,610 km2 (3,618,780 sq mi) (land + inland water only). The listed total area changed to 9,629,091 km2 (3,717,813 sq mi) in 1997 (Great Lakes area and coastal waters added), to 9,631,418 km2 (3,718,711 sq mi) in 2004, to 9,631,420 km2 (3,718,710 sq mi) in 2006, and to 9,826,630 km2 (3,794,080 sq mi) in 2007 (territorial waters added). Currently, the CIA World Factbook gives 9,826,675 km2 (3,794,100 sq mi), the United Nations Statistics Division gives 9,629,091 km2 (3,717,813 sq mi), and the Encyclopædia Britannica gives 9,522,055 km2 (3,676,486 sq mi)(Great Lakes area included but not coastal waters). These source consider only the 50 states and the Federal District, and exclude overseas territories.\n",
      "According to the Encyclopedia Britannica, what is the total area of the US in miles?\n",
      "[('from', 0), ('1989', 1), ('through', 2), ('1996', 3), (',', 4), ('the', 5), ('total', 6), ('area', 7), ('of', 8), ('the', 9), ('us', 10), ('was', 11), ('listed', 12), ('as', 13), ('9', 14), (',', 15), ('37', 16), ('##2', 17), (',', 18), ('610', 19), ('km', 20), ('##2', 21), ('(', 22), ('3', 23), (',', 24), ('61', 25), ('##8', 26), (',', 27), ('780', 28), ('sq', 29), ('mi', 30), (')', 31), ('(', 32), ('land', 33), ('+', 34), ('inland', 35), ('water', 36), ('only', 37), (')', 38), ('.', 39), ('the', 40), ('listed', 41), ('total', 42), ('area', 43), ('changed', 44), ('to', 45), ('9', 46), (',', 47), ('62', 48), ('##9', 49), (',', 50), ('09', 51), ('##1', 52), ('km', 53), ('##2', 54), ('(', 55), ('3', 56), (',', 57), ('71', 58), ('##7', 59), (',', 60), ('81', 61), ('##3', 62), ('sq', 63), ('mi', 64), (')', 65), ('in', 66), ('1997', 67), ('(', 68), ('great', 69), ('lakes', 70), ('area', 71), ('and', 72), ('coastal', 73), ('waters', 74), ('added', 75), (')', 76), (',', 77), ('to', 78), ('9', 79), (',', 80), ('63', 81), ('##1', 82), (',', 83), ('41', 84), ('##8', 85), ('km', 86), ('##2', 87), ('(', 88), ('3', 89), (',', 90), ('71', 91), ('##8', 92), (',', 93), ('71', 94), ('##1', 95), ('sq', 96), ('mi', 97), (')', 98), ('in', 99), ('2004', 100), (',', 101), ('to', 102), ('9', 103), (',', 104), ('63', 105), ('##1', 106), (',', 107), ('420', 108), ('km', 109), ('##2', 110), ('(', 111), ('3', 112), (',', 113), ('71', 114), ('##8', 115), (',', 116), ('710', 117), ('sq', 118), ('mi', 119), (')', 120), ('in', 121), ('2006', 122), (',', 123), ('and', 124), ('to', 125), ('9', 126), (',', 127), ('82', 128), ('##6', 129), (',', 130), ('630', 131), ('km', 132), ('##2', 133), ('(', 134), ('3', 135), (',', 136), ('79', 137), ('##4', 138), (',', 139), ('08', 140), ('##0', 141), ('sq', 142), ('mi', 143), (')', 144), ('in', 145), ('2007', 146), ('(', 147), ('territorial', 148), ('waters', 149), ('added', 150), (')', 151), ('.', 152), ('currently', 153), (',', 154), ('the', 155), ('cia', 156), ('world', 157), ('fact', 158), ('##book', 159), ('gives', 160), ('9', 161), (',', 162), ('82', 163), ('##6', 164), (',', 165), ('67', 166), ('##5', 167), ('km', 168), ('##2', 169), ('(', 170), ('3', 171), (',', 172), ('79', 173), ('##4', 174), (',', 175), ('100', 176), ('sq', 177), ('mi', 178), (')', 179), (',', 180), ('the', 181), ('united', 182), ('nations', 183), ('statistics', 184), ('division', 185), ('gives', 186), ('9', 187), (',', 188), ('62', 189), ('##9', 190), (',', 191), ('09', 192), ('##1', 193), ('km', 194), ('##2', 195), ('(', 196), ('3', 197), (',', 198), ('71', 199), ('##7', 200), (',', 201), ('81', 202), ('##3', 203), ('sq', 204), ('mi', 205), (')', 206), (',', 207), ('and', 208), ('the', 209), ('en', 210), ('##cy', 211), ('##cl', 212), ('##op', 213), ('##æ', 214), ('##dia', 215), ('brit', 216), ('##ann', 217), ('##ica', 218), ('gives', 219), ('9', 220), (',', 221), ('52', 222), ('##2', 223), (',', 224), ('05', 225), ('##5', 226), ('km', 227), ('##2', 228), ('(', 229), ('3', 230), (',', 231), ('67', 232), ('##6', 233), (',', 234), ('48', 235), ('##6', 236), ('sq', 237), ('mi', 238), (')', 239), ('(', 240), ('great', 241), ('lakes', 242), ('area', 243), ('included', 244), ('but', 245), ('not', 246), ('coastal', 247), ('waters', 248), (')', 249), ('.', 250), ('these', 251), ('source', 252), ('consider', 253), ('only', 254), ('the', 255), ('50', 256), ('states', 257), ('and', 258), ('the', 259), ('federal', 260), ('district', 261), (',', 262), ('and', 263), ('exclude', 264), ('overseas', 265), ('territories', 266), ('.', 267), ('according', 268), ('to', 269), ('the', 270), ('encyclopedia', 271), ('brit', 272), ('##ann', 273), ('##ica', 274), (',', 275), ('what', 276), ('is', 277), ('the', 278), ('total', 279), ('area', 280), ('of', 281), ('the', 282), ('us', 283), ('in', 284), ('miles', 285), ('?', 286)]\n"
     ]
    }
   ],
   "source": [
    "print(contexts[c_i])\n",
    "print(questions[q_i])\n",
    "print (list(zip(tokenized_text, range(len(tokenized_text)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('overseas', 265), 7.748758), (('district', 261), 9.652206), (('and', 258), 10.993108), (('federal', 260), 11.238921), (('waters', 248), 11.343638)]\n"
     ]
    }
   ],
   "source": [
    "key_index = 266\n",
    "min_dist = 99999\n",
    "min_index = -1\n",
    "close_terms = Counter()\n",
    "for word, index in zip(tokenized_text, range(len(tokenized_text))):\n",
    "    if index == key_index :#or tokenized_text[index] == tokenized_text[key_index]:\n",
    "        continue\n",
    "    dist = np.linalg.norm(output[0][-1][0,key_index,:].data.numpy()\n",
    "                          -output[0][-1][0,index,:].data.numpy())\n",
    "    close_terms[(tokenized_text[index], index)] = dist\n",
    "    if dist < min_dist:\n",
    "        min_dist = dist\n",
    "        min_index = index\n",
    "print(list(reversed(close_terms.most_common()))[:5])  #(tokenized_text[min_index], min_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_encoder_layers, _ = output\n",
    "all_encoder_layers = all_encoder_layers\n",
    "\n",
    "for b, example_index in enumerate(example_indices):\n",
    "    feature = features[example_index.item()]\n",
    "    unique_id = int(feature.unique_id)\n",
    "    # feature = unique_id_to_feature[unique_id]\n",
    "    output_json = collections.OrderedDict()\n",
    "    output_json[\"linex_index\"] = unique_id\n",
    "    all_out_features = []\n",
    "    for (i, token) in enumerate(feature.tokens):\n",
    "        all_layers = []\n",
    "        for (j, layer_index) in enumerate(layer_indexes):\n",
    "            layer_output = all_encoder_layers[int(layer_index)].detach().cpu().numpy()\n",
    "            layer_output = layer_output[b]\n",
    "            layers = collections.OrderedDict()\n",
    "            layers[\"index\"] = layer_index\n",
    "            layers[\"values\"] = [\n",
    "                round(x.item(), 6) for x in layer_output[i]\n",
    "            ]\n",
    "            all_layers.append(layers)\n",
    "        out_features = collections.OrderedDict()\n",
    "        out_features[\"token\"] = token\n",
    "        out_features[\"layers\"] = all_layers\n",
    "        all_out_features.append(out_features)\n",
    "    output_json[\"features\"] = all_out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
