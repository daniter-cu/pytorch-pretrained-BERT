{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your label namespace was 'pos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n"
     ]
    }
   ],
   "source": [
    "res = predictor.predict(\n",
    "  sentence=\"If I bring 10 dollars tomorrow, can you buy me lunch?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['class_probabilities', 'spans', 'tokens', 'pos_tags', 'num_spans', 'hierplane_tree', 'trees'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spans\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "[[0, 0], [0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [0, 9], [0, 10], [0, 11], [0, 12], [1, 1], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [1, 8], [1, 9], [1, 10], [1, 11], [1, 12], [2, 2], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [2, 10], [2, 11], [2, 12], [3, 3], [3, 4], [3, 5], [3, 6], [3, 7], [3, 8], [3, 9], [3, 10], [3, 11], [3, 12], [4, 4], [4, 5], [4, 6], [4, 7], [4, 8], [4, 9], [4, 10], [4, 11], [4, 12], [5, 5], [5, 6], [5, 7], [5, 8], [5, 9], [5, 10], [5, 11], [5, 12], [6, 6], [6, 7], [6, 8], [6, 9], [6, 10], [6, 11], [6, 12], [7, 7], [7, 8], [7, 9], [7, 10], [7, 11], [7, 12], [8, 8], [8, 9], [8, 10], [8, 11], [8, 12], [9, 9], [9, 10], [9, 11], [9, 12], [10, 10], [10, 11], [10, 12], [11, 11], [11, 12], [12, 12]]\n",
      "####################\n",
      "tokens\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "['If', 'I', 'bring', '10', 'dollars', 'tomorrow', ',', 'can', 'you', 'buy', 'me', 'lunch', '?']\n",
      "####################\n",
      "pos_tags\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "['IN', 'PRP', 'VBP', 'CD', 'NNS', 'NN', ',', 'MD', 'PRP', 'VB', 'PRP', 'NN', '.']\n",
      "####################\n",
      "num_spans\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "91\n",
      "####################\n",
      "hierplane_tree\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "{'linkNameToLabel': {'.': 'pos', ',': 'pos', '-LRB-': 'pos', '-RRB-': 'pos', '``': 'pos', '\"\"': 'pos', \"''\": 'pos', ':': 'pos', '$': 'pos', '#': 'pos', 'AFX': 'pos', 'CC': 'pos', 'CD': 'pos', 'DT': 'pos', 'EX': 'pos', 'FW': 'pos', 'HYPH': 'pos', 'IN': 'pos', 'JJ': 'pos', 'JJR': 'pos', 'JJS': 'pos', 'LS': 'pos', 'MD': 'pos', 'NIL': 'pos', 'NN': 'pos', 'NNP': 'pos', 'NNPS': 'pos', 'NNS': 'pos', 'PDT': 'pos', 'POS': 'pos', 'PRP': 'pos', 'PRP$': 'pos', 'RB': 'pos', 'RBR': 'pos', 'RBS': 'pos', 'RP': 'pos', 'SP': 'pos', 'SYM': 'pos', 'TO': 'pos', 'UH': 'pos', 'VB': 'pos', 'VBD': 'pos', 'VBG': 'pos', 'VBN': 'pos', 'VBP': 'pos', 'VBZ': 'pos', 'WDT': 'pos', 'WP': 'pos', 'WP$': 'pos', 'WRB': 'pos', 'ADD': 'pos', 'NFP': 'pos', 'GW': 'pos', 'XX': 'pos', 'BES': 'pos', 'HVS': 'pos', '_SP': 'pos'}, 'nodeTypeToStyle': {'.': ['color0'], ',': ['color0'], '-LRB-': ['color0'], '-RRB-': ['color0'], '``': ['color0'], '\"\"': ['color0'], \"''\": ['color0'], ':': ['color0'], '$': ['color0'], '#': ['color0'], 'AFX': ['color0'], 'CC': ['color0'], 'CD': ['color0'], 'DT': ['color0'], 'EX': ['color0'], 'FW': ['color0'], 'HYPH': ['color0'], 'IN': ['color0'], 'JJ': ['color0'], 'JJR': ['color0'], 'JJS': ['color0'], 'LS': ['color0'], 'MD': ['color0'], 'NIL': ['color0'], 'NN': ['color0'], 'NNP': ['color0'], 'NNPS': ['color0'], 'NNS': ['color0'], 'PDT': ['color0'], 'POS': ['color0'], 'PRP': ['color0'], 'PRP$': ['color0'], 'RB': ['color0'], 'RBR': ['color0'], 'RBS': ['color0'], 'RP': ['color0'], 'SP': ['color0'], 'SYM': ['color0'], 'TO': ['color0'], 'UH': ['color0'], 'VB': ['color0'], 'VBD': ['color0'], 'VBG': ['color0'], 'VBN': ['color0'], 'VBP': ['color0'], 'VBZ': ['color0'], 'WDT': ['color0'], 'WP': ['color0'], 'WP$': ['color0'], 'WRB': ['color0'], 'ADD': ['color0'], 'NFP': ['color0'], 'GW': ['color0'], 'XX': ['color0'], 'BES': ['color0'], 'HVS': ['color0'], '_SP': ['color0'], 'NP': ['color1'], 'NX': ['color1'], 'QP': ['color1'], 'NAC': ['color1'], 'VP': ['color2'], 'S': ['color3'], 'SQ': ['color3'], 'SBAR': ['color3'], 'SBARQ': ['color3'], 'SINQ': ['color3'], 'FRAG': ['color3'], 'X': ['color3'], 'WHADVP': ['color4'], 'WHADJP': ['color4'], 'WHNP': ['color4'], 'WHPP': ['color4'], 'PP': ['color6'], 'ADJP': ['color5'], 'ADVP': ['color5'], 'CONJP': ['color5'], 'INTJ': ['color5'], 'LST': ['color5', 'seq'], 'PRN': ['color5'], 'PRT': ['color5'], 'RRC': ['color5'], 'UCP': ['color5']}, 'text': 'If I bring 10 dollars tomorrow , can you buy me lunch ?', 'root': {'word': 'If I bring 10 dollars tomorrow , can you buy me lunch ?', 'nodeType': 'SQ', 'attributes': ['SQ'], 'link': 'SQ', 'children': [{'word': 'If I bring 10 dollars tomorrow', 'nodeType': 'SBAR', 'attributes': ['SBAR'], 'link': 'SBAR', 'children': [{'word': 'If', 'nodeType': 'IN', 'attributes': ['IN'], 'link': 'IN'}, {'word': 'I bring 10 dollars tomorrow', 'nodeType': 'S', 'attributes': ['S'], 'link': 'S', 'children': [{'word': 'I', 'nodeType': 'NP', 'attributes': ['NP'], 'link': 'NP', 'children': [{'word': 'I', 'nodeType': 'PRP', 'attributes': ['PRP'], 'link': 'PRP'}]}, {'word': 'bring 10 dollars tomorrow', 'nodeType': 'VP', 'attributes': ['VP'], 'link': 'VP', 'children': [{'word': 'bring', 'nodeType': 'VBP', 'attributes': ['VBP'], 'link': 'VBP'}, {'word': '10 dollars', 'nodeType': 'NP', 'attributes': ['NP'], 'link': 'NP', 'children': [{'word': '10', 'nodeType': 'CD', 'attributes': ['CD'], 'link': 'CD'}, {'word': 'dollars', 'nodeType': 'NNS', 'attributes': ['NNS'], 'link': 'NNS'}]}, {'word': 'tomorrow', 'nodeType': 'NP', 'attributes': ['NP'], 'link': 'NP', 'children': [{'word': 'tomorrow', 'nodeType': 'NN', 'attributes': ['NN'], 'link': 'NN'}]}]}]}]}, {'word': ',', 'nodeType': ',', 'attributes': [','], 'link': ','}, {'word': 'can', 'nodeType': 'MD', 'attributes': ['MD'], 'link': 'MD'}, {'word': 'you', 'nodeType': 'NP', 'attributes': ['NP'], 'link': 'NP', 'children': [{'word': 'you', 'nodeType': 'PRP', 'attributes': ['PRP'], 'link': 'PRP'}]}, {'word': 'buy me lunch', 'nodeType': 'VP', 'attributes': ['VP'], 'link': 'VP', 'children': [{'word': 'buy', 'nodeType': 'VB', 'attributes': ['VB'], 'link': 'VB'}, {'word': 'me', 'nodeType': 'NP', 'attributes': ['NP'], 'link': 'NP', 'children': [{'word': 'me', 'nodeType': 'PRP', 'attributes': ['PRP'], 'link': 'PRP'}]}, {'word': 'lunch', 'nodeType': 'NP', 'attributes': ['NP'], 'link': 'NP', 'children': [{'word': 'lunch', 'nodeType': 'NN', 'attributes': ['NN'], 'link': 'NN'}]}]}, {'word': '?', 'nodeType': '.', 'attributes': ['.'], 'link': '.'}]}}\n",
      "####################\n",
      "trees\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "(SQ (SBAR (IN If) (S (NP (PRP I)) (VP (VBP bring) (NP (CD 10) (NNS dollars)) (NP (NN tomorrow))))) (, ,) (MD can) (NP (PRP you)) (VP (VB buy) (NP (PRP me)) (NP (NN lunch))) (. ?))\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "for k, v in res.items():\n",
    "    if k == 'class_probabilities':\n",
    "        continue\n",
    "    print(k)\n",
    "    print(\"~\"*20)\n",
    "    print(v)\n",
    "    print(\"#\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(SQ   \n",
    " (SBAR    \n",
    "  (IN If)    \n",
    "  (S    \n",
    "   (NP (PRP I))    \n",
    "   (VP (VBP bring)    \n",
    "    (NP (CD 10) (NNS dollars))    \n",
    "    (NP (NN tomorrow)))   \n",
    "  )   \n",
    " )    \n",
    " (, ,)    \n",
    " (MD can)    \n",
    " (NP (PRP you))    \n",
    " (VP (VB buy)    \n",
    "  (NP (PRP me))    \n",
    "  (NP (NN lunch))   \n",
    " )    \n",
    " (. ?)   \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "contexts = []\n",
    "examples = []\n",
    "with open(\"dataset/dev-v2.0.json\", 'r') as handle:\n",
    "    jdata = json.load(handle)\n",
    "    data = jdata['data']\n",
    "for i in range(len(data)):\n",
    "    section = data[i]['paragraphs']\n",
    "    for sec in section:\n",
    "        context = sec['context']\n",
    "        contexts.append(context)\n",
    "        qas = sec['qas']\n",
    "        for j in range(len(qas)):\n",
    "            question = qas[j]['question']\n",
    "            unanswerable = qas[j]['is_impossible']\n",
    "            questions.append(question)\n",
    "            examples.append((len(contexts)-1, len(questions)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.93 s, sys: 26.8 ms, total: 4.96 s\n",
      "Wall time: 1.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trees = []\n",
    "for q in questions[:10]:\n",
    "    res = predictor.predict(sentence=q)\n",
    "    trees.append(res['trees'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(SBARQ (IN In) (WHNP (WP what) (NN country)) (SQ (VBZ is) (NP (NNP Normandy)) (VP (VBN located))) (. ?))',\n",
       " '(SBARQ (WHADVP (WRB When)) (SINV (VBD were) (NP (DT the) (NNPS Normans)) (PP (IN in) (NP (NNP Normandy)))) (. ?))',\n",
       " '(SBARQ (WHPP (IN From) (WHNP (WDT which) (NNS countries))) (SQ (VBD did) (NP (DT the) (NNP Norse)) (VP (VB originate))) (. ?))',\n",
       " '(SBARQ (WHNP (WP Who)) (SQ (VP (VBD was) (NP (DT the) (NNP Norse) (NN leader)))) (. ?))',\n",
       " '(SBARQ (WHNP (WDT What) (NN century)) (SQ (VBD did) (NP (DT the) (NNPS Normans)) (ADVP (RB first)) (VP (VBP gain) (NP (PRP$ their) (JJ separate) (NN identity)))) (. ?))',\n",
       " \"(SBARQ (WHNP (WP Who)) (S (VP (VBD gave) (NP (PRP$ their) (NN name)) (PP (IN to) (NP (NNP Normandy))) (PP (IN in) (NP (NP (DT the) (CD 1000) (POS 's)) (CC and) (NP (CD 1100) (POS 's)))))))\",\n",
       " '(SBARQ (WHNP (WP What)) (SQ (VBZ is) (NP (NNP France)) (NP (NP (DT a) (NN region)) (PP (IN of)))) (. ?))',\n",
       " '(SBARQ (WHNP (WP Who)) (SQ (VBD did) (NP (NNP King) (NNP Charles) (NNP III)) (VP (VBP swear) (NP (NN fealty)) (PP (IN to)))) (. ?))',\n",
       " '(SBARQ (WHADVP (WRB When)) (SQ (VBD did) (NP (DT the) (JJ Frankish) (NN identity)) (VP (VB emerge))) (. ?))',\n",
       " '(SBARQ (WHNP (WP Who)) (SQ (VBD was) (NP (DT the) (NN duke)) (PP (IN in) (NP (NP (DT the) (NN battle)) (PP (IN of) (NP (NNP Hastings)))))) (. ?))']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is not used for a precise definition of what it means to solve a problem using a given amount of time and space?\n",
      "Calling...\n",
      "####################\n",
      "How is Turing machine M said not to operate?\n",
      "Calling...\n",
      "####################\n",
      "What is the expression used to identify any given series of solutions capable of being solved within time on a deterministic Turing machine?\n",
      "Calling...\n",
      "####################\n",
      "What is the least critical resource measured in assessing the determination of a Turing machine's ability to solve any given set of problems?\n",
      "Calling...\n",
      "####################\n",
      "How can decision problem B be solved in time x(f)?\n",
      "Calling...\n",
      "####################\n",
      "Time and space are both examples of what type of resource?\n",
      "Calling...\n",
      "####################\n",
      "A complexity resource can also be described as what other type of resource?\n",
      "Calling...\n",
      "####################\n",
      "What is typically used to broadly define complexity measures?\n",
      "Calling...\n",
      "####################\n",
      "Communication complexity is an example of what type of measure?\n",
      "Calling...\n",
      "####################\n",
      "Decision tree is an example of what type of measure?\n",
      "Calling...\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "def print_np(entry):\n",
    "    phrase = \"\"\n",
    "    for child in entry['children']:\n",
    "        if child['nodeType'] != 'DT':\n",
    "            phrase += child['word']+\" \"\n",
    "    return(\"[\"+entry['nodeType']+\"]\"+phrase.strip())\n",
    "\n",
    "def print_nps(entry, pos=None):\n",
    "    print(\"Calling...\")\n",
    "    if entry['nodeType'].startswith('VB'):\n",
    "        if not nlp.vocab[entry['word'].lower()].is_stop:\n",
    "            yield \"[\"+entry['nodeType']+\"]\"+ entry['word']\n",
    "    elif entry['nodeType'].startswith(\"WH\"):\n",
    "        yield \"[\"+entry['nodeType']+\"]\"+ entry['word']\n",
    "    elif entry['nodeType'] == 'NP':\n",
    "        keep = True\n",
    "        for child in entry['children']:\n",
    "            if child['nodeType'] == 'NP' or child['nodeType'] == 'PP':\n",
    "                keep = False\n",
    "        if keep:\n",
    "            yield print_np(entry) # (entry['word'])\n",
    "        else:\n",
    "            if 'children' in entry and entry['children']:\n",
    "                for  child in entry['children']:\n",
    "                    print_nps(child)\n",
    "    else:\n",
    "        if 'children' in entry and entry['children']:\n",
    "            for child in entry['children']:\n",
    "                print_nps(child)\n",
    "\n",
    "for q in questions[360:370]:\n",
    "    print(q)\n",
    "    res = predictor.predict(sentence=q)\n",
    "    for x in print_nps(res['hierplane_tree']['root']):\n",
    "        print(x)\n",
    "    #print(res['trees'])\n",
    "    print(\"#\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "maintain order in x of y scenarios  \n",
    "capture the NN part of NP where there is a PP inside the NP\n",
    "\n",
    "* generate all the parts\n",
    "* include any required dependencies\n",
    "* generate the training data with masks and different # of conditionals \n",
    "* write model to train\n",
    "* kick off a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "import json\n",
    "import pickle\n",
    "import spacy\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_np(entry):\n",
    "    phrase = \"\"\n",
    "    for child in entry['children']:\n",
    "        if child['nodeType'] != 'DT':\n",
    "            phrase += child['word']+\" \"\n",
    "    return((\"[\"+entry['nodeType']+\"]\",phrase.strip()))\n",
    "\n",
    "def get_q_parts(entry, nlp, tokens):\n",
    "    if entry['nodeType'].startswith('VB'):\n",
    "        if not nlp.vocab[entry['word'].lower()].is_stop:\n",
    "            tokens.append((\"[\"+entry['nodeType']+\"]\", entry['word']))\n",
    "    elif entry['nodeType'].startswith(\"WH\"):\n",
    "        tokens.append((\"[\"+entry['nodeType']+\"]\", entry['word']))\n",
    "    elif entry['nodeType'] == 'NP':\n",
    "        keep = True\n",
    "        for child in entry['children']:\n",
    "            if child['nodeType'] == 'NP' or child['nodeType'] == 'PP':\n",
    "                keep = False\n",
    "        if keep:\n",
    "            tokens.append(print_np(entry)) # (entry['word'])\n",
    "        else:\n",
    "            if 'children' in entry and entry['children']:\n",
    "                for child in entry['children']:\n",
    "                    get_q_parts(child, nlp, tokens)\n",
    "    else:\n",
    "        if 'children' in entry and entry['children']:\n",
    "            for child in entry['children']:\n",
    "                get_q_parts(child, nlp, tokens)\n",
    "\n",
    "\n",
    "def build_labels(dev_data_file, test_data_file, limit=None):\n",
    "    predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    questions = {}\n",
    "\n",
    "    for data_file in [dev_data_file, test_data_file]:\n",
    "        with open(data_file, 'r') as handle: # update\n",
    "            jdata = json.load(handle)\n",
    "            data = jdata['data']\n",
    "        for i in range(len(data)):\n",
    "            section = data[i]['paragraphs']\n",
    "            for sec in section:\n",
    "                qas = sec['qas']\n",
    "                for j in range(len(qas)):\n",
    "                    qid = qas[j]['id']\n",
    "                    question = qas[j]['question']\n",
    "                    questions[qid] = question\n",
    "\n",
    "    labels = {}\n",
    "    counter = 0\n",
    "    section = 0#int(sys.argv[1])\n",
    "    chunk = 6000\n",
    "    for id, q in list(questions.items())[chunk*section:chunk*(section+1)] if limit is None else list(questions.items())[:limit]:\n",
    "        res = predictor.predict(sentence=q)\n",
    "        tokens = []\n",
    "        get_q_parts(res['hierplane_tree']['root'], nlp, tokens)\n",
    "        labels[id] = tokens\n",
    "        counter += 1\n",
    "        if counter % 1000 == 0:\n",
    "            print(\"Finished with \", str(counter))\n",
    "    return labels\n",
    "\n",
    "#     with open(\"part_labels\"+str(section)+\".pkl\", \"wb\") as f:\n",
    "#         pickle.dump(labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = build_labels(\"dataset/dev-v2.0.json\", \"dataset/train-v2.0.json\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From which countries did the Norse originate?'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from']\n",
      "['the']\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "t1 = tokenizer.tokenize(questions[2])\n",
    "t2 = tokenizer.tokenize(contexts[0])\n",
    "match = SequenceMatcher(None,  \n",
    "                        t1, \n",
    "                        t2, \n",
    "                        autojunk=False).get_matching_blocks()\n",
    "for m in match:\n",
    "    if m.size == 0:\n",
    "        continue\n",
    "    print(t1[m.a:m.a+m.size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from']\n",
      "['the']\n",
      "['norse']\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "t1 = tokenizer.tokenize(questions[2])\n",
    "t2 = tokenizer.tokenize(contexts[0])\n",
    "while i < len(t1) - 1:\n",
    "    match = SequenceMatcher(None,  \n",
    "                            t1, \n",
    "                            t2, \n",
    "                            autojunk=False).find_longest_match(i, len(t1), 0, len(t2))\n",
    "    if match.size == 0:\n",
    "        break\n",
    "    print(t1[match.a:match.a+match.size])\n",
    "    i = match.a+match.size\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['from', 'which', 'countries', 'did', 'the', 'norse', 'originate', '?']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_stop = [\"a\", \"an\", \"and\", \"as\", \"or\", \"the\", \"that\", \"which\", \"when\", \"whose\", \"is\", \"was\", \"what\", \"to\", \"in\", \"at\"]\n",
    "strip_stop = [\"a\", \"an\", \"and\", \"as\", \"or\", \"the\", \"that\", \"which\", \"when\", \"whose\", \"is\", \"was\", \"what\", \"to\", \"of\"]\n",
    "#my_stop = strip_stop + [\"to\", \"in\", \"at\", \"on\", \"under\"]\n",
    "\n",
    "\n",
    "def individual_filter(term):\n",
    "    if term.startswith(\"##\"):\n",
    "        return False\n",
    "    if term in nlp.vocab and  (nlp.vocab[term].is_stop or nlp.vocab[term].is_punct):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def simple_filter(term):\n",
    "    if term in strip_stop or (term in nlp.vocab and nlp.vocab[term].is_punct):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def strip_terms(phrase):\n",
    "    fltr = [simple_filter(t) for t in phrase]\n",
    "    start = fltr.index(True)\n",
    "    end = list(reversed(fltr)).index(True)\n",
    "    return phrase[start:len(fltr)-end]\n",
    "\n",
    "def matcher(source, target):\n",
    "    i = 0\n",
    "    matches = []\n",
    "    while i < len(source):\n",
    "        ii = 1\n",
    "        current = []\n",
    "        for j in range(len(target)):\n",
    "            if source[i] == target[j]:\n",
    "                cand = []\n",
    "                for ii in range(len(source) - i):\n",
    "                    if j+ii > len(target) - 1:\n",
    "                        break\n",
    "                    if source[i+ii] == target[j+ii]:\n",
    "                        cand.append(source[i+ii])\n",
    "                    else:\n",
    "                        if len(cand) > len(current):\n",
    "                            current = list(cand)\n",
    "                        break\n",
    "                if len(cand) > len(current):\n",
    "                    current = list(cand)\n",
    "        if current:\n",
    "            matches.append(current)\n",
    "        i += len(current) if current else 1\n",
    "        \n",
    "    ## filters\n",
    "    matches = [m for m in matches if sum(\n",
    "        [individual_filter(token) for token in m]) > 0]\n",
    "    \n",
    "    matches = [strip_terms(m) for m in matches]\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['norse']]\n"
     ]
    }
   ],
   "source": [
    "print(matcher(t1, t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[','].is_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches(q, c):\n",
    "    t1 = tokenizer.tokenize(q)\n",
    "    t2 = tokenizer.tokenize(c)\n",
    "    matches = []\n",
    "    i = 0\n",
    "    while i < len(t1) - 1:\n",
    "        match = SequenceMatcher(None,  \n",
    "                                t1, \n",
    "                                t2, \n",
    "                                autojunk=False).find_longest_match(i, len(t1), 0, len(t2))\n",
    "        if match.size == 0:\n",
    "            break\n",
    "        matches.append(t1[match.a:match.a+match.size])\n",
    "        i = match.a+match.size\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION What are both Branko Milanovic and Joseph Stiglitz?\n",
      "MATCHED WORDS [['bran', '##ko', 'milano', '##vic'], ['joseph', 'st', '##ig', '##litz']]\n",
      "SPANS FOUND {'What are both Branko Milanovic and Joseph Stiglitz ?': 'SBARQ', 'What': 'WP', 'are both Branko Milanovic and Joseph Stiglitz': 'SQ', 'are': 'VBP', 'both Branko Milanovic and Joseph Stiglitz': 'NP', 'both': 'DT', 'Branko Milanovic and Joseph Stiglitz': 'NP', 'Branko Milanovic': 'NP', 'Branko': 'NNP', 'Milanovic': 'NNP', 'and': 'CC', 'Joseph Stiglitz': 'NP', 'Joseph': 'NNP', 'Stiglitz': 'NNP', '?': '.'}\n",
      "~~~~ ['bran', '##ko', 'milano', '##vic'] -> ('Branko Milanovic', 'NP')\n",
      "~~~~ ['joseph', 'st', '##ig', '##litz'] -> ('Joseph Stiglitz', 'NP')\n",
      "TOKENS [('[WHNP]', 'What'), ('[NP]', 'Branko Milanovic'), ('[NP]', 'Joseph Stiglitz')]\n",
      "####################\n",
      "QUESTION How many people did Hamas kill between 2010 to 2017?\n",
      "MATCHED WORDS [['people'], ['hamas']]\n",
      "SPANS FOUND {'How many people did Hamas kill between 2010 to 2017 ?': 'SBARQ', 'How many people': 'WHNP', 'How many': 'WHADJP', 'How': 'WRB', 'many': 'JJ', 'people': 'NNS', 'did Hamas kill between 2010 to 2017': 'SQ', 'did': 'VBD', 'Hamas': 'NNP', 'kill between 2010 to 2017': 'VP', 'kill': 'VB', 'between 2010 to 2017': 'PP', 'between 2010': 'PP', 'between': 'IN', '2010': 'CD', 'to 2017': 'PP', 'to': 'IN', '2017': 'CD', '?': '.'}\n",
      "~~~~ ['people'] -> ('people', 'NNS')\n",
      "~~~~ ['hamas'] -> ('Hamas', 'NNP')\n",
      "TOKENS [('[WHNP]', 'How many people'), ('[NP]', 'Hamas'), ('[VB]', 'kill'), ('[NP]', '2010'), ('[NP]', '2017')]\n",
      "####################\n",
      "QUESTION What is an example of work that a centrifugal governor-equipped steam engine was suitable for?\n",
      "MATCHED WORDS [['cent', '##ri', '##fu', '##gal', 'governor'], ['equipped'], ['steam', 'engine'], ['suitable', 'for']]\n",
      "SPANS FOUND {'What is an example of work that a centrifugal governor - equipped steam engine was suitable for ?': 'SBARQ', 'What': 'WP', 'is an example of work that a centrifugal governor - equipped steam engine was suitable for': 'SQ', 'is': 'VBZ', 'an example of work that a centrifugal governor - equipped steam engine was suitable for': 'NP', 'an example': 'NP', 'an': 'DT', 'example': 'NN', 'of': 'IN', 'work that a centrifugal governor - equipped steam engine was suitable for': 'NP', 'work': 'NN', 'that a centrifugal governor - equipped steam engine was suitable for': 'SBAR', 'that': 'WDT', 'a centrifugal governor - equipped steam engine was suitable for': 'S', 'a centrifugal governor - equipped steam engine': 'NP', 'a centrifugal governor': 'NP', 'a': 'DT', 'centrifugal': 'JJ', 'governor': 'NN', '-': 'HYPH', 'equipped steam engine': 'NP', 'equipped': 'VBN', 'steam': 'NN', 'engine': 'NN', 'was suitable for': 'VP', 'was': 'VBD', 'suitable for': 'ADJP', 'suitable': 'JJ', 'for': 'IN', '?': '.'}\n",
      "~~~~ ['cent', '##ri', '##fu', '##gal', 'governor'] -> ('a centrifugal governor', 'NP')\n",
      "~~~~ ['equipped'] -> ('equipped', 'VBN')\n",
      "~~~~ ['steam', 'engine'] -> ('equipped steam engine', 'NP')\n",
      "~~~~ ['suitable', 'for'] -> ('suitable for', 'ADJP')\n",
      "TOKENS [('[WHNP]', 'What'), ('[NP]', 'example'), ('[NP]', 'work'), ('[WHNP]', 'that'), ('[NP]', 'centrifugal governor'), ('[NP]', 'equipped steam engine')]\n",
      "####################\n",
      "QUESTION What is oxygen respiration therapy used to treat?\n",
      "MATCHED WORDS [['oxygen'], ['res', '##piration'], ['therapy'], ['used', 'to', 'treat']]\n",
      "SPANS FOUND {'What is oxygen respiration therapy used to treat ?': 'SBARQ', 'What': 'WP', 'is oxygen respiration therapy used to treat': 'SQ', 'is': 'VBZ', 'oxygen respiration therapy': 'NP', 'oxygen': 'NN', 'respiration': 'NN', 'therapy': 'NN', 'used to treat': 'VP', 'used': 'VBD', 'to treat': 'VP', 'to': 'TO', 'treat': 'VB', '?': '.'}\n",
      "~~~~ ['oxygen'] -> ('oxygen', 'NN')\n",
      "~~~~ ['res', '##piration'] -> ('respiration', 'NN')\n",
      "~~~~ ['therapy'] -> ('therapy', 'NN')\n",
      "~~~~ ['used', 'to', 'treat'] -> ('used to treat', 'VP')\n",
      "TOKENS [('[WHNP]', 'What'), ('[NP]', 'oxygen respiration therapy'), ('[VB]', 'treat')]\n",
      "####################\n",
      "QUESTION What prominent South African family distilled brandy and were Huguenots?\n",
      "MATCHED WORDS [['south'], ['di', '##sti'], ['brandy'], ['hug', '##uen', '##ots']]\n",
      "SPANS FOUND {'What prominent South African family distilled brandy and were Huguenots ?': 'SBARQ', 'What prominent South African family': 'WHNP', 'What': 'WDT', 'prominent South African': 'ADJP', 'prominent': 'JJ', 'South African': 'ADJP', 'South': 'JJ', 'African': 'JJ', 'family': 'NN', 'distilled brandy and were Huguenots': 'VP', 'distilled brandy': 'VP', 'distilled': 'VBD', 'brandy': 'NN', 'and': 'CC', 'were Huguenots': 'VP', 'were': 'VBD', 'Huguenots': 'NNS', '?': '.'}\n",
      "~~~~ ['south'] -> ('South', 'JJ')\n",
      "~~~~ ['di', '##sti'] -> ('distilled', 'VBD')\n",
      "~~~~ ['brandy'] -> ('brandy', 'NN')\n",
      "~~~~ ['hug', '##uen', '##ots'] -> ('Huguenots', 'NNS')\n",
      "TOKENS [('[WHNP]', 'What prominent South African family'), ('[VBD]', 'distilled'), ('[NP]', 'brandy'), ('[NP]', 'Huguenots')]\n",
      "####################\n",
      "QUESTION How are the votes weighted to ensure the smaller states are dominated by larger ones?\n",
      "MATCHED WORDS [['votes'], ['weighted'], ['smaller'], ['states', 'are'], ['dominated', 'by', 'larger']]\n",
      "SPANS FOUND {'How are the votes weighted to ensure the smaller states are dominated by larger ones ?': 'SBARQ', 'How': 'WRB', 'are the votes weighted to ensure the smaller states are dominated by larger ones': 'SQ', 'are': 'VBP', 'the votes': 'NP', 'the': 'DT', 'votes': 'NNS', 'weighted to ensure the smaller states are dominated by larger ones': 'VP', 'weighted': 'VBN', 'to ensure the smaller states are dominated by larger ones': 'VP', 'to': 'TO', 'ensure the smaller states are dominated by larger ones': 'VP', 'ensure': 'VB', 'the smaller states are dominated by larger ones': 'S', 'the smaller states': 'NP', 'smaller': 'JJR', 'states': 'NNS', 'are dominated by larger ones': 'VP', 'dominated by larger ones': 'VP', 'dominated': 'VBN', 'by larger ones': 'PP', 'by': 'IN', 'larger ones': 'NP', 'larger': 'JJR', 'ones': 'NNS', '?': '.'}\n",
      "~~~~ ['votes'] -> ('votes', 'NNS')\n",
      "~~~~ ['weighted'] -> ('weighted', 'VBN')\n",
      "~~~~ ['smaller'] -> ('smaller', 'JJR')\n",
      "~~~~ ['states', 'are'] -> ('the smaller states are dominated by larger ones', 'S')\n",
      "~~~~ ['dominated', 'by', 'larger'] -> ('dominated by larger ones', 'VP')\n",
      "TOKENS [('[WHADVP]', 'How'), ('[NP]', 'votes'), ('[VBN]', 'weighted'), ('[VB]', 'ensure'), ('[NP]', 'smaller states'), ('[VBN]', 'dominated'), ('[NP]', 'larger ones')]\n",
      "####################\n",
      "QUESTION What is the metric they use to determine how busy airports are?\n",
      "MATCHED WORDS [['airports']]\n",
      "SPANS FOUND {'What is the metric they use to determine how busy airports are ?': 'SBARQ', 'What': 'WP', 'is the metric they use to determine how busy airports are': 'SQ', 'is': 'VBZ', 'the metric they use to determine how busy airports are': 'NP', 'the metric': 'NP', 'the': 'DT', 'metric': 'JJ', 'they use to determine how busy airports are': 'S', 'they': 'PRP', 'use to determine how busy airports are': 'VP', 'use': 'VBP', 'to determine how busy airports are': 'VP', 'to': 'TO', 'determine how busy airports are': 'VP', 'determine': 'VB', 'how busy airports are': 'SBAR', 'how busy': 'WHADVP', 'how': 'WRB', 'busy': 'JJ', 'airports are': 'S', 'airports': 'NNS', 'are': 'VBP', '?': '.'}\n",
      "~~~~ ['airports'] -> ('airports', 'NNS')\n",
      "TOKENS [('[WHNP]', 'What'), ('[NP]', 'metric'), ('[NP]', 'they'), ('[VBP]', 'use'), ('[VB]', 'determine'), ('[WHADVP]', 'how busy'), ('[NP]', 'airports')]\n",
      "####################\n",
      "QUESTION In the process of vaccination, what is introduced in order to develop a specific immunity?\n",
      "MATCHED WORDS [['va', '##cci', '##nation'], ['in', 'order'], ['develop'], ['specific', 'immunity']]\n",
      "SPANS FOUND {'In the process of vaccination , what is introduced in order to develop a specific immunity ?': 'SQ', 'In the process of vaccination': 'PP', 'In': 'IN', 'the process of vaccination': 'NP', 'the process': 'NP', 'the': 'DT', 'process': 'NN', 'of vaccination': 'PP', 'of': 'IN', 'vaccination': 'NN', ',': ',', 'what': 'WP', 'is introduced in order to develop a specific immunity': 'SQ', 'is': 'VBZ', 'introduced in order to develop a specific immunity': 'VP', 'introduced': 'VBN', 'in order to develop a specific immunity': 'SBAR', 'in order': 'PP', 'in': 'IN', 'order': 'NN', 'to develop a specific immunity': 'VP', 'to': 'TO', 'develop a specific immunity': 'VP', 'develop': 'VB', 'a specific immunity': 'NP', 'a': 'DT', 'specific': 'JJ', 'immunity': 'NN', '?': '.'}\n",
      "~~~~ ['va', '##cci', '##nation'] -> ('vaccination', 'NN')\n",
      "~~~~ ['in', 'order'] -> ('in order', 'PP')\n",
      "~~~~ ['develop'] -> ('develop', 'VB')\n",
      "~~~~ ['specific', 'immunity'] -> ('a specific immunity', 'NP')\n",
      "TOKENS [('[NP]', 'process'), ('[NP]', 'vaccination'), ('[WHNP]', 'what'), ('[VBN]', 'introduced'), ('[VB]', 'develop'), ('[NP]', 'specific immunity')]\n",
      "####################\n",
      "QUESTION When were the talks held for braodcast right to the Primier league for a five year period from the 1992 season?\n",
      "MATCHED WORDS [['talks'], ['held', 'for'], ['league', 'for', 'a', 'five'], ['year', 'period'], ['from', 'the', '1992', 'season']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPANS FOUND {'When were the talks held for braodcast right to the Primier league for a five year period from the 1992 season ?': 'SBARQ', 'When': 'WRB', 'were the talks held for braodcast right to the Primier league for a five year period from the 1992 season': 'SQ', 'were': 'VBD', 'the talks': 'NP', 'the': 'DT', 'talks': 'NNS', 'held for braodcast right to the Primier league for a five year period from the 1992 season': 'VP', 'held': 'VBN', 'for braodcast right to the Primier league': 'PP', 'for': 'IN', 'braodcast right': 'NP', 'braodcast': 'NN', 'right': 'NN', 'to the Primier league': 'PP', 'to': 'IN', 'the Primier league': 'NP', 'Primier': 'NNP', 'league': 'NN', 'for a five year period from the 1992 season': 'PP', 'a five year period from the 1992 season': 'NP', 'a five year period': 'NP', 'a': 'DT', 'five': 'CD', 'year': 'NN', 'period': 'NN', 'from the 1992 season': 'PP', 'from': 'IN', 'the 1992 season': 'NP', '1992': 'CD', 'season': 'NN', '?': '.'}\n",
      "~~~~ ['talks'] -> ('talks', 'NNS')\n",
      "~~~~ ['held', 'for'] -> ('held for braodcast right to the Primier league for a five year period from the 1992 season', 'VP')\n",
      "~~~~ ['league', 'for', 'a', 'five'] -> ('held for braodcast right to the Primier league for a five year period from the 1992 season', 'VP')\n",
      "~~~~ ['year', 'period'] -> ('a five year period', 'NP')\n",
      "~~~~ ['from', 'the', '1992', 'season'] -> ('from the 1992 season', 'PP')\n",
      "TOKENS [('[WHADVP]', 'When'), ('[NP]', 'talks'), ('[VBN]', 'held'), ('[NP]', 'braodcast right'), ('[NP]', 'Primier league'), ('[NP]', 'five year period'), ('[NP]', '1992 season')]\n",
      "####################\n",
      "QUESTION Who were the main detractors of the humoral theory of immunity?\n",
      "MATCHED WORDS [['humor', '##al', 'theory', 'of', 'immunity']]\n",
      "SPANS FOUND {'Who were the main detractors of the humoral theory of immunity ?': 'SBARQ', 'Who': 'WP', 'were the main detractors of the humoral theory of immunity': 'VP', 'were': 'VBD', 'the main detractors of the humoral theory of immunity': 'NP', 'the main detractors': 'NP', 'the': 'DT', 'main': 'JJ', 'detractors': 'NNS', 'of the humoral theory of immunity': 'PP', 'of': 'IN', 'the humoral theory of immunity': 'NP', 'the humoral theory': 'NP', 'humoral': 'JJ', 'theory': 'NN', 'of immunity': 'PP', 'immunity': 'NN', '?': '.'}\n",
      "~~~~ ['humor', '##al', 'theory', 'of', 'immunity'] -> ('the humoral theory of immunity', 'NP')\n",
      "TOKENS [('[WHNP]', 'Who'), ('[NP]', 'main detractors'), ('[NP]', 'humoral theory'), ('[NP]', 'immunity')]\n",
      "####################\n",
      "QUESTION What organization argued that drought, among other effects, could cause the Amazon forest to reach a \"tipping point?\"\n",
      "MATCHED WORDS [['drought'], ['effects'], ['amazon'], ['forest'], ['tipping', 'point']]\n",
      "SPANS FOUND {'What organization argued that drought , among other effects , could cause the Amazon forest to reach a \" tipping point ? \"': 'SBARQ', 'What organization': 'WHNP', 'What': 'WDT', 'organization': 'NN', 'argued': 'VBD', 'that': 'IN', 'drought , among other effects , could cause the Amazon forest to reach a \" tipping point ? \"': 'S', 'drought , among other effects , could cause the Amazon forest to reach a \" tipping point': 'S', 'drought': 'NN', ',': ',', 'among other effects': 'PP', 'among': 'IN', 'other effects': 'NP', 'other': 'JJ', 'effects': 'NNS', 'could cause the Amazon forest to reach a \" tipping point': 'VP', 'could': 'MD', 'cause the Amazon forest to reach a \" tipping point': 'VP', 'cause': 'VB', 'the Amazon forest to reach a \" tipping point': 'S', 'the Amazon forest': 'NP', 'the': 'DT', 'Amazon': 'NNP', 'forest': 'NN', 'to reach a \" tipping point': 'VP', 'to': 'TO', 'reach a \" tipping point': 'VP', 'reach': 'VB', 'a \" tipping point': 'NP', 'a': 'DT', '\"': \"''\", 'tipping': 'NN', 'point': 'NN', '?': '.'}\n",
      "~~~~ ['drought'] -> ('drought', 'NN')\n",
      "~~~~ ['effects'] -> ('effects', 'NNS')\n",
      "~~~~ ['amazon'] -> ('Amazon', 'NNP')\n",
      "~~~~ ['forest'] -> ('forest', 'NN')\n",
      "~~~~ ['tipping', 'point'] -> ('a \" tipping point', 'NP')\n",
      "TOKENS [('[WHNP]', 'What organization'), ('[VBD]', 'argued'), ('[NP]', 'drought'), ('[NP]', 'other effects'), ('[VB]', 'cause'), ('[NP]', 'Amazon forest'), ('[VB]', 'reach'), ('[NP]', '\" tipping point')]\n",
      "####################\n",
      "QUESTION When did Microsoft decide to appeal the ruling?\n",
      "MATCHED WORDS [['microsoft'], ['appeal', 'the', 'ruling']]\n",
      "SPANS FOUND {'When did Microsoft decide to appeal the ruling ?': 'SBARQ', 'When': 'WRB', 'did Microsoft decide to appeal the ruling': 'SQ', 'did': 'VBD', 'Microsoft': 'NNP', 'decide to appeal the ruling': 'VP', 'decide': 'VB', 'to appeal the ruling': 'VP', 'to': 'TO', 'appeal the ruling': 'VP', 'appeal': 'VB', 'the ruling': 'NP', 'the': 'DT', 'ruling': 'NN', '?': '.'}\n",
      "~~~~ ['microsoft'] -> ('Microsoft', 'NNP')\n",
      "~~~~ ['appeal', 'the', 'ruling'] -> ('appeal the ruling', 'VP')\n",
      "TOKENS [('[WHADVP]', 'When'), ('[NP]', 'Microsoft'), ('[VB]', 'decide'), ('[VB]', 'appeal'), ('[NP]', 'ruling')]\n",
      "####################\n",
      "QUESTION Whose army liberated Duchy in 1806?\n",
      "MATCHED WORDS [['army'], ['liberated'], ['duchy'], ['in', '1806']]\n",
      "SPANS FOUND {'Whose army liberated Duchy in 1806 ?': 'SBARQ', 'Whose army': 'WHNP', 'Whose': 'WP$', 'army': 'NN', 'liberated Duchy in 1806': 'VP', 'liberated': 'VBD', 'Duchy': 'NNP', 'in 1806': 'PP', 'in': 'IN', '1806': 'CD', '?': '.'}\n",
      "~~~~ ['army'] -> ('army', 'NN')\n",
      "~~~~ ['liberated'] -> ('liberated', 'VBD')\n",
      "~~~~ ['duchy'] -> ('Duchy', 'NNP')\n",
      "~~~~ ['in', '1806'] -> ('in 1806', 'PP')\n",
      "TOKENS [('[WHNP]', 'Whose army'), ('[VBD]', 'liberated'), ('[NP]', 'Duchy'), ('[NP]', '1806')]\n",
      "####################\n",
      "QUESTION Extensions of what drew Taurus to Jacksonville?\n",
      "MATCHED WORDS [['drew'], ['jacksonville']]\n",
      "SPANS FOUND {'Extensions of what drew Taurus to Jacksonville ?': 'NP', 'Extensions': 'NNS', 'of': 'IN', 'what drew Taurus to Jacksonville': 'SBAR', 'what': 'WP', 'drew Taurus to Jacksonville': 'VP', 'drew': 'VBD', 'Taurus': 'NNP', 'to Jacksonville': 'PP', 'to': 'IN', 'Jacksonville': 'NNP', '?': '.'}\n",
      "~~~~ ['drew'] -> ('drew', 'VBD')\n",
      "~~~~ ['jacksonville'] -> ('Jacksonville', 'NNP')\n",
      "TOKENS [('[NP]', 'Extensions'), ('[WHNP]', 'what'), ('[VBD]', 'drew'), ('[NP]', 'Taurus'), ('[NP]', 'Jacksonville')]\n",
      "####################\n",
      "QUESTION What do encrypted broadcasts never require?\n",
      "MATCHED WORDS [['en', '##cr', '##yp', '##ted'], ['broadcasts'], ['require']]\n",
      "SPANS FOUND {'What do encrypted broadcasts never require ?': 'SBARQ', 'What': 'WP', 'do encrypted broadcasts never require': 'SQ', 'do': 'VBP', 'encrypted broadcasts': 'NP', 'encrypted': 'VBN', 'broadcasts': 'NNS', 'never': 'RB', 'require': 'VBP', '?': '.'}\n",
      "~~~~ ['en', '##cr', '##yp', '##ted'] -> ('encrypted', 'VBN')\n",
      "~~~~ ['broadcasts'] -> ('broadcasts', 'NNS')\n",
      "~~~~ ['require'] -> ('require', 'VBP')\n",
      "TOKENS [('[WHNP]', 'What'), ('[NP]', 'encrypted broadcasts'), ('[VBP]', 'require')]\n",
      "####################\n",
      "QUESTION What was the incident over taxes at Ballarat called?\n",
      "MATCHED WORDS [['taxes'], ['at', 'ball', '##arat']]\n",
      "SPANS FOUND {'What was the incident over taxes at Ballarat called ?': 'SBARQ', 'What': 'WP', 'was the incident over taxes at Ballarat called': 'SQ', 'was': 'VBD', 'the incident over taxes at Ballarat': 'NP', 'the incident': 'NP', 'the': 'DT', 'incident': 'NN', 'over taxes at Ballarat': 'PP', 'over': 'IN', 'taxes': 'NNS', 'at Ballarat': 'PP', 'at': 'IN', 'Ballarat': 'NNP', 'called': 'VBD', '?': '.'}\n",
      "~~~~ ['taxes'] -> ('taxes', 'NNS')\n",
      "~~~~ ['at', 'ball', '##arat'] -> ('at Ballarat', 'PP')\n",
      "TOKENS [('[WHNP]', 'What'), ('[NP]', 'incident'), ('[NP]', 'taxes'), ('[NP]', 'Ballarat'), ('[VBD]', 'called')]\n",
      "####################\n",
      "QUESTION What did DECnet Phase I become?\n",
      "MATCHED WORDS [['dec', '##net', 'phase']]\n",
      "SPANS FOUND {'What did DECnet Phase I become ?': 'SBARQ', 'What': 'WP', 'did DECnet Phase I become': 'SQ', 'did': 'VBD', 'DECnet Phase': 'NP', 'DECnet': 'NNP', 'Phase': 'NNP', 'I': 'PRP', 'become': 'VBP', '?': '.'}\n",
      "~~~~ ['dec', '##net', 'phase'] -> ('DECnet Phase', 'NP')\n",
      "TOKENS [('[WHNP]', 'What'), ('[NP]', 'DECnet Phase'), ('[NP]', 'I')]\n",
      "####################\n",
      "QUESTION What is held inside the formal legal ownership registration system in many developing countries?\n",
      "MATCHED WORDS [['held'], ['formal'], ['legal', 'ownership'], ['registration', 'system'], ['developing', 'countries']]\n",
      "SPANS FOUND {'What is held inside the formal legal ownership registration system in many developing countries ?': 'SBARQ', 'What': 'WP', 'is held inside the formal legal ownership registration system in many developing countries': 'SQ', 'is': 'VBZ', 'held inside the formal legal ownership registration system in many developing countries': 'VP', 'held': 'VBN', 'inside the formal legal ownership registration system in many developing countries': 'PP', 'inside': 'IN', 'the formal legal ownership registration system in many developing countries': 'NP', 'the formal legal ownership registration system': 'NP', 'the': 'DT', 'formal': 'JJ', 'legal': 'JJ', 'ownership': 'NN', 'registration': 'NN', 'system': 'NN', 'in many developing countries': 'PP', 'in': 'IN', 'many developing countries': 'NP', 'many': 'JJ', 'developing': 'VBG', 'countries': 'NNS', '?': '.'}\n",
      "~~~~ ['held'] -> ('held', 'VBN')\n",
      "~~~~ ['formal'] -> ('formal', 'JJ')\n",
      "~~~~ ['legal', 'ownership'] -> ('the formal legal ownership registration system', 'NP')\n",
      "~~~~ ['registration', 'system'] -> ('the formal legal ownership registration system', 'NP')\n",
      "~~~~ ['developing', 'countries'] -> ('many developing countries', 'NP')\n",
      "TOKENS [('[WHNP]', 'What'), ('[VBN]', 'held'), ('[NP]', 'formal legal ownership registration system'), ('[NP]', 'many developing countries')]\n",
      "####################\n",
      "QUESTION What new product did Bank of America introduce in 1958?\n",
      "MATCHED WORDS [['new', 'product'], ['bank', 'of', 'america'], ['1958']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPANS FOUND {'What new product did Bank of America introduce in 1958 ?': 'SBARQ', 'What new product': 'WHNP', 'What': 'WDT', 'new': 'JJ', 'product': 'NN', 'did Bank of America introduce in 1958': 'SQ', 'did': 'VBD', 'Bank of America': 'NP', 'Bank': 'NNP', 'of America': 'PP', 'of': 'IN', 'America': 'NNP', 'introduce in 1958': 'VP', 'introduce': 'VBP', 'in 1958': 'PP', 'in': 'IN', '1958': 'CD', '?': '.'}\n",
      "~~~~ ['new', 'product'] -> ('What new product', 'WHNP')\n",
      "~~~~ ['bank', 'of', 'america'] -> ('Bank of America', 'NP')\n",
      "~~~~ ['1958'] -> ('1958', 'CD')\n",
      "TOKENS [('[WHNP]', 'What new product'), ('[NP]', 'Bank'), ('[NP]', 'America'), ('[VBP]', 'introduce'), ('[NP]', '1958')]\n",
      "####################\n",
      "QUESTION What's the name of where the Rhine branches off near Dordrecht?\n",
      "MATCHED WORDS [['rhine'], ['branches', 'off'], ['near', 'do', '##rd', '##recht']]\n",
      "SPANS FOUND {\"What 's the name of where the Rhine branches off near Dordrecht ?\": 'SBARQ', 'What': 'WP', \"'s the name of where the Rhine branches off near Dordrecht\": 'VP', \"'s\": 'VBZ', 'the name of where the Rhine branches off near Dordrecht': 'NP', 'the name': 'NP', 'the': 'DT', 'name': 'NN', 'of where the Rhine branches off near Dordrecht': 'PP', 'of': 'IN', 'where the Rhine branches off near Dordrecht': 'SBAR', 'where': 'WRB', 'the Rhine branches off near Dordrecht': 'S', 'the Rhine': 'NP', 'Rhine': 'NNP', 'branches off near Dordrecht': 'VP', 'branches': 'VBZ', 'off': 'RP', 'near Dordrecht': 'PP', 'near': 'IN', 'Dordrecht': 'NNP', '?': '.'}\n",
      "~~~~ ['rhine'] -> ('Rhine', 'NNP')\n",
      "~~~~ ['branches', 'off'] -> ('branches off near Dordrecht', 'VP')\n",
      "~~~~ ['near', 'do', '##rd', '##recht'] -> ('near Dordrecht', 'PP')\n",
      "TOKENS [('[WHNP]', 'What'), ('[VBZ]', \"'s\"), ('[NP]', 'name'), ('[WHADVP]', 'where'), ('[NP]', 'Rhine'), ('[VBZ]', 'branches'), ('[NP]', 'Dordrecht')]\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "def x_in_y(query, base):\n",
    "    try:\n",
    "        l = len(query)\n",
    "    except TypeError:\n",
    "        l = 1\n",
    "        query = type(base)((query,))\n",
    "\n",
    "    for i in range(len(base)):\n",
    "        if base[i:i+l] == query:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "for i in random.sample(range(len(examples)), 20):\n",
    "    q = questions[examples[i][1]]\n",
    "    c = contexts[examples[i][0]]\n",
    "    print(\"QUESTION\",q)#print(c, \"\\n\", q)\n",
    "    matched_terms = matcher(tokenizer.tokenize(q),tokenizer.tokenize(c))\n",
    "    print(\"MATCHED WORDS\", matched_terms)\n",
    "    res = predictor.predict(sentence=q)\n",
    "    tokens = []\n",
    "    get_q_parts(res['hierplane_tree']['root'], nlp, tokens)\n",
    "    spans = {}\n",
    "    gather_spans(res['hierplane_tree']['root'], spans)\n",
    "    print(\"SPANS FOUND\", spans)\n",
    "    for s in matched_terms:\n",
    "        min_span = (None,None)\n",
    "        for cand in spans.keys():\n",
    "            if x_in_y(s, tokenizer.tokenize(cand)) and ( min_span[0] is None or len(cand) < len(min_span[0]) ):\n",
    "                min_span = (cand, spans[cand])\n",
    "        print(\"~~~~\",s, \"->\", min_span)\n",
    "    print(\"TOKENS\", tokens)\n",
    "    print(\"#\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'What measurement do scientists used to determine the quality of water ?': 'SBARQ', 'What measurement': 'WHNP', 'What': 'WDT', 'measurement': 'NN', 'do scientists used to determine the quality of water': 'SQ', 'do': 'VBP', 'scientists': 'NNS', 'used to determine the quality of water': 'VP', 'used': 'VBN', 'to determine the quality of water': 'VP', 'to': 'TO', 'determine the quality of water': 'VP', 'determine': 'VB', 'the quality of water': 'NP', 'the quality': 'NP', 'the': 'DT', 'quality': 'NN', 'of water': 'PP', 'of': 'IN', 'water': 'NN', '?': '.'}\n"
     ]
    }
   ],
   "source": [
    "def gather_spans(o, out):\n",
    "    out[o['word']] = o['nodeType']\n",
    "    if 'children' in o:\n",
    "        for child in o['children']:\n",
    "            gather_spans(child, out)\n",
    "\n",
    "out = {}\n",
    "gather_spans(res['hierplane_tree']['root'], out)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'What measurement do scientists used to determine the quality of water ?',\n",
       " 'nodeType': 'SBARQ',\n",
       " 'attributes': ['SBARQ'],\n",
       " 'link': 'SBARQ',\n",
       " 'children': [{'word': 'What measurement',\n",
       "   'nodeType': 'WHNP',\n",
       "   'attributes': ['WHNP'],\n",
       "   'link': 'WHNP',\n",
       "   'children': [{'word': 'What',\n",
       "     'nodeType': 'WDT',\n",
       "     'attributes': ['WDT'],\n",
       "     'link': 'WDT'},\n",
       "    {'word': 'measurement',\n",
       "     'nodeType': 'NN',\n",
       "     'attributes': ['NN'],\n",
       "     'link': 'NN'}]},\n",
       "  {'word': 'do scientists used to determine the quality of water',\n",
       "   'nodeType': 'SQ',\n",
       "   'attributes': ['SQ'],\n",
       "   'link': 'SQ',\n",
       "   'children': [{'word': 'do',\n",
       "     'nodeType': 'VBP',\n",
       "     'attributes': ['VBP'],\n",
       "     'link': 'VBP'},\n",
       "    {'word': 'scientists',\n",
       "     'nodeType': 'NP',\n",
       "     'attributes': ['NP'],\n",
       "     'link': 'NP',\n",
       "     'children': [{'word': 'scientists',\n",
       "       'nodeType': 'NNS',\n",
       "       'attributes': ['NNS'],\n",
       "       'link': 'NNS'}]},\n",
       "    {'word': 'used to determine the quality of water',\n",
       "     'nodeType': 'VP',\n",
       "     'attributes': ['VP'],\n",
       "     'link': 'VP',\n",
       "     'children': [{'word': 'used',\n",
       "       'nodeType': 'VBN',\n",
       "       'attributes': ['VBN'],\n",
       "       'link': 'VBN'},\n",
       "      {'word': 'to determine the quality of water',\n",
       "       'nodeType': 'S',\n",
       "       'attributes': ['S'],\n",
       "       'link': 'S',\n",
       "       'children': [{'word': 'to determine the quality of water',\n",
       "         'nodeType': 'VP',\n",
       "         'attributes': ['VP'],\n",
       "         'link': 'VP',\n",
       "         'children': [{'word': 'to',\n",
       "           'nodeType': 'TO',\n",
       "           'attributes': ['TO'],\n",
       "           'link': 'TO'},\n",
       "          {'word': 'determine the quality of water',\n",
       "           'nodeType': 'VP',\n",
       "           'attributes': ['VP'],\n",
       "           'link': 'VP',\n",
       "           'children': [{'word': 'determine',\n",
       "             'nodeType': 'VB',\n",
       "             'attributes': ['VB'],\n",
       "             'link': 'VB'},\n",
       "            {'word': 'the quality of water',\n",
       "             'nodeType': 'NP',\n",
       "             'attributes': ['NP'],\n",
       "             'link': 'NP',\n",
       "             'children': [{'word': 'the quality',\n",
       "               'nodeType': 'NP',\n",
       "               'attributes': ['NP'],\n",
       "               'link': 'NP',\n",
       "               'children': [{'word': 'the',\n",
       "                 'nodeType': 'DT',\n",
       "                 'attributes': ['DT'],\n",
       "                 'link': 'DT'},\n",
       "                {'word': 'quality',\n",
       "                 'nodeType': 'NN',\n",
       "                 'attributes': ['NN'],\n",
       "                 'link': 'NN'}]},\n",
       "              {'word': 'of water',\n",
       "               'nodeType': 'PP',\n",
       "               'attributes': ['PP'],\n",
       "               'link': 'PP',\n",
       "               'children': [{'word': 'of',\n",
       "                 'nodeType': 'IN',\n",
       "                 'attributes': ['IN'],\n",
       "                 'link': 'IN'},\n",
       "                {'word': 'water',\n",
       "                 'nodeType': 'NP',\n",
       "                 'attributes': ['NP'],\n",
       "                 'link': 'NP',\n",
       "                 'children': [{'word': 'water',\n",
       "                   'nodeType': 'NN',\n",
       "                   'attributes': ['NN'],\n",
       "                   'link': 'NN'}]}]}]}]}]}]}]}]},\n",
       "  {'word': '?', 'nodeType': '.', 'attributes': ['.'], 'link': '.'}]}"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['hierplane_tree']['root']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['must', 'be', 'older', 'than'], ['inclusion', '##s']]\n"
     ]
    }
   ],
   "source": [
    " \n",
    "#[['of', 'victorian', '##s'], ['christian']]\n",
    "\n",
    "q =\"What says that formations must be older than the inclusions inside them?\"\n",
    "c = \"The principle of inclusions and components states that, with sedimentary rocks, if inclusions (or clasts) are found in a formation, then the inclusions must be older than the formation that contains them. For example, in sedimentary rocks, it is common for gravel from an older formation to be ripped up and included in a newer layer. A similar situation with igneous rocks occurs when xenoliths are found. These foreign bodies are picked up as magma or lava flows, and are incorporated, later to cool in the matrix. As a result, xenoliths are older than the rock which contains them.\"\n",
    "print(matcher(tokenizer.tokenize(q),tokenizer.tokenize(c)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = predictor.predict(sentence=\"How much did Silas B. Cobb pledge to the university?\")\n",
    "tokens = []\n",
    "get_q_parts(res['hierplane_tree']['root'], nlp, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[WHNP]', 'How much'), ('[NP]', 'Silas B. Cobb'), ('[NP]', 'university')]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'How much did Silas B. Cobb pledge to the university ?',\n",
       " 'nodeType': 'SBARQ',\n",
       " 'attributes': ['SBARQ'],\n",
       " 'link': 'SBARQ',\n",
       " 'children': [{'word': 'How much',\n",
       "   'nodeType': 'WHNP',\n",
       "   'attributes': ['WHNP'],\n",
       "   'link': 'WHNP',\n",
       "   'children': [{'word': 'How',\n",
       "     'nodeType': 'WRB',\n",
       "     'attributes': ['WRB'],\n",
       "     'link': 'WRB'},\n",
       "    {'word': 'much', 'nodeType': 'JJ', 'attributes': ['JJ'], 'link': 'JJ'}]},\n",
       "  {'word': 'did Silas B. Cobb pledge to the university',\n",
       "   'nodeType': 'SQ',\n",
       "   'attributes': ['SQ'],\n",
       "   'link': 'SQ',\n",
       "   'children': [{'word': 'did',\n",
       "     'nodeType': 'VBD',\n",
       "     'attributes': ['VBD'],\n",
       "     'link': 'VBD'},\n",
       "    {'word': 'Silas B. Cobb',\n",
       "     'nodeType': 'NP',\n",
       "     'attributes': ['NP'],\n",
       "     'link': 'NP',\n",
       "     'children': [{'word': 'Silas',\n",
       "       'nodeType': 'NNP',\n",
       "       'attributes': ['NNP'],\n",
       "       'link': 'NNP'},\n",
       "      {'word': 'B.', 'nodeType': 'NNP', 'attributes': ['NNP'], 'link': 'NNP'},\n",
       "      {'word': 'Cobb',\n",
       "       'nodeType': 'NNP',\n",
       "       'attributes': ['NNP'],\n",
       "       'link': 'NNP'}]},\n",
       "    {'word': 'pledge to the university',\n",
       "     'nodeType': 'VP',\n",
       "     'attributes': ['VP'],\n",
       "     'link': 'VP',\n",
       "     'children': [{'word': 'pledge',\n",
       "       'nodeType': 'NN',\n",
       "       'attributes': ['NN'],\n",
       "       'link': 'NN'},\n",
       "      {'word': 'to the university',\n",
       "       'nodeType': 'PP',\n",
       "       'attributes': ['PP'],\n",
       "       'link': 'PP',\n",
       "       'children': [{'word': 'to',\n",
       "         'nodeType': 'IN',\n",
       "         'attributes': ['IN'],\n",
       "         'link': 'IN'},\n",
       "        {'word': 'the university',\n",
       "         'nodeType': 'NP',\n",
       "         'attributes': ['NP'],\n",
       "         'link': 'NP',\n",
       "         'children': [{'word': 'the',\n",
       "           'nodeType': 'DT',\n",
       "           'attributes': ['DT'],\n",
       "           'link': 'DT'},\n",
       "          {'word': 'university',\n",
       "           'nodeType': 'NN',\n",
       "           'attributes': ['NN'],\n",
       "           'link': 'NN'}]}]}]}]},\n",
       "  {'word': '?', 'nodeType': '.', 'attributes': ['.'], 'link': '.'}]}"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['hierplane_tree']['root']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7370\n"
     ]
    }
   ],
   "source": [
    "for i, q in enumerate(questions):\n",
    "    if \"What is the name of the private day school for K-12 students the university runs?\" in q:\n",
    "         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7370\n",
      "732\n"
     ]
    }
   ],
   "source": [
    "for i, e in enumerate(examples):\n",
    "    \n",
    "    if e[1] == 7370:\n",
    "        print(i)\n",
    "        print(e[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The university runs a number of academic institutions and programs apart from its undergraduate and postgraduate schools. It operates the University of Chicago Laboratory Schools (a private day school for K-12 students and day care), the Sonia Shankman Orthogenic School (a residential treatment program for those with behavioral and emotional problems), and four public charter schools on the South Side of Chicago administered by the university's Urban Education Institute. In addition, the Hyde Park Day School, a school for students with learning disabilities, maintains a location on the University of Chicago campus. Since 1983, the University of Chicago has maintained the University of Chicago School Mathematics Project, a mathematics program used in urban primary and secondary schools. The university runs a program called the Council on Advanced Studies in the Social Sciences and Humanities, which administers interdisciplinary workshops to provide a forum for graduate students, faculty, and visiting scholars to present scholarly work in progress. The university also operates the University of Chicago Press, the largest university press in the United States.\n"
     ]
    }
   ],
   "source": [
    "print(contexts[732])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This motivates the concept of a problem being hard for a complexity class. A problem X is hard for a class of problems C if every problem in C can be reduced to X. Thus no problem in C is harder than X, since an algorithm for X allows us to solve any problem in C. Of course, the notion of hard problems depends on the type of reduction being used. For complexity classes larger than P, polynomial-time reductions are commonly used. In particular, the set of problems that are hard for NP is the set of NP-hard problems.\n",
      "If a problem X is in C and hard for C, then X is said to be complete for C. This means that X is the hardest problem in C. (Since many problems could be equally hard, one might say that X is one of the hardest problems in C.) Thus the class of NP-complete problems contains the most difficult problems in NP, in the sense that they are the ones most likely not to be in P. Because the problem P = NP is not solved, being able to reduce a known NP-complete problem, Π2, to another problem, Π1, would indicate that there is no known polynomial-time solution for Π1. This is because a polynomial-time solution to Π1 would yield a polynomial-time solution to Π2. Similarly, because all NP problems can be reduced to the set, finding an NP-complete problem that can be solved in polynomial time would mean that P = NP.\n",
      "The complexity class P is often seen as a mathematical abstraction modeling those computational tasks that admit an efficient algorithm. This hypothesis is called the Cobham–Edmonds thesis. The complexity class NP, on the other hand, contains many problems that people would like to solve efficiently, but for which no efficient algorithm is known, such as the Boolean satisfiability problem, the Hamiltonian path problem and the vertex cover problem. Since deterministic Turing machines are special non-deterministic Turing machines, it is easily observed that each problem in P is also member of the class NP.\n",
      "The question of whether P equals NP is one of the most important open questions in theoretical computer science because of the wide implications of a solution. If the answer is yes, many important problems can be shown to have more efficient solutions. These include various types of integer programming problems in operations research, many problems in logistics, protein structure prediction in biology, and the ability to find formal proofs of pure mathematics theorems. The P versus NP problem is one of the Millennium Prize Problems proposed by the Clay Mathematics Institute. There is a US$1,000,000 prize for resolving the problem.\n",
      "It was shown by Ladner that if P ≠ NP then there exist problems in NP that are neither in P nor NP-complete. Such problems are called NP-intermediate problems. The graph isomorphism problem, the discrete logarithm problem and the integer factorization problem are examples of problems believed to be NP-intermediate. They are some of the very few NP problems not known to be in P or to be NP-complete.\n",
      "The graph isomorphism problem is the computational problem of determining whether two finite graphs are isomorphic. An important unsolved problem in complexity theory is whether the graph isomorphism problem is in P, NP-complete, or NP-intermediate. The answer is not known, but it is believed that the problem is at least not NP-complete. If graph isomorphism is NP-complete, the polynomial time hierarchy collapses to its second level. Since it is widely believed that the polynomial hierarchy does not collapse to any finite level, it is believed that graph isomorphism is not NP-complete. The best algorithm for this problem, due to Laszlo Babai and Eugene Luks has run time 2O(√(n log(n))) for graphs with n vertices.\n",
      "The integer factorization problem is the computational problem of determining the prime factorization of a given integer. Phrased as a decision problem, it is the problem of deciding whether the input has a factor less than k. No efficient integer factorization algorithm is known, and this fact forms the basis of several modern cryptographic systems, such as the RSA algorithm. The integer factorization problem is in NP and in co-NP (and even in UP and co-UP). If the problem is NP-complete, the polynomial time hierarchy will collapse to its first level (i.e., NP will equal co-NP). The best known algorithm for integer factorization is the general number field sieve, which takes time O(e(64/9)1/3(n.log 2)1/3(log (n.log 2))2/3) to factor an n-bit integer. However, the best known quantum algorithm for this problem, Shor's algorithm, does run in polynomial time. Unfortunately, this fact doesn't say much about where the problem lies with respect to non-quantum complexity classes.\n",
      "Many known complexity classes are suspected to be unequal, but this has not been proved. For instance P ⊆ NP ⊆ PP ⊆ PSPACE, but it is possible that P = PSPACE. If P is not equal to NP, then P is not equal to PSPACE either. Since there are many known complexity classes between P and PSPACE, such as RP, BPP, PP, BQP, MA, PH, etc., it is possible that all these complexity classes collapse to one class. Proving that any of these classes are unequal would be a major breakthrough in complexity theory.\n",
      "Along the same lines, co-NP is the class containing the complement problems (i.e. problems with the yes/no answers reversed) of NP problems. It is believed that NP is not equal to co-NP; however, it has not yet been proven. It has been shown that if these two complexity classes are not equal then P is not equal to NP.\n",
      "Problems that can be solved in theory (e.g., given large but finite time), but which in practice take too long for their solutions to be useful, are known as intractable problems. In complexity theory, problems that lack polynomial-time solutions are considered to be intractable for more than the smallest inputs. In fact, the Cobham–Edmonds thesis states that only those problems that can be solved in polynomial time can be feasibly computed on some computational device. Problems that are known to be intractable in this sense include those that are EXPTIME-hard. If NP is not the same as P, then the NP-complete problems are also intractable in this sense. To see why exponential-time algorithms might be unusable in practice, consider a program that makes 2n operations before halting. For small n, say 100, and assuming for the sake of example that the computer does 1012 operations each second, the program would run for about 4 × 1010 years, which is the same order of magnitude as the age of the universe. Even with a much faster computer, the program would only be useful for very small instances and in that sense the intractability of a problem is somewhat independent of technological progress. Nevertheless, a polynomial time algorithm is not always practical. If its running time is, say, n15, it is unreasonable to consider it efficient and it is still useless except on small instances.\n",
      "What intractability means in practice is open to debate. Saying that a problem is not in P does not imply that all large cases of the problem are hard or even that most of them are. For example, the decision problem in Presburger arithmetic has been shown not to be in P, yet algorithms have been written that solve the problem in reasonable times in most cases. Similarly, algorithms can solve the NP-complete knapsack problem over a wide range of sizes in less than quadratic time and SAT solvers routinely handle large instances of the NP-complete Boolean satisfiability problem.\n",
      "In 1967, Manuel Blum developed an axiomatic complexity theory based on his axioms and proved an important result, the so-called, speed-up theorem. The field really began to flourish in 1971 when the US researcher Stephen Cook and, working independently, Leonid Levin in the USSR, proved that there exist practically relevant problems that are NP-complete. In 1972, Richard Karp took this idea a leap forward with his landmark paper, \"Reducibility Among Combinatorial Problems\", in which he showed that 21 diverse combinatorial and graph theoretical problems, each infamous for its computational intractability, are NP-complete.\n",
      "Starting in the late 1950s, American computer scientist Paul Baran developed the concept Distributed Adaptive Message Block Switching with the goal to provide a fault-tolerant, efficient routing method for telecommunication messages as part of a research program at the RAND Corporation, funded by the US Department of Defense. This concept contrasted and contradicted the theretofore established principles of pre-allocation of network bandwidth, largely fortified by the development of telecommunications in the Bell System. The new concept found little resonance among network implementers until the independent work of Donald Davies at the National Physical Laboratory (United Kingdom) (NPL) in the late 1960s. Davies is credited with coining the modern name packet switching and inspiring numerous packet switching networks in Europe in the decade following, including the incorporation of the concept in the early ARPANET in the United States.\n",
      "During this time, the discovery of oil in the North Sea and the following \"It's Scotland's oil\" campaign of the Scottish National Party (SNP) resulted in rising support for Scottish independence, as well as the SNP. The party argued that the revenues from the oil were not benefitting Scotland as much as they should. The combined effect of these events led to Prime Minister Wilson committing his government to some form of devolved legislature in 1974. However, it was not until 1978 that final legislative proposals for a Scottish Assembly were passed by the United Kingdom Parliament.\n",
      "The election produced a majority SNP government, making this the first time in the Scottish Parliament where a party has commanded a parliamentary majority. The SNP took 16 seats from Labour, with many of their key figures not returned to parliament, although Labour leader Iain Gray retained East Lothian by 151 votes. The SNP took a further eight seats from the Liberal Democrats and one seat from the Conservatives. The SNP overall majority meant that there was sufficient support in the Scottish Parliament to hold a referendum on Scottish independence.\n",
      "For the Conservatives, the main disappointment was the loss of Edinburgh Pentlands, the seat of former party leader David McLetchie, to the SNP. McLetchie was elected on the Lothian regional list and the Conservatives suffered a net loss of five seats, with leader Annabel Goldie claiming that their support had held firm. Nevertheless, she too announced she would step down as leader of the party. Cameron congratulated the SNP on their victory but vowed to campaign for the Union in the independence referendum.\n"
     ]
    }
   ],
   "source": [
    "for c in contexts:\n",
    "    if \"NP\" in c:\n",
    "        print (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "x remove stop words (to, the)\n",
    "- random \"nots\" and \"don'ts\" that appear bewteen stuff\n",
    "    - What is the process that asks a more specific question about all possible algorithms that could not be used to solve the same problem?\n",
    "[['question', 'about', 'all', 'possible', 'algorithms', 'that', 'could'], ['be', 'used', 'to', 'solve', 'the', 'same', 'problem']]\n",
    "- maybe use LSTM for minor changes and limit in length (Vitamin c vs vitamin d)\n",
    "- many answerable questions use synonyms (\"isolated\" vs \"seperated\") but unanswerable less so, but probabilities of copy is always higher than paraphrase (input explicitly?????)\n",
    "- close match (What do gravitational forces act between?\n",
    "    [['gravitational'], ['forces'], ['act'], ['between']] -> \" gravitational force acts between masses\"\n",
    "    - practice vs practices\n",
    "- typoes (primier league vs primer)\n",
    "- overlap with q words (may not matter since we still use those)\n",
    "- Check how model handles this:\n",
    "    - The principle of inclusions and components states that, with sedimentary rocks, if inclusions (or clasts) are found in a formation, then the inclusions must be older than the formation that contains them. For example, in sedimentary rocks, it is common for gravel from an older formation to be ripped up and included in a newer layer. A similar situation with igneous rocks occurs when xenoliths are found. These foreign bodies are picked up as magma or lava flows, and are incorporated, later to cool in the matrix. As a result, xenoliths are older than the rock which contains them. \n",
    "     - What says that formations must be older than the inclusions inside them?  \n",
    "     \n",
    "x single endings ([\"'\"], ['t']) or ##s, ##ed, etc  \n",
    "x remove commas  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing and \"Parsing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_stop = [\"a\", \"an\", \"and\", \"as\", \"or\", \"the\", \"that\", \"which\", \"when\", \"whose\", \"is\", \"was\", \"what\", \"to\", \"in\", \"at\"]\n",
    "strip_stop = [\"a\", \"an\", \"and\", \"as\", \"or\", \"the\", \"that\", \"which\", \"when\", \"whose\", \"is\", \"was\", \"what\", \"to\", \"of\"]\n",
    "#my_stop = strip_stop + [\"to\", \"in\", \"at\", \"on\", \"under\"]\n",
    "\n",
    "\n",
    "def individual_filter(term):\n",
    "    if term.startswith(\"##\"):\n",
    "        return False\n",
    "    if term in nlp.vocab and  (nlp.vocab[term].is_stop or nlp.vocab[term].is_punct):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def simple_filter(term):\n",
    "    if term in strip_stop or (term in nlp.vocab and nlp.vocab[term].is_punct):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def strip_terms(phrase):\n",
    "    fltr = [simple_filter(t) for t in phrase]\n",
    "    start = fltr.index(True)\n",
    "    end = list(reversed(fltr)).index(True)\n",
    "    return phrase[start:len(fltr)-end]\n",
    "\n",
    "def matcher(source, target):\n",
    "    i = 0\n",
    "    matches = []\n",
    "    while i < len(source):\n",
    "        ii = 1\n",
    "        current = []\n",
    "        for j in range(len(target)):\n",
    "            if source[i] == target[j]:\n",
    "                cand = []\n",
    "                for ii in range(len(source) - i):\n",
    "                    if j+ii > len(target) - 1:\n",
    "                        break\n",
    "                    if source[i+ii] == target[j+ii]:\n",
    "                        cand.append(source[i+ii])\n",
    "                    else:\n",
    "                        if len(cand) > len(current):\n",
    "                            current = list(cand)\n",
    "                        break\n",
    "                if len(cand) > len(current):\n",
    "                    current = list(cand)\n",
    "        if current:\n",
    "            matches.append(current)\n",
    "        i += len(current) if current else 1\n",
    "        \n",
    "    ## filters\n",
    "    matches = [m for m in matches if sum(\n",
    "        [individual_filter(token) for token in m]) > 0]\n",
    "    \n",
    "    matches = [strip_terms(m) for m in matches]\n",
    "    return matches\n",
    "\n",
    "def gather_spans(o, out):\n",
    "    out[o['word']] = o['nodeType']\n",
    "    if 'children' in o:\n",
    "        for child in o['children']:\n",
    "            gather_spans(child, out)\n",
    "\n",
    "def x_in_y(query, base):\n",
    "    try:\n",
    "        l = len(query)\n",
    "    except TypeError:\n",
    "        l = 1\n",
    "        query = type(base)((query,))\n",
    "\n",
    "    for i in range(len(base)):\n",
    "        if base[i:i+l] == query:\n",
    "            return True\n",
    "    return False            \n",
    "            \n",
    "def get_copies(q, c, parse):\n",
    "    copies =  matcher(tokenizer.tokenize(q), tokenizer.tokenize(c))\n",
    "    spans = {}\n",
    "    gather_spans(parse, spans)\n",
    "    annotated_copies = []\n",
    "    for s in copies:\n",
    "        min_span = (None,None)\n",
    "        for cand in spans.keys():\n",
    "            if x_in_y(s, tokenizer.tokenize(cand)) and ( min_span[0] is None or len(cand) < len(min_span[0]) ):\n",
    "                min_span = (cand, spans[cand])\n",
    "        annotated_copies.append((s, min_span))\n",
    "    return annotated_copies\n",
    "        \n",
    "\n",
    "def get_q_parts(entry, nlp, tokens):\n",
    "    if entry['nodeType'].startswith('VB'):\n",
    "        if not nlp.vocab[entry['word'].lower()].is_stop:\n",
    "            tokens.append((\"[\"+entry['nodeType']+\"]\", entry['word']))\n",
    "    elif entry['nodeType'].startswith(\"WH\"):\n",
    "        tokens.append((\"[\"+entry['nodeType']+\"]\", entry['word']))\n",
    "    elif entry['nodeType'] == 'NP':\n",
    "        keep = True\n",
    "        for child in entry['children']:\n",
    "            if child['nodeType'] == 'NP' or child['nodeType'] == 'PP':\n",
    "                keep = False\n",
    "        if keep:\n",
    "            tokens.append(print_np(entry)) # (entry['word'])\n",
    "        else:\n",
    "            if 'children' in entry and entry['children']:\n",
    "                for child in entry['children']:\n",
    "                    get_q_parts(child, nlp, tokens)\n",
    "    else:\n",
    "        if 'children' in entry and entry['children']:\n",
    "            for child in entry['children']:\n",
    "                get_q_parts(child, nlp, tokens)\n",
    "\n",
    "def get_const_chunks(q):\n",
    "    res = predictor.predict(sentence=q)\n",
    "    tokens = []\n",
    "    get_q_parts(res['hierplane_tree']['root'], nlp, tokens)\n",
    "    return tokens, res['hierplane_tree']['root']\n",
    "\n",
    "def get_ordered_qsegs(q, c):\n",
    "    print(q)\n",
    "    q_chunks, parse = get_const_chunks(q)\n",
    "    print(q_chunks)\n",
    "    copied_chunks = get_copies(q, c, parse)\n",
    "    print(copied_chunks)\n",
    "    #print(parse)\n",
    "    assert False\n",
    "    combined_chunks = combine_chunks(copied_chunks, q_chunks, parse)\n",
    "    ordered_qsegs = order_chunks(combined_chunks, parse)\n",
    "    return ordered_qsegs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2426\n",
      "What is the name of the private day school for K-12 students the university runs?\n",
      "[('[WHNP]', 'What'), ('[NP]', 'name'), ('[NP]', 'private day school'), ('[NP]', 'K-12 students'), ('[NP]', 'university'), ('[VBZ]', 'runs')]\n",
      "[(['private', 'day', 'school', 'for', 'k', '-', '12', 'students'], ('the private day school for K-12 students the university runs', 'NP')), (['university', 'runs'], ('the university runs', 'S'))]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-295-f0fc6cb40f7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_ordered_qsegs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-282-c9f5fc1596d4>\u001b[0m in \u001b[0;36mget_ordered_qsegs\u001b[0;34m(q, c)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopied_chunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;31m#print(parse)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0mcombined_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopied_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mordered_qsegs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morder_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#####FIX THIS SHITS\n",
    "for i in random.sample(range(len(examples)), 1):\n",
    "    print(i)\n",
    "    i = 7370\n",
    "    q = questions[examples[i][1]]\n",
    "    c = contexts[examples[i][0]]\n",
    "    print(get_ordered_qsegs(q,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
