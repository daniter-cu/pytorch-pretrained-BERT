{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"examples/\")\n",
    "\n",
    "import logging\n",
    "import argparse\n",
    "import json\n",
    "from tqdm import tqdm, trange\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForPreTraining \n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_cond import InputExample, random_word, InputFeatures, BERTDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args\n",
    "gradient_accumulation_steps = 1\n",
    "train_batch_size = 1\n",
    "eval_file = \"dataset/dev-v2.0.json\"\n",
    "max_seq_length=256\n",
    "on_memory = True\n",
    "bert_model = \"model_cond/pytorch_model6.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/27/2019 11:52:44 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/daniter/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "Loading Squad: 100%|██████████| 35/35 [00:00<00:00, 1409.17it/s]\n",
      "Loading Squad: 100%|██████████| 35/35 [00:00<00:00, 1514.50it/s]\n",
      "03/27/2019 11:52:48 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/daniter/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "03/27/2019 11:52:48 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /Users/daniter/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/xx/8h5l1j614vv5wmbx9fbj69wm0000gn/T/tmpk4iy_m96\n",
      "03/27/2019 11:52:52 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the vocab size: 30522\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "\n",
    "# Load eval_data\n",
    "eval_dataset_answerable = BERTDataset(eval_file, \"qparts/part_labels%s.pkl\", tokenizer, seq_len=max_seq_length,\n",
    "                                    on_memory=on_memory)\n",
    "eval_dataset_unanswerable = BERTDataset(eval_file, \"qparts/part_labels%s.pkl\", tokenizer, seq_len=max_seq_length,\n",
    "                                    on_memory=on_memory, keep_answerable=False)\n",
    "\n",
    "# Prepare model\n",
    "model_state_dict = torch.load(bert_model, map_location='cpu') #TODO daniter: remove this map_location\n",
    "## TODO daniter: check if bert model is being loaded correctly\n",
    "model = BertForPreTraining.from_pretrained(\"bert-base-uncased\", state_dict=model_state_dict)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Prepare optimizer\n",
    "print(\"Checking the vocab size:\", len(tokenizer.vocab))\n",
    "# 768 is bert hidden size, 256 is GRU hidden size, 1 is the layers in the GRU\n",
    "\n",
    "# eval loader\n",
    "eval_sampler_ans = SequentialSampler(eval_dataset_answerable)\n",
    "eval_dataloader_ans = DataLoader(eval_dataset_answerable, sampler=eval_sampler_ans,\n",
    "                                 batch_size=train_batch_size)\n",
    "eval_sampler_unans = SequentialSampler(eval_dataset_unanswerable)\n",
    "eval_dataloader_unans = DataLoader(eval_dataset_unanswerable, sampler=eval_sampler_unans,\n",
    "                                   batch_size=train_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'at', 'the', 'same', 'time', 'the', 'mongols', 'imported', 'central', 'asian', 'muslims', 'to', 'serve', 'as', 'administrators', 'in', 'china', ',', 'the', 'mongols', 'also', 'sent', 'han', 'chinese', 'and', 'k', '##hita', '##ns', 'from', 'china', 'to', 'serve', 'as', 'administrators', 'over', 'the', 'muslim', 'population', 'in', 'bu', '##khar', '##a', 'in', 'central', 'asia', ',', 'using', 'foreigners', 'to', 'curt', '##ail', 'the', 'power', 'of', 'the', 'local', 'peoples', 'of', 'both', 'lands', '.', 'han', 'chinese', 'were', 'moved', 'to', 'central', 'asian', 'areas', 'like', 'be', '##sh', 'bali', '##q', ',', 'alma', '##li', '##q', ',', 'and', 'sam', '##ar', '##qa', '##nd', 'by', 'the', 'mongols', 'where', 'they', 'worked', 'as', 'artisans', 'and', 'farmers', '.', 'alan', '##s', 'were', 'recruited', 'into', 'the', 'mongol', 'forces', 'with', 'one', 'unit', 'called', '\"', 'right', 'alan', 'guard', '\"', 'which', 'was', 'combined', 'with', '\"', 'recently', 'surrendered', '\"', 'soldiers', ',', 'mongols', ',', 'and', 'chinese', 'soldiers', 'stationed', 'in', 'the', 'area', 'of', 'the', 'former', 'kingdom', 'of', 'q', '##och', '##o', 'and', 'in', 'be', '##sh', 'bali', '##kh', 'the', 'mongols', 'established', 'a', 'chinese', 'military', 'colony', 'led', 'by', 'chinese', 'general', 'qi', 'kong', '##zhi', '(', 'ch', \"'\", 'i', 'kung', '-', 'chi', '##h', ')', '.', 'after', 'the', 'mongol', 'conquest', 'of', 'central', 'asia', 'by', 'gen', '##ghi', '##s', 'khan', ',', 'foreigners', 'were', 'chosen', 'as', 'administrators', 'and', 'co', '-', 'management', 'with', 'chinese', 'and', 'q', '##ara', '-', 'k', '##hita', '##ys', '(', 'k', '##hita', '##ns', ')', 'of', 'gardens', 'and', 'fields', 'in', 'sam', '##ar', '##qa', '##nd', 'was', 'put', 'upon', 'the', 'muslims', 'as', 'a', 'requirement', 'since', 'muslims', 'were', 'not', 'allowed', 'to', 'manage', 'without', 'them', '.', 'the', 'mongol', 'appointed', 'governor', 'of', 'sam', '##ar', '##qa', '##nd', 'was', 'a', 'q', '##ara', '-', 'k', '[SEP]', 'japan', '[SEP]', 'bring', '[SEP]', 'mongols', '[SEP]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "target = 219 #50\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss_ans = 0\n",
    "    for batch_i, eval_batch in enumerate(eval_dataloader_unans):\n",
    "        eval_batch = tuple(t.to(device) for t in eval_batch)\n",
    "        input_ids, input_mask, segment_ids, lm_label_ids, is_next = eval_batch\n",
    "        if batch_i != target:\n",
    "            continue\n",
    "        if batch_i == target:\n",
    "            print(tokenizer.convert_ids_to_tokens(input_ids.data.numpy()[0]))\n",
    "        output, _ = model(input_ids, segment_ids, input_mask, None, None)\n",
    "        if batch_i == target:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['administrators']\n"
     ]
    }
   ],
   "source": [
    "labels = [lab for lab in lm_label_ids.data.numpy().ravel() if lab != -1]\n",
    "tokens = tokenizer.convert_ids_to_tokens(labels)\n",
    "print(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254\n",
      "['what', 'asian']\n"
     ]
    }
   ],
   "source": [
    "start_i = tokenizer.convert_ids_to_tokens(input_ids.data.numpy()[0]).index('[PAD]')\n",
    "print(start_i)\n",
    "print(tokenizer.convert_ids_to_tokens(np.argmax(output[0].data.numpy(), axis=1))[start_i:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'oxygen', 'is', 'the', 'most', 'abundant', 'chemical', 'element', 'by', 'mass', 'in', 'the', 'earth', \"'\", 's', 'bio', '##sphere', ',', 'air', ',', 'sea', 'and', 'land', '.', 'oxygen', 'is', 'the', 'third', 'most', 'abundant', 'chemical', 'element', 'in', 'the', 'universe', ',', 'after', 'hydrogen', 'and', 'helium', '.', 'about', '0', '.', '9', '%', 'of', 'the', 'sun', \"'\", 's', 'mass', 'is', 'oxygen', '.', 'oxygen', 'constitutes', '49', '.', '2', '%', 'of', 'the', 'earth', \"'\", 's', 'crust', 'by', 'mass', 'and', 'is', 'the', 'major', 'component', 'of', 'the', 'world', \"'\", 's', 'oceans', '(', '88', '.', '8', '%', 'by', 'mass', ')', '.', 'oxygen', 'gas', 'is', 'the', 'second', 'most', 'common', 'component', 'of', 'the', 'earth', \"'\", 's', 'atmosphere', ',', 'taking', 'up', '20', '.', '8', '%', 'of', 'its', 'volume', 'and', '23', '.', '1', '%', 'of', 'its', 'mass', '(', 'some', '101', '##5', 'tonnes', ')', '.', '[', 'd', ']', 'earth', 'is', 'unusual', 'among', 'the', 'planets', 'of', 'the', 'solar', 'system', 'in', 'having', 'such', 'a', 'high', 'concentration', 'of', 'oxygen', 'gas', 'in', 'its', 'atmosphere', ':', 'mars', '(', 'with', '0', '.', '1', '%', 'o', '2', 'by', 'volume', ')', 'and', 'venus', 'have', 'far', 'lower', 'concentrations', '.', 'the', 'o', '2', 'surrounding', 'these', 'other', 'planets', 'is', 'produced', 'solely', 'by', 'ultraviolet', 'radiation', 'impact', '##ing', 'oxygen', '-', 'containing', 'molecules', 'such', 'as', 'carbon', 'dioxide', '.', '[SEP]', 'major', 'part', '[SEP]', 'mass', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "target = 4 #50\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss_ans = 0\n",
    "    for batch_i, eval_batch in enumerate(eval_dataloader_ans):\n",
    "        eval_batch = tuple(t.to(device) for t in eval_batch)\n",
    "        input_ids, input_mask, segment_ids, lm_label_ids, is_next = eval_batch\n",
    "        if batch_i != target:\n",
    "            continue\n",
    "        if batch_i == target:\n",
    "            print(tokenizer.convert_ids_to_tokens(input_ids.data.numpy()[0]))\n",
    "        output, _ = model(input_ids, segment_ids, input_mask, None, None)\n",
    "        if batch_i == target:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oxygen']\n"
     ]
    }
   ],
   "source": [
    "labels = [lab for lab in lm_label_ids.data.numpy().ravel() if lab != -1]\n",
    "tokens = tokenizer.convert_ids_to_tokens(labels)\n",
    "print(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203\n",
      "['earth', \"'\", 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', \"'\", 's', \"'\", 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', \"'\", \"'\", \"'\", \"'\", 's', 's', 's', 's', 's', 's', 's']\n"
     ]
    }
   ],
   "source": [
    "start_i = tokenizer.convert_ids_to_tokens(input_ids.data.numpy()[0]).index('[PAD]')\n",
    "print(start_i)\n",
    "print(tokenizer.convert_ids_to_tokens(np.argmax(output[0].data.numpy(), axis=1))[start_i:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['earth'] tensor(9.5742)\n",
      "['oxygen'] tensor(7.6202)\n",
      "['world'] tensor(6.4623)\n",
      "['what'] tensor(6.1127)\n",
      "['most'] tensor(5.4165)\n",
      "['oceans'] tensor(5.3127)\n",
      "['atmosphere'] tensor(4.8200)\n",
      "['which'] tensor(4.7238)\n",
      "['carbon'] tensor(4.6831)\n",
      "['water'] tensor(4.6738)\n",
      "['ocean'] tensor(4.6734)\n",
      "['mars'] tensor(4.6709)\n",
      "['planet'] tensor(4.5660)\n",
      "['air'] tensor(4.5641)\n",
      "[\"'\"] tensor(4.4854)\n",
      "['hydrogen'] tensor(4.3737)\n",
      "['its'] tensor(4.0654)\n",
      "['neptune'] tensor(3.9373)\n",
      "['how'] tensor(3.7449)\n",
      "['life'] tensor(3.7068)\n",
      "['composed'] tensor(3.5799)\n",
      "['humans'] tensor(3.5747)\n",
      "['solar'] tensor(3.5316)\n",
      "['atmospheric'] tensor(3.5303)\n",
      "['sea'] tensor(3.4772)\n",
      "####################\n",
      "[\"'\"] tensor(11.3023)\n",
      "['s'] tensor(7.1991)\n",
      "['gas'] tensor(7.0532)\n",
      "['.'] tensor(6.5830)\n",
      "['element'] tensor(6.2769)\n",
      "['surface'] tensor(6.2494)\n",
      "['percent'] tensor(6.2119)\n",
      "['%'] tensor(6.0049)\n",
      "['much'] tensor(5.8014)\n",
      "['planet'] tensor(5.6757)\n",
      "['and'] tensor(5.6275)\n",
      "['ocean'] tensor(5.5587)\n",
      "['level'] tensor(5.5387)\n",
      "['##s'] tensor(5.4988)\n",
      "['energy'] tensor(5.4311)\n",
      "['substance'] tensor(5.3968)\n",
      "['body'] tensor(5.3332)\n",
      "['abundance'] tensor(5.1219)\n",
      "['##ble'] tensor(4.9977)\n",
      "['system'] tensor(4.9723)\n",
      "['percentage'] tensor(4.9170)\n",
      "['oxygen'] tensor(4.8352)\n",
      "['water'] tensor(4.8249)\n",
      "['amount'] tensor(4.6506)\n",
      "['atmosphere'] tensor(4.5360)\n",
      "####################\n",
      "['s'] tensor(12.5258)\n",
      "[\"'\"] tensor(8.7114)\n",
      "['of'] tensor(8.5539)\n",
      "['ocean'] tensor(7.4021)\n",
      "['earth'] tensor(6.9465)\n",
      "['the'] tensor(6.1289)\n",
      "['atmosphere'] tensor(5.9366)\n",
      "['body'] tensor(5.8338)\n",
      "['surface'] tensor(5.6394)\n",
      "['%'] tensor(5.6372)\n",
      "['natural'] tensor(5.5420)\n",
      "['water'] tensor(5.5403)\n",
      "['gas'] tensor(5.5398)\n",
      "['oceans'] tensor(5.4483)\n",
      "['element'] tensor(5.2785)\n",
      "['sea'] tensor(5.0042)\n",
      "['in'] tensor(4.9932)\n",
      "['energy'] tensor(4.9406)\n",
      "['marine'] tensor(4.9362)\n",
      "['oxygen'] tensor(4.8644)\n",
      "['##s'] tensor(4.6103)\n",
      "['atmospheric'] tensor(4.5546)\n",
      "['system'] tensor(4.4369)\n",
      "['.'] tensor(4.3127)\n",
      "['##ble'] tensor(4.2762)\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "for j in range(3):\n",
    "    c = Counter()\n",
    "    for i, o in enumerate(output[0][start_i+j]):\n",
    "        c[i] = o\n",
    "    for x, val in c.most_common(25):\n",
    "        print(tokenizer.convert_ids_to_tokens([x]), val)\n",
    "    print(\"#\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "1- Print context and question, manually enter the set of conditionals, and evaluate   \n",
    "\n",
    "2- evaluate some answerable and unanswerable questions by full question conditional and getting actual probabilities of each thing correctly normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations \n",
    "\n",
    "def build_input(context, tokens_b, target_tokens):\n",
    "    tokenized_context = tokenizer.tokenize(context)\n",
    "    buff_size = sum([len(t) for t in tokens_b]) + len(tokens_b) - 1 + len(target_tokens)\n",
    "    if len(tokenized_context) + buff_size > max_seq_length - 3:\n",
    "        end = max_seq_length - 3 - buff_size\n",
    "        tokenized_context = tokenized_context[:end]\n",
    "    \n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokenized_context:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    for i, conditional in enumerate(tokens_b):\n",
    "        for token in conditional:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(1)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(1)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    return torch.tensor([input_ids]), torch.tensor([input_mask]), torch.tensor([segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oxygen is the most abundant chemical element by mass in the Earth's biosphere, air, sea and land. Oxygen is the third most abundant chemical element in the universe, after hydrogen and helium. About 0.9% of the Sun's mass is oxygen. Oxygen constitutes 49.2% of the Earth's crust by mass and is the major component of the world's oceans (88.8% by mass). Oxygen gas is the second most common component of the Earth's atmosphere, taking up 20.8% of its volume and 23.1% of its mass (some 1015 tonnes).[d] Earth is unusual among the planets of the Solar System in having such a high concentration of oxygen gas in its atmosphere: Mars (with 0.1% O\n",
      "2 by volume) and Venus have far lower concentrations. The O\n",
      "2 surrounding these other planets is produced solely by ultraviolet radiation impacting oxygen-containing molecules such as carbon dioxide.\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Where  by mass is oxygen a major part?\n",
      "[('[NP]', 'oxygen'), ('[NP]', 'major part'), ('[NP]', 'mass'), ('[WHADVP]', 'Where')]\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "tensor(3.7444)\n",
      "[('[NP]', 'oxygen'), ('[NP]', 'mass'), ('[NP]', 'major part'), ('[WHADVP]', 'Where')]\n"
     ]
    }
   ],
   "source": [
    "target = 4 #50\n",
    "order = [2, 3, 1, 0]\n",
    "cid, qid, targetid, _ = eval_dataloader_ans.dataset.examples[target]\n",
    "context = eval_dataloader_ans.dataset.contexts[cid]\n",
    "question = eval_dataloader_ans.dataset.questions[qid]\n",
    "raw_targ = eval_dataloader_ans.dataset.raw_targets[targetid]\n",
    "\n",
    "raw_targ = [raw_targ[i] for i in order]\n",
    "raw_targ_copy = list(raw_targ)\n",
    "print(context)\n",
    "print(\"~\"*20)\n",
    "print(question)\n",
    "print(raw_targ)\n",
    "print(\"~\"*20)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    best_perm = None\n",
    "    best_odds = 0\n",
    "    num_perms = len(list(permutations(range(len(raw_targ_copy)))))\n",
    "    if num_perms > 24:\n",
    "        print(\"Too many options\")\n",
    "        raw_targ_copy = None\n",
    "    for perm_idx, raw_targ in enumerate(permutations(raw_targ_copy)):\n",
    "        targs_2_tokens = [tokenizer.tokenize(t) for _, t in raw_targ]\n",
    "        targs_2_ids = [tokenizer.convert_tokens_to_ids(t) for t in targs_2_tokens]\n",
    "        total_odds = 0\n",
    "        for token_idx in range(len(raw_targ)):\n",
    "            odds = 0\n",
    "            #print(\"Given \", [t for _, t in raw_targ[:token_idx]], \"predict\", raw_targ[token_idx][1])\n",
    "            # print(\"P(%s | %s )\" % (raw_targ[token_idx][1], \",\".join([t for _, t in raw_targ[:token_idx]])))\n",
    "            input_ids, input_mask, segment_ids = build_input(context, targs_2_tokens[:token_idx])\n",
    "            output, _ = model(input_ids, segment_ids, input_mask, None, None)\n",
    "            #print(tokenizer.convert_ids_to_tokens(np.argmax(output[0].data.numpy(), axis=1)))\n",
    "            start_id = np.where(input_mask.data.numpy() == 0)[1][0]\n",
    "            for t_i, t in enumerate(targs_2_ids[token_idx]):\n",
    "                odds += output[0][start_i+t_i][t]\n",
    "            odds = odds/len(targs_2_ids[token_idx])\n",
    "            # print(odds)\n",
    "            total_odds += odds\n",
    "        total_odds /= len(raw_targ)\n",
    "        if total_odds > best_odds:\n",
    "            best_odds = total_odds\n",
    "            best_perm = list(raw_targ)\n",
    "        # print(\"Total Odds:\", total_odds)\n",
    "        print(\"Finished \", perm_idx, \"of\", num_perms)\n",
    "print(best_odds)\n",
    "print(best_perm)\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The official record high temperature for Fresno is 115 °F (46.1 °C), set on July 8, 1905, while the official record low is 17 °F (−8 °C), set on January 6, 1913. The average windows for 100 °F (37.8 °C)+, 90 °F (32.2 °C)+, and freezing temperatures are June 1 thru September 13, April 26 thru October 9, and December 10 thru January 28, respectively, and no freeze occurred between in the 1983/1984 season. Annual rainfall has ranged from 23.57 inches (598.7 mm) in the “rain year” from July 1982 to June 1983 down to 4.43 inches (112.5 mm) from July 1933 to June 1934. The most rainfall in one month was 9.54 inches (242.3 mm) in November 1885 and the most rainfall in 24 hours 3.55 inches (90.2 mm) on November 18, 1885. Measurable precipitation falls on an average of 48 days annually. Snow is a rarity; the heaviest snowfall at the airport was 2.2 inches (0.06 m) on January 21, 1962.\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "What is the record high in January?\n",
      "[('[WHNP]', 'What'), ('[NP]', 'record high'), ('[NP]', 'January')]\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Finished  0 of 6\n",
      "Finished  1 of 6\n",
      "Finished  2 of 6\n",
      "Finished  3 of 6\n",
      "Finished  4 of 6\n",
      "Finished  5 of 6\n",
      "tensor(2.8984)\n",
      "[('[WHNP]', 'What'), ('[NP]', 'record high'), ('[NP]', 'January')]\n"
     ]
    }
   ],
   "source": [
    "target = 11 #50\n",
    "cid, qid, targetid, _ = eval_dataloader_unans.dataset.examples[target]\n",
    "context = eval_dataloader_ans.dataset.contexts[cid]\n",
    "question = eval_dataloader_ans.dataset.questions[qid]\n",
    "raw_targ = eval_dataloader_ans.dataset.raw_targets[targetid]\n",
    "raw_targ_copy = list(raw_targ)\n",
    "\n",
    "print(context)\n",
    "print(\"~\"*20)\n",
    "print(question)\n",
    "print(raw_targ)\n",
    "print(\"~\"*20)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    best_perm = None\n",
    "    best_odds = 0\n",
    "    num_perms = len(list(permutations(range(len(raw_targ_copy)))))\n",
    "    if num_perms > 24:\n",
    "        print(\"Too many options\")\n",
    "        raw_targ_copy = None\n",
    "    for perm_idx, raw_targ in enumerate(permutations(raw_targ_copy)):\n",
    "        targs_2_tokens = [tokenizer.tokenize(t) for _, t in raw_targ]\n",
    "        targs_2_ids = [tokenizer.convert_tokens_to_ids(t) for t in targs_2_tokens]\n",
    "        total_odds = 0\n",
    "        for token_idx in range(len(raw_targ)):\n",
    "            odds = 0\n",
    "            #print(\"Given \", [t for _, t in raw_targ[:token_idx]], \"predict\", raw_targ[token_idx][1])\n",
    "            # print(\"P(%s | %s )\" % (raw_targ[token_idx][1], \",\".join([t for _, t in raw_targ[:token_idx]])))\n",
    "            input_ids, input_mask, segment_ids = build_input(context, targs_2_tokens[:token_idx])\n",
    "            output, _ = model(input_ids, segment_ids, input_mask, None, None)\n",
    "            #print(tokenizer.convert_ids_to_tokens(np.argmax(output[0].data.numpy(), axis=1)))\n",
    "            start_id = np.where(input_mask.data.numpy() == 0)[1][0]\n",
    "            for t_i, t in enumerate(targs_2_ids[token_idx]):\n",
    "                odds += output[0][start_i+t_i][t]\n",
    "            odds = odds/len(targs_2_ids[token_idx])\n",
    "            # print(odds)\n",
    "            total_odds += odds\n",
    "        total_odds /= len(raw_targ)\n",
    "        if total_odds > best_odds:\n",
    "            best_odds = total_odds\n",
    "            best_perm = list(raw_targ)\n",
    "        # print(\"Total Odds:\", total_odds)\n",
    "        print(\"Finished \", perm_idx, \"of\", num_perms)\n",
    "print(best_odds)\n",
    "print(best_perm)\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(permutations(range(5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.\n"
     ]
    }
   ],
   "source": [
    "context = 0\n",
    "ans_questions = set()\n",
    "unans_questions = set()\n",
    "ans_examples = []\n",
    "unans_examples = []\n",
    "for x in eval_dataloader_unans.dataset.examples:\n",
    "    if x[0] == context:\n",
    "        if x[1] not in unans_questions:\n",
    "            unans_examples.append(x)\n",
    "        unans_questions.add(x[1])        \n",
    "for x in eval_dataloader_ans.dataset.examples:\n",
    "    if x[0] == context:\n",
    "        if x[1] not in ans_questions:\n",
    "            ans_examples.append(x)\n",
    "        ans_questions.add(x[1])\n",
    "print (eval_dataloader_unans.dataset.contexts[context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~\n",
      "From which countries did the Norse originate?\n",
      "[('[WHPP]', 'From which countries'), ('[NP]', 'Norse'), ('[VB]', 'originate')]\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Total Odds: tensor(1.8462)\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "What century did the Normans first gain their separate identity?\n",
      "[('[WHNP]', 'What century'), ('[NP]', 'Normans'), ('[VBP]', 'gain'), ('[NP]', 'their separate identity')]\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Total Odds: tensor(3.4586)\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "In what country is Normandy located?\n",
      "[('[WHNP]', 'what country'), ('[NP]', 'Normandy'), ('[VBN]', 'located')]\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Total Odds: tensor(3.2766)\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "When were the Normans in Normandy?\n",
      "[('[WHADVP]', 'When'), ('[NP]', 'Normans'), ('[NP]', 'Normandy')]\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Total Odds: tensor(3.9580)\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Who was the Norse leader?\n",
      "[('[WHNP]', 'Who'), ('[NP]', 'Norse leader')]\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Total Odds: tensor(2.0593)\n"
     ]
    }
   ],
   "source": [
    "for example in ans_examples:\n",
    "    cid, qid, targetid, _ = example\n",
    "    context = eval_dataloader_ans.dataset.contexts[cid]\n",
    "    question = eval_dataloader_ans.dataset.questions[qid]\n",
    "    raw_targ = eval_dataloader_ans.dataset.raw_targets[targetid]\n",
    "    raw_targ_copy = list(raw_targ)\n",
    "\n",
    "    #print(context)\n",
    "    print(\"~\"*20)\n",
    "    print(question)\n",
    "    print(raw_targ)\n",
    "    print(\"~\"*20)\n",
    "    \n",
    "    first_targets = [(tag, words) for tag, words in raw_targ if \"-\"  not in tag  and \"W\" not in tag]\n",
    "    middle_targets_int = [(tag, words) for tag, words in raw_targ if \"-\"   in tag]\n",
    "    middle_targets = []\n",
    "    for i in range(20):\n",
    "        for tag, words in middle_targets_int:\n",
    "            if str(i) in tag:\n",
    "                middle_targets.append((tag,words))\n",
    "\n",
    "    second_targets = [(tag, words) for tag, words in raw_targ if \"W\" in tag]\n",
    "    random.shuffle(first_targets)\n",
    "    random.shuffle(second_targets)\n",
    "    raw_targ = (first_targets + middle_targets + second_targets)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        targs_2_tokens = [tokenizer.tokenize(t) for _, t in raw_targ]\n",
    "        targs_2_ids = [tokenizer.convert_tokens_to_ids(t) for t in targs_2_tokens]\n",
    "        total_odds = 0\n",
    "        for token_idx in range(len(raw_targ)):\n",
    "            odds = 0\n",
    "            #print(\"Given \", [t for _, t in raw_targ[:token_idx]], \"predict\", raw_targ[token_idx][1])\n",
    "            # print(\"P(%s | %s )\" % (raw_targ[token_idx][1], \",\".join([t for _, t in raw_targ[:token_idx]])))\n",
    "            input_ids, input_mask, segment_ids = build_input(context, targs_2_tokens[:token_idx])\n",
    "            output, _ = model(input_ids, segment_ids, input_mask, None, None)\n",
    "            #print(tokenizer.convert_ids_to_tokens(np.argmax(output[0].data.numpy(), axis=1)))\n",
    "            start_id = np.where(input_mask.data.numpy() == 0)[1][0]\n",
    "            for t_i, t in enumerate(targs_2_ids[token_idx]):\n",
    "                odds += output[0][start_i+t_i][t]\n",
    "            odds = odds/len(targs_2_ids[token_idx])\n",
    "            # print(odds)\n",
    "            total_odds += odds\n",
    "        total_odds /= len(raw_targ)\n",
    "        if total_odds > best_odds:\n",
    "            best_odds = total_odds\n",
    "            best_perm = list(raw_targ)\n",
    "        print(\"Total Odds:\", total_odds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~\n",
      "What is France a region of?\n",
      "[('[WHNP]', 'What'), ('[NP]', 'France'), ('[NP]', 'region')]\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Total Odds: tensor(4.0956)\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "When did the Frankish identity emerge?\n",
      "[('[WHADVP]', 'When'), ('[NP]', 'Frankish identity'), ('[VB]', 'emerge')]\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Total Odds: tensor(2.2725)\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Who gave their name to Normandy in the 1000's and 1100's\n",
      "[('[WHNP]', 'Who'), ('[VBD]', 'gave'), ('[NP]', 'their name'), ('[NP]', 'Normandy'), ('[NP]', \"1000 's\"), ('[NP]', \"1100 's\")]\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Total Odds: tensor(2.9115)\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Who did King Charles III swear fealty to?\n",
      "[('[WHNP]', 'Who'), ('[NP]', 'King Charles III'), ('[VBP]', 'swear'), ('[NP]', 'fealty')]\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Total Odds: tensor(3.1540)\n"
     ]
    }
   ],
   "source": [
    "for example in unans_examples:\n",
    "    cid, qid, targetid, _ = example\n",
    "    context = eval_dataloader_ans.dataset.contexts[cid]\n",
    "    question = eval_dataloader_ans.dataset.questions[qid]\n",
    "    raw_targ = eval_dataloader_ans.dataset.raw_targets[targetid]\n",
    "\n",
    "    raw_targ_copy = list(raw_targ)\n",
    "    #print(context)\n",
    "    print(\"~\"*20)\n",
    "    print(question)\n",
    "    print(raw_targ)\n",
    "    print(\"~\"*20)\n",
    "    \n",
    "    first_targets = [(tag, words) for tag, words in raw_targ if \"-\"  not in tag  and \"W\" not in tag]\n",
    "    middle_targets_int = [(tag, words) for tag, words in raw_targ if \"-\"   in tag]\n",
    "    middle_targets = []\n",
    "    for i in range(20):\n",
    "        for tag, words in middle_targets_int:\n",
    "            if str(i) in tag:\n",
    "                middle_targets.append((tag,words))\n",
    "\n",
    "    second_targets = [(tag, words) for tag, words in raw_targ if \"W\" in tag]\n",
    "    random.shuffle(first_targets)\n",
    "    random.shuffle(second_targets)\n",
    "    raw_targ = (first_targets + middle_targets + second_targets)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        targs_2_tokens = [tokenizer.tokenize(t) for _, t in raw_targ]\n",
    "        targs_2_ids = [tokenizer.convert_tokens_to_ids(t) for t in targs_2_tokens]\n",
    "        total_odds = 0\n",
    "        for token_idx in range(len(raw_targ)):\n",
    "            odds = 0\n",
    "            #print(\"Given \", [t for _, t in raw_targ[:token_idx]], \"predict\", raw_targ[token_idx][1])\n",
    "            # print(\"P(%s | %s )\" % (raw_targ[token_idx][1], \",\".join([t for _, t in raw_targ[:token_idx]])))\n",
    "            input_ids, input_mask, segment_ids = build_input(context, targs_2_tokens[:token_idx])\n",
    "            output, _ = model(input_ids, segment_ids, input_mask, None, None)\n",
    "            #print(tokenizer.convert_ids_to_tokens(np.argmax(output[0].data.numpy(), axis=1)))\n",
    "            start_id = np.where(input_mask.data.numpy() == 0)[1][0]\n",
    "            for t_i, t in enumerate(targs_2_ids[token_idx]):\n",
    "                odds += output[0][start_i+t_i][t]\n",
    "            odds = odds/len(targs_2_ids[token_idx])\n",
    "            # print(odds)\n",
    "            total_odds += odds\n",
    "        total_odds /= len(raw_targ)\n",
    "        if total_odds > best_odds:\n",
    "            best_odds = total_odds\n",
    "            best_perm = list(raw_targ)\n",
    "        print(\"Total Odds:\", total_odds)\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples(contexts):\n",
    "    ans_examples = []\n",
    "    unans_examples = []\n",
    "    for context in contexts:\n",
    "        ans_questions = set()\n",
    "        unans_questions = set()\n",
    "        for x in eval_dataloader_unans.dataset.examples:\n",
    "            if x[0] == context:\n",
    "                if x[1] not in unans_questions:\n",
    "                    unans_examples.append(x)\n",
    "                unans_questions.add(x[1])        \n",
    "        for x in eval_dataloader_ans.dataset.examples:\n",
    "            if x[0] == context:\n",
    "                if x[1] not in ans_questions:\n",
    "                    ans_examples.append(x)\n",
    "                ans_questions.add(x[1])\n",
    "        #print (eval_dataloader_unans.dataset.contexts[context])\n",
    "    return((ans_examples, unans_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import LogSoftmax\n",
    "softmax_model = LogSoftmax(dim=0)\n",
    "\n",
    "def perplexity(logit_idx, dist):\n",
    "    log_prob = 0\n",
    "    for i, lg_idx in enumerate(logit_idx):\n",
    "        prob = softmax_model(dist[i])[lg_idx]\n",
    "        log_prob += prob\n",
    "    return (log_prob / len(logit_idx)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_odds(examples, dataloader):\n",
    "    if len(examples) == 0:\n",
    "        return 0\n",
    "    total_total_odds = 0\n",
    "    total_total_perlex = 0\n",
    "    max_perplex = 0\n",
    "    for example in examples:\n",
    "        cid, qid, targetid, _ = example\n",
    "        context = dataloader.dataset.contexts[cid]\n",
    "        question = dataloader.dataset.questions[qid]\n",
    "        raw_targ = dataloader.dataset.raw_targets[targetid]\n",
    "\n",
    "        raw_targ_copy = list(raw_targ)\n",
    "\n",
    "        first_targets = [(tag, words) for tag, words in raw_targ if \"-\"  not in tag  and \"W\" not in tag]\n",
    "        middle_targets_int = [(tag, words) for tag, words in raw_targ if \"-\"   in tag]\n",
    "        middle_targets = []\n",
    "        for i in range(20):\n",
    "            for tag, words in middle_targets_int:\n",
    "                if str(i) in tag:\n",
    "                    middle_targets.append((tag,words))\n",
    "\n",
    "        second_targets = [(tag, words) for tag, words in raw_targ if \"W\" in tag]\n",
    "        random.shuffle(first_targets)\n",
    "        random.shuffle(second_targets)\n",
    "        raw_targ = (first_targets + middle_targets + second_targets)\n",
    "        raw_targ = [t for t in raw_targ if t[1]]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            targs_2_tokens = [tokenizer.tokenize(t) for _, t in raw_targ]\n",
    "            targs_2_ids = [tokenizer.convert_tokens_to_ids(t) for t in targs_2_tokens]\n",
    "            total_odds = 0\n",
    "            min_odds = 100\n",
    "            total_perplex = 0\n",
    "            for token_idx in range(len(raw_targ)):\n",
    "                odds = 0\n",
    "                odds_list = []\n",
    "                input_ids, input_mask, segment_ids = build_input(context, targs_2_tokens[:token_idx], targs_2_tokens[token_idx])\n",
    "                output, _ = model(input_ids, segment_ids, input_mask, None, None)\n",
    "                \n",
    "                #print(input_mask)\n",
    "                start_id = np.where(input_mask.data.numpy() == 0)[1][0]\n",
    "                for t_i, t in enumerate(targs_2_ids[token_idx]):\n",
    "                    odds += output[0][start_i+t_i][t]\n",
    "                    odds_list.append(output[0][start_i+t_i][t])\n",
    "                if len(targs_2_ids[token_idx]) == 0:\n",
    "                    print(token_idx, targs_2_ids, targs_2_ids[token_idx], raw_targ)\n",
    "                odds = odds/len(targs_2_ids[token_idx])\n",
    "                if odds < min_odds:\n",
    "                    min_odds = odds\n",
    "                # print(odds)\n",
    "                total_odds += odds\n",
    "                perplex = perplexity(targs_2_ids[token_idx], output[0][start_i:])\n",
    "                total_perplex += perplex / len(raw_targ)\n",
    "                #print(perplex)\n",
    "            #print(\"Perplexity\", -total_perplex)\n",
    "            total_odds /= len(raw_targ)\n",
    "            total_total_odds += total_odds\n",
    "            total_total_perlex += -total_perplex\n",
    "            if -total_perplex > max_perplex:\n",
    "                max_perplex = -total_perplex\n",
    "            #print(\"Total Odds:\", total_odds)\n",
    "            #print(\"Min odds:\", min_odds)\n",
    "    return max_perplex#(total_total_perlex / len(examples))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.5535) tensor(3.3792)\n",
      "tensor(-1.0365) tensor(-1.3864)\n",
      "tensor(4.2462) tensor(4.7270)\n",
      "tensor(4.1131) tensor(1.8831)\n",
      "tensor(0.4667) tensor(3.4406)\n",
      "tensor(2.7906) tensor(2.3163)\n",
      "tensor(3.4063) tensor(2.8470)\n",
      "tensor(5.2118) tensor(4.2501)\n",
      "tensor(1.7427) tensor(1.4449)\n",
      "tensor(2.4113) tensor(1.0845)\n",
      "tensor(3.4868) tensor(1.6543)\n",
      "tensor(-2.3763) tensor(-3.0515)\n",
      "tensor(3.3259) tensor(1.9908)\n",
      "tensor(2.5147) tensor(1.4459)\n",
      "tensor(2.6071) tensor(2.7918)\n",
      "tensor(4.4486) tensor(2.1497)\n",
      "tensor(3.0473) tensor(3.1637)\n",
      "tensor(5.8250) tensor(2.5912)\n",
      "tensor(0.6822) tensor(0.7596)\n",
      "tensor(4.7678) tensor(2.6951)\n"
     ]
    }
   ],
   "source": [
    "contexts = range(20)\n",
    "ans_e, unans_e = get_examples(contexts)\n",
    "for context in contexts:\n",
    "    avg_ans_odds = get_avg_odds([e for e in ans_e if e[0] == context], eval_dataloader_ans)\n",
    "    avg_unans_odds = get_avg_odds([e for e in unans_e if e[0] == context], eval_dataloader_unans)\n",
    "    print(avg_ans_odds, avg_unans_odds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_context_and_questions(c_i, ans_e, unans_e):\n",
    "    context = eval_dataloader_ans.dataset.contexts[c_i]\n",
    "    ans_q = [eval_dataloader_ans.dataset.questions[x[1]] for x in ans_e if x[0] == c_i]\n",
    "    unans_q = [eval_dataloader_unans.dataset.questions[x[1]] for x in unans_e if x[0] == c_i]\n",
    "    print(context)\n",
    "    print(\"~\"*20)\n",
    "    print(\"Answerable\")\n",
    "    for q in ans_q:\n",
    "        print(q)\n",
    "    print(\"~\"*20)\n",
    "    print(\"Unanswerable\")\n",
    "    for q in unans_q:\n",
    "        print(q)\n",
    "    print(\"#\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.286541938781738 10.638350486755371\n",
      "14.448400815327961 16.05839467048645\n",
      "10.816518306732178 12.168883959452312\n",
      "18.07494010925293 19.494102478027344\n",
      "9.415565808614094 10.754651069641113\n",
      "12.242334365844725 12.955363273620605\n",
      "10.79151039123535 10.99641799926758\n",
      "9.725075960159302 9.76213812828064\n",
      "10.705849409103394 10.28580681482951\n",
      "10.716663519541422 11.060950597127277\n",
      "8.0015549659729 9.269022941589355\n",
      "17.86482661111014 18.38697361946106\n",
      "9.857380747795105 11.658464670181274\n",
      "9.096316719055176 8.354247689247131\n",
      "8.80791441599528 12.401814460754395\n",
      "8.38168478012085 10.11616325378418\n",
      "8.31816953420639 9.740534782409668\n",
      "11.500575224558514 9.75913174947103\n",
      "19.504870891571045 19.296866734822594\n",
      "6.222853660583496 10.500753402709961\n"
     ]
    }
   ],
   "source": [
    "contexts = random.sample(range(1203), 20)\n",
    "ans_e, unans_e = get_examples(contexts)\n",
    "for context in contexts:\n",
    "    #print_context_and_questions(context, ans_e, unans_e)\n",
    "    #print(\"ans\")\n",
    "    avg_ans_odds = get_avg_odds([e for e in ans_e if e[0] == context], eval_dataloader_ans)\n",
    "    #print(\"unans\")\n",
    "    avg_unans_odds = get_avg_odds([e for e in unans_e if e[0] == context], eval_dataloader_unans)\n",
    "    print(avg_ans_odds, avg_unans_odds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things that work on aggregate:\n",
    "- maximum perplexity\n",
    "- average logit scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
