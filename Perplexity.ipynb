{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import pickle\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_local//pytorch_model.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-c5ba25a46380>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_state_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_local//pytorch_model.bin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForMaskedLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_state_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#model = BertForMaskedLM.from_pretrained('model_local/test/model.tgz')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ptbert/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_local//pytorch_model.bin'"
     ]
    }
   ],
   "source": [
    "model_state_dict = torch.load(\"model_local//pytorch_model.bin\", map_location='cpu') \n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased', state_dict=model_state_dict)\n",
    "#model = BertForMaskedLM.from_pretrained('model_local/test/model.tgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with open(\"dataset/dev-v2.0.json\", 'r') as handle:\n",
    "    jdata = json.load(handle)\n",
    "    data = jdata['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prob_nn(context, question):\n",
    "    gt_question = question\n",
    "    gt_q_tokens = tokenizer.tokenize(gt_question)\n",
    "    gt_indexed_q_tokens = tokenizer.convert_tokens_to_ids(gt_q_tokens)\n",
    "    \n",
    "    context_tokens = tokenizer.tokenize(context)\n",
    "    indexed_context_tokens = tokenizer.convert_tokens_to_ids(context_tokens)\n",
    "    \n",
    "    tokens_tensor = torch.tensor([indexed_context_tokens + gt_indexed_q_tokens])\n",
    "    segments_tensors = torch.tensor([0]*len(indexed_context_tokens) + [1]*len(gt_indexed_q_tokens))\n",
    "    predictions = model(tokens_tensor, segments_tensors)\n",
    "    \n",
    "    total = 0\n",
    "    context_len = len(context_tokens)\n",
    "    q_len = len(gt_indexed_q_tokens)\n",
    "    for i in range(q_len):\n",
    "        preds = predictions[0, context_len+i]\n",
    "        m = torch.nn.LogSoftmax(0)\n",
    "        total += -m(preds)[gt_indexed_q_tokens[i]]\n",
    "    entropy = total / q_len\n",
    "    perplexity = torch.exp(entropy)\n",
    "    return perplexity.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerable_probs = []\n",
    "unanswerable_probs = []\n",
    "counter = 0\n",
    "for i in range(1):#len(data)):\n",
    "    section = data[i]['paragraphs']\n",
    "    for sec in section:\n",
    "        context = sec['context']\n",
    "        qas = sec['qas']\n",
    "        for j in range(1):#len(qas)):\n",
    "            question = qas[j]['question']\n",
    "            label = qas[j]['is_impossible']\n",
    "            try:\n",
    "                prob = calc_prob_nn(context, question)\n",
    "            except:\n",
    "                continue\n",
    "            if label:\n",
    "                unanswerable_probs.append(prob)\n",
    "            else:\n",
    "                answerable_probs.append(prob)\n",
    "            counter += 1\n",
    "            if counter % 100 == 0:\n",
    "                print(\"Processed \", counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 39)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unanswerable_probs), len(answerable_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.038330316543579,\n",
       " 1.0055084228515625,\n",
       " 1.0810725688934326,\n",
       " 1.012487530708313,\n",
       " 1.380039095878601,\n",
       " 1.640554666519165,\n",
       " 1.1055219173431396,\n",
       " 1.0394196510314941,\n",
       " 1.0550795793533325,\n",
       " 1.0171822309494019,\n",
       " 1.2519900798797607,\n",
       " 1.336911916732788,\n",
       " 1.0026198625564575,\n",
       " 1.01677668094635,\n",
       " 2.2120251655578613,\n",
       " 1.0343737602233887,\n",
       " 2.997744560241699,\n",
       " 1.0117734670639038,\n",
       " 1.0477514266967773,\n",
       " 1.0544703006744385,\n",
       " 1.3831052780151367,\n",
       " 1.418177604675293,\n",
       " 1.0180962085723877,\n",
       " 1.1981474161148071,\n",
       " 1.0166099071502686,\n",
       " 1.3279578685760498,\n",
       " 1.7596973180770874,\n",
       " 1.0504934787750244,\n",
       " 1.149644374847412,\n",
       " 1.032496452331543,\n",
       " 2.6909704208374023,\n",
       " 1.014782428741455,\n",
       " 1.3074921369552612,\n",
       " 1.0248439311981201,\n",
       " 1.0974032878875732,\n",
       " 1.178715705871582,\n",
       " 1.0841788053512573,\n",
       " 1.1735104322433472,\n",
       " 1.133049488067627]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answerable_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_question = question\n",
    "gt_q_tokens = tokenizer.tokenize(gt_question)\n",
    "gt_indexed_q_tokens = tokenizer.convert_tokens_to_ids(gt_q_tokens)\n",
    "\n",
    "context_tokens = tokenizer.tokenize(context)\n",
    "indexed_context_tokens = tokenizer.convert_tokens_to_ids(context_tokens)\n",
    "\n",
    "tokens_tensor = torch.tensor([indexed_context_tokens + gt_indexed_q_tokens])\n",
    "segments_tensors = torch.tensor([0]*len(indexed_context_tokens) + [1]*len(gt_indexed_q_tokens))\n",
    "predictions = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "total = 0\n",
    "context_len = len(context_tokens)\n",
    "q_len = len(gt_indexed_q_tokens)\n",
    "for i in range(q_len):\n",
    "    preds = predictions[0, context_len+i]\n",
    "    m = torch.nn.LogSoftmax(0)\n",
    "    tmp =  -m(preds)[gt_indexed_q_tokens[i]]\n",
    "    print (\"max\", max(-m(preds)))\n",
    "    print (\"min\", min(-m(preds)))\n",
    "    print (tmp)\n",
    "    total += tmp\n",
    "entropy = total / q_len\n",
    "perplexity = torch.exp(entropy)\n",
    "perplexity.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
