{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import pickle\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_state_dict = torch.load(\"model_local//pytorch_model.bin\", map_location='cpu') \n",
    "#model = BertForMaskedLM.from_pretrained('bert-base-uncased', state_dict=model_state_dict)\n",
    "model = BertForMaskedLM.from_pretrained('model_local/test/model.tgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with open(\"dataset/dev-v2.0.json\", 'r') as handle:\n",
    "    jdata = json.load(handle)\n",
    "    data = jdata['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prob_nn(context, question):\n",
    "    gt_question = question\n",
    "    gt_q_tokens = tokenizer.tokenize(gt_question)\n",
    "    gt_indexed_q_tokens = tokenizer.convert_tokens_to_ids(gt_q_tokens)\n",
    "    \n",
    "    context_tokens = tokenizer.tokenize(context)\n",
    "    indexed_context_tokens = tokenizer.convert_tokens_to_ids(context_tokens)\n",
    "    \n",
    "    tokens_tensor = torch.tensor([indexed_context_tokens + gt_indexed_q_tokens])\n",
    "    segments_tensors = torch.tensor([0]*len(indexed_context_tokens) + [1]*len(gt_indexed_q_tokens))\n",
    "    predictions = model(tokens_tensor, segments_tensors)\n",
    "    \n",
    "    total = 0\n",
    "    context_len = len(context_tokens)\n",
    "    q_len = len(gt_indexed_q_tokens)\n",
    "    for i in range(q_len):\n",
    "        preds = predictions[0, context_len+i]\n",
    "        m = torch.nn.LogSoftmax(0)\n",
    "        total += -m(preds)[gt_indexed_q_tokens[i]]\n",
    "    entropy = total / q_len\n",
    "    perplexity = torch.exp(entropy)\n",
    "    return perplexity.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerable_probs = []\n",
    "unanswerable_probs = []\n",
    "counter = 0\n",
    "for i in range(1):#len(data)):\n",
    "    section = data[i]['paragraphs']\n",
    "    for sec in section:\n",
    "        context = sec['context']\n",
    "        qas = sec['qas']\n",
    "        for j in range(1):#len(qas)):\n",
    "            question = qas[j]['question']\n",
    "            label = qas[j]['is_impossible']\n",
    "            try:\n",
    "                prob = calc_prob_nn(context, question)\n",
    "            except:\n",
    "                continue\n",
    "            if label:\n",
    "                unanswerable_probs.append(prob)\n",
    "            else:\n",
    "                answerable_probs.append(prob)\n",
    "            counter += 1\n",
    "            if counter % 100 == 0:\n",
    "                print(\"Processed \", counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 39)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unanswerable_probs), len(answerable_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.038330316543579,\n",
       " 1.0055084228515625,\n",
       " 1.0810725688934326,\n",
       " 1.012487530708313,\n",
       " 1.380039095878601,\n",
       " 1.640554666519165,\n",
       " 1.1055219173431396,\n",
       " 1.0394196510314941,\n",
       " 1.0550795793533325,\n",
       " 1.0171822309494019,\n",
       " 1.2519900798797607,\n",
       " 1.336911916732788,\n",
       " 1.0026198625564575,\n",
       " 1.01677668094635,\n",
       " 2.2120251655578613,\n",
       " 1.0343737602233887,\n",
       " 2.997744560241699,\n",
       " 1.0117734670639038,\n",
       " 1.0477514266967773,\n",
       " 1.0544703006744385,\n",
       " 1.3831052780151367,\n",
       " 1.418177604675293,\n",
       " 1.0180962085723877,\n",
       " 1.1981474161148071,\n",
       " 1.0166099071502686,\n",
       " 1.3279578685760498,\n",
       " 1.7596973180770874,\n",
       " 1.0504934787750244,\n",
       " 1.149644374847412,\n",
       " 1.032496452331543,\n",
       " 2.6909704208374023,\n",
       " 1.014782428741455,\n",
       " 1.3074921369552612,\n",
       " 1.0248439311981201,\n",
       " 1.0974032878875732,\n",
       " 1.178715705871582,\n",
       " 1.0841788053512573,\n",
       " 1.1735104322433472,\n",
       " 1.133049488067627]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answerable_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_question = question\n",
    "gt_q_tokens = tokenizer.tokenize(gt_question)\n",
    "gt_indexed_q_tokens = tokenizer.convert_tokens_to_ids(gt_q_tokens)\n",
    "\n",
    "context_tokens = tokenizer.tokenize(context)\n",
    "indexed_context_tokens = tokenizer.convert_tokens_to_ids(context_tokens)\n",
    "\n",
    "tokens_tensor = torch.tensor([indexed_context_tokens + gt_indexed_q_tokens])\n",
    "segments_tensors = torch.tensor([0]*len(indexed_context_tokens) + [1]*len(gt_indexed_q_tokens))\n",
    "predictions = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "total = 0\n",
    "context_len = len(context_tokens)\n",
    "q_len = len(gt_indexed_q_tokens)\n",
    "for i in range(q_len):\n",
    "    preds = predictions[0, context_len+i]\n",
    "    m = torch.nn.LogSoftmax(0)\n",
    "    tmp =  -m(preds)[gt_indexed_q_tokens[i]]\n",
    "    print (\"max\", max(-m(preds)))\n",
    "    print (\"min\", min(-m(preds)))\n",
    "    print (tmp)\n",
    "    total += tmp\n",
    "entropy = total / q_len\n",
    "perplexity = torch.exp(entropy)\n",
    "perplexity.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
